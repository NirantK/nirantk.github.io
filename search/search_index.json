{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>I'm a Machine Learning Consultant and AI Engineer. I help startups and founders build AI products and services. This includes things like LLMs and Vector Databases, which are amazing tools which unlock a lot of new, intelligent value but also have their own limits.</p> <p>I recently built FastEmbed, an embedding library built for speed using ONNX. It is maintained by Qdrant - a vector search engine.</p> <p>Trivia:</p> <ol> <li>Dr. Andrew Ng recommends Awesome NLP, a repo I've maintained since 2018 at Stanford's Deep Learning course CS 230.</li> <li>Top 5 GenAI Scientists in India, Analytics India Magazine</li> <li>I have written a book on Natural Language Processing </li> </ol>"},{"location":"#background","title":"Background","text":"<p>Machine Learning and AI Engineer with 7+ years of experience in chat bots, retrieval, ranking and language modeling. </p> <p>As a Machine Learning Engineer, I</p> <ul> <li>Trained the first Hindi LM</li> <li>Deployed Sentence Transformers and Annoy (vector search library) for cosine Similarity powered search in 2018 in production</li> <li>Managed a team of 3 engineers to build a support chatbot for 1M chat messages per month</li> </ul> <p>As an AI Engineer,</p> <ul> <li>Have built and deployed Question Answering systems for 3+ years, including 2 projects with OpenAI LLMs e.g. text-davinci-003, GPT3.5 and GPT4</li> <li>Hallucination-free summarization and question answering systems</li> </ul> <p>PS: What is an AI Engineer? Here you go: https://latent.space/p/ai-engineer</p>"},{"location":"#book","title":"Book","text":"<p>Book: NLP in Python: Quickstart Guide</p> <p>Code: Github</p> <p>I wrote this book in 2018 to make Natural Language Programming more accessible for software engineers and programmers. This had a very design and code-first view of tools and their limitations. Today, most of it is outdated, I do not recommend buying it. </p> <p></p>"},{"location":"#papers-and-open-source-contributions","title":"Papers and Open Source Contributions","text":"<ol> <li> <p>Hinglish: github, paper focussed on code-mixed languages was published in ACL 2019.</p> </li> <li> <p>Awesome Project Ideas  - Curated list of machine learning (mostly deep learning) project ideas with datasets. These ideas range from Vision, Text, Forecasting to Recommender Systems</p> </li> <li> <p>Awesome NLP  Curated list of Natural Language Processing Resources. I've been the Primary Maintainer for Awesome-NLP</p> <ul> <li>Recommended by Dr. Andrew Ng's (Stanford) CS 230</li> <li>Featured in Github's Official Machine Learning Collection since 2016 and </li> </ul> </li> <li> <p>State of the Art Language Modeling in Hindi + new datasets, check the code here at hindi2vec </p> </li> <li> <p>Comparative Study of Preprocessing and Classification Methods in Character Recognition of Natural Scene Images. In: Machine Intelligence and Signal Processing. Advances in Intelligent Systems and Computing, vol 390. Springer, New Delhi (2015). https://doi.org/10.1007/978-81-322-2625-3_11</p> </li> </ol>"},{"location":"#talks","title":"Talks","text":"<ol> <li>Fifth Elephant MLOps Conf 2021: Slides</li> <li>PyCon India 2019: Slides and Youtube</li> <li>inMobi Tech Talks: A Nightmare on the LM Street; Slides</li> <li>Wingify DevFest: NLP for Indian Languages; Slides, Youtube</li> <li>PyData Bengaluru Inaugral Talk: Quiz Generation with spaCy; Youtube</li> </ol>"},{"location":"#web-mentions-my-5-minutes-of-internet-fame","title":"Web Mentions - My 5 minutes of Internet fame","text":"<ol> <li> <p>Search and Informational Retrieval Ranking Challenge hosted by Bing AI Team (2019)</p> </li> <li> <p>Won the Kaggle Kernel Prize (2019)</p> <ul> <li>The Hitchhiker's Guide to NLP in spaCy won the first ever NLP themed Kaggle Kernel award. I won a free licensed copy of Prodi.gy worth $390 with it, and $500 in cash. </li> </ul> </li> <li> <p>Exploratory Programming Notes found helpful by Nobel Laureate (2018)</p> <ul> <li>Tips, Tricks, Best Practices for working with Jupyter Notebook's was appreciated by Economics Nobel Laureate 2018 Dr. Paul Romer: <p>Nirant, this looks very helpful. Re your recommendation to use f-strings, do you know a good place to learn about them for someone new to Python? Everything I\u2019ve found seems to be for someone making the transition from older ways that a newbie doesn\u2019t need to learn.</p>\u2014 Paul Romer (@paulmromer) April 15, 2018 </li> </ul> </li> <li> <p>FactorDaily's piece on The great rush to data sciences in India ends with a direct quote from me. </p> <ul> <li>FactorDaily is a new age news company which sits at the intersection of technology with life, culture and society in India.</li> </ul> </li> <li> <p>First Runner's Up at the Future Group Datathon (March 2019)</p> <ul> <li>Two stage Machine Learning hackathon called Tathastu, working on recommendation systems and item information extraction problems</li> </ul> </li> <li> <p>Opened AI Hackathon (2019)</p> <ul> <li>Awesome NCERT won the Best use of IBM Watson API; blog</li> <li>Idea: Find recent+relevant news articles against any NCERT chapter in sciences and social studies</li> </ul> </li> </ol>"},{"location":"endorsed/","title":"Endorsed Friends","text":"<p>Last Updated: July 2025</p> <p>Talented friends I can endorse for their work -- either because I've worked with them or know them socially.</p> <p>I'm happy to connect you with them if you're looking to hire or collaborate with them, with double opt-in from both sides.</p>"},{"location":"endorsed/#endorsed-engineering-talent","title":"Endorsed Engineering Talent","text":"Name Skills Remarks/Notes Endorsed By Rohit Bhardwaj Data Engineer Worked with him at verloop.io, solving analytics problems Nirant Harsh Gupta Generalist Backend engineer, Search, LLMs, Data Augumentation and Pipeline Ex-JPMC, Helpshift, GSoC mentor and student. Built felvin.com one of the first no-code tools for building AI applications, raised angel investments for it. Know him socially. Nirant Dhruv Anand Search Consultant, VectorDB &amp; LLM ex-CMU, Google &amp; Meta Know him via the GenerativeAI Community Nirant Kumar Shivendu Product engineer with interest in Devtools, Search &amp; LLMs Builds auth devtools at SuperTokens, he was the dev behind this blog. Update: Has joined Qdrant.tech after the endorsement! Nirant"},{"location":"endorsed/#endorsed-non-engineering-talent","title":"Endorsed Non-Engineering Talent","text":"Name Skills Remarks/Notes Endorsed By Eepsita Gupta SaaS Positioning Has worked with the likes of Capillary, Vymo, DHL. Know her socially Nirant Nivedita Prabhu Brand, Advertising &amp; Marketing Mollen-Lowe Lintas, Ogilvy &amp; inMobi alum, dear friend Nirant Shubhi Saxena Engineer-turned-early-Product: PLG, Pre-Sales, Sales or Marketing Was an anchoring and energetic friend for the GenerativeAI DeepHack which we did together. Update: Has joined Quizizz in the Product Management Organization Nirant Preet Singh Khalsa Generalist Extraordinaire Ex-CRED, Accel, Update: Has moved to VC firm EQT in Europe after the endorsement Nirant <p>You can request addition and edits to this page by sending a PR to this file</p>"},{"location":"endorsed/#speakers-mentors-and-jury-for-generativeai","title":"Speakers, Mentors and Jury for GenerativeAI","text":"<p>These are folks whose functional skills have been endorsed by me. I don't have a personal relationship with all of them, but I've seen their work and can vouch for their skills. Please feel free to reach out to them for speaking, mentoring or judging opportunities.</p> Name Organisation Designation Social Link Usha Rengaraju Exa Protocol Chief Data Scientist LinkedIn Twitter Srujana Merugu Amazon Principal Data Scientist LinkedIn Soumyadeep Mukherjee Dashtoon Co-Founder LinkedIn Soumendra Dhanee VJT Inc Chief Scientist LinkedIn Shubha Shedthikere Swiggy Senior Manager - Data Science LinkedIn Shruti Rijhwani Google AI Research Scientist LinkedIn Shreya Jain Simpl Principal Product Manager LinkedIn Shankar Natarajan Microsoft Principal Product Manager LinkedIn Priyanka Bhatt Walmart Global Tech India Senior Manager II, Data Science, Conversational AI LinkedIn Pratyush Kumar SarvamAI Co-founder LinkedIn Pratik Desai KissanAI Co-founder Twitter Mansi Gupta Google Workspace Software Engineer on LLMs LinkedIn Manisha Raisinghania LogiNext Co-Founder LinkedIn Lavanya Tekumalla AiFonic Labs Co-founder LinkedIn Lalit Pagaria Oraika Co-Founder LinkedIn Heer Shingala Independent Consultant Marketing Consultant LinkedIn Heena Bansal InMobi Senior Data Scientist LinkedIn Dr. Sumod Mohan AutoInfer Co-Founder LinkedIn Divya Tak Joyous Studio Co-founder/Creative Artist LinkedIn Dev Aggarwal Gooey.ai Co-founder LinkedIn Chirasmita Mallick G2 Principal Data Scientist LinkedIn Email Aparna Gupta Microsoft Principal Software Engineer Manager LinkedIn"},{"location":"community/","title":"Generative AI Community: General Guide","text":""},{"location":"community/#rules-resources-and-recommendations-for-the-generative-ai-community","title":"Rules, Resources and Recommendations for the Generative AI Community","text":""},{"location":"community/#about","title":"About","text":"<p>Our community is a vibrant nexus connecting over 2,000 AI enthusiasts, predominantly in Bangalore and San Francisco. It's a congregation of high-impact professionals including CTOs, senior ML engineers, and open-source hackers from influential projects and startups like Guardrails, Ragas, Llama Index, Qdrant, FastEmbed, KissanAI, Sarvam, Portkey, and more.</p> <p>Each month, our community engages over 200 active members. We discuss topics ranging from compute cost improvements via vLLM to latency and evaluations. </p> <p>As a team, we have done monthly events since February, which include talks, demo days, hackathons, and a 24-hour hackathon. Our speakers have included industry luminaries like Amod Malviya (Flipkart/Udaan) and Prof. Pratyush (AI4Bharat/Sarvam), offering invaluable insights into the latest AI advancements.</p> <p>Our community's impact is evident in the success stories it has nurtured. Ragas has evolved into a business and recently got into YCombinator. Previous speaker Ravi Theja found a significant roles at Llama Index, while collaborator Shivendu Kumar joined Qdrant. Notably, FastEmbed itself was born out of the community's articulated needs, exemplifying our commitment to fostering real-world AI solutions.</p> <p>In essence, our community is not just a forum for discussion but a melting pot for AI Engineering innovation and collaboration. You can join us on WhatsApp here.</p>"},{"location":"community/#whatsapp-community-rules","title":"WhatsApp Community Rules","text":"<p>\u2705 - Please do these behaviours!</p> <p>\u274c - These may result in bans</p> <p>\ud83d\udc4e - we discourage these behaviours, but context matters</p> <p>These guides are good rules of thumbs for behaviour, but admins reserve all rights to remove or ban members or conversations. </p>"},{"location":"community/#content-sharing","title":"Content Sharing","text":"<p>\u2705 Things that you find interesting, curious or noteworthy around AI research, philosophy, implementation and future. Also new tools or how you\u2019re using them. </p> <p>\u2705 Give context when sharing if possible, or ask questions, especially if you are looking for a specific kind of help</p> <p>\u274c Self promotion of your products, your company or your events. Group should not be used as your marketing channel. </p> <p>\ud83d\udc4e Sharing paid/paywalled content. You can describe if something is relevant to the discussion but sharing this content is discouraged</p>"},{"location":"community/#adding-friends","title":"Adding Friends","text":"<p>\u2705 Share the group link with your friends and ask them to apply: https://nas.io/the-generativeai-group</p> <p>\ud83d\udc4e Asking on main to add your friend. Ping admins directly. </p>"},{"location":"community/#general-conversation","title":"General Conversation","text":"<p>\u2705 Ask questions, especially follow ups if a conversation or topic intrigues you</p> <p>\u2705 Share knowledge that you have with the community members, we can only grow the AI community in India if we help each other out</p> <p>\u2705 Act in good faith and with integrity. When in doubt ask</p> <p>\u2705 Disclose any conflicts of interests in a particular conversation</p> <p>\u274c Conversations about topics with proven harm technologies like deep fakes, surveillance, or discrimination promoting tech</p> <p>\u274c Harassment or abuse, targeted towards a group or an individual</p> <p>\u274c Inactive members. If you are inactive for more than 60 days, you will be removed from the group</p> <p>\ud83d\udc4e Off topic discussions. Avoid spamming others\u2019 inboxes with messages irrelevant to the group. Take those discussions to DMs</p> <p>\ud83d\udc4e Sensationalism and exaggerations. </p> <p>\ud83d\udc4e Don\u2019t ask to just ask. Add context e.g. what you've tried, what can folks help with</p>"},{"location":"community/#how-can-i-promote-my-productblogevent-on-group","title":"How can I promote my product/blog/event on group?","text":"<p>\u2705 Have someone else share it on the group</p> <p>\u274c Promote your own product/blog/event</p>"},{"location":"community/#resources","title":"\ud83d\udcda Resources","text":""},{"location":"community/#hackathon-and-jury-recommendations","title":"Hackathon and Jury Recommendations","text":"<ul> <li>Mentors and Jury from India</li> </ul>"},{"location":"community/#finances-expenses","title":"Finances &amp; Expenses","text":"<p>All our budget and expenses are public. You can see them here:\u00a0Generative AI Finances</p>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>We enforce the Hasgeek Code of Conduct -- and by being part of our communications e.g. WhatsApp group or attending any of ever events, you agree to adhere to the same. In case of violations, please report to coc {at} nirantk (dot) com </p>"},{"location":"community/#data-usage-messages-in-the-group-may-be-made-public-or-used-for-training-a-bot-we-cannot-prevent-members-from-downloading-chat-history","title":"\u26a0\ufe0f Data Usage: Messages in the group may be made public or used for training a bot. We cannot prevent members from downloading chat history.","text":""},{"location":"community/#generativeai-startup-ecosystem-for-founders-and-investors","title":"GenerativeAI Startup Ecosystem for Founders and Investors","text":"<p>Description: This Gen AI group is strictly for founder + Investors only, if you are not one of these you can be removed by admins anytime without prior acknowledgment. When you join please drop 3-4 line introduction about you and/or your product along with LinkedIn link for other members to connect with you.</p> <p>Read these Startup Group rules first</p> <p>For adding new members, please reach out to Nisha from SuperU on WhatsApp</p> <p>For promoting any upcoming event, please reach out to Aditya on WhatsApp</p>"},{"location":"community/#community-moderators","title":"Community Moderators","text":"<p>The community is run with sincere and ongoing efforts from the following people: </p>"},{"location":"community/#ai-for-creatives","title":"AI for Creatives","text":"<ul> <li>Computational Mama </li> <li>Divya Tak</li> </ul>"},{"location":"community/#women-in-ai","title":"Women in AI","text":"<ul> <li>Pratiksha Dake - Founder, Graphia</li> <li>Navita Pareek - Founder, InstaPhotoAI</li> </ul>"},{"location":"community/#agents-and-tool-usage","title":"Agents and Tool Usage","text":"<ul> <li>Sachin Dharashivkar - Founder, AthenaAgent</li> <li>Vignesh Baskaran - Co-founder, HexoAI</li> </ul>"},{"location":"community/#expertise-and-advisory-board-aka-moderators","title":"Expertise and Advisory Board aka Moderators:","text":"<p>These are the folks who pitch in ideas, help with moderation, lend their expertise and time to the community. This is a voluntary role and I'm deeply grateful for their time and effort: </p> <ol> <li>Computational Mama, Creative Extraordinaire</li> <li>Dr. Pratik Desai - CEO, KissanAI</li> <li>Divya Tak - Co-founder @joyus.studio || Product and Game Design</li> <li>Shubhi Saxena - Product at Quizizz</li> <li>Ravi Theja - Open Source Contributor, Llama Index</li> <li>Pratyush Choudhury, AI SaaS &amp; Infrastructure Investor @ Together Fund</li> <li>Ojasvi Yadav, AI Lead, WriteSonic</li> <li>Dev Aggarwal, Technical Founder at Gooey.AI</li> <li>Kaushik Bokka, Stealth Founder, ex-LightningAI, Fynd</li> <li>Sudharshan Chandra Babu, Co-founder, FrontierAI</li> <li>Sumod Mohan - CEO, AutoInfer</li> <li>Rahul Bhatnagar - Founder, Kidovo</li> <li>Kumar Shivendu - Systems Engineer, Qdrant</li> <li>Aditya Agrawal - Co-founder, SuperU, ex-Tesla</li> <li>Abhishek Mishra - Technical Lead, Intel -- Resident LLM Finetuning Expert</li> <li>Sandesh Anand - Co-founder, Seezo, ex-Razorpay</li> <li>Aryaman Behera - Co-founder, Repello AI</li> <li>Anshuman Pandey - CEO, NimbleBox.ai</li> <li>Pratiksha Dake - Founder, Graphia</li> <li>Navita Pareek - Founder, InstaPhotoAI</li> <li>Sachin Dharashivkar - Founder, AthenaAgent</li> <li>Vignesh Baskaran - Co-founder, HexoAI</li> <li>Dhruv Anand - Search Consultant, VectorDB &amp; LLM</li> <li>Soumyadeep Mukherjee - Co-founder and Engineer, Dashtoon</li> </ol>"},{"location":"community/#founding-members","title":"Founding Members:","text":"<p>These are the co-conspirators which provide the emotional, mental fuel to keep this community going. They are the ones who helped me get this off the ground. I'm deeply grateful for their support:</p> <ul> <li>Soumyadeep Mukherjee - Co-founder and Engineer, Dashtoon</li> <li>Divya Tak - Co-founder @joyus.studio || Product and Game Design</li> <li>Zainab Bawa - COO, Hasgeek</li> <li>Pranjal Mehta - Co-founder, Stealth (inactive in community)</li> </ul> <p>On behalf of the community, I'd like to thank all of the above for their time and effort. - Nirant Kasliwal - AI Engineer at Qdrant</p>"},{"location":"community/deephackdemos/","title":"Generative AI Hack Demos","text":"<p>\ud83d\udc4b You've landed at the perfect spot to check out all the coolest demos from the #GenerativeAIHack.</p> <p>\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 90+ hackers on Day 1, 150+ folks at Demo day</p> <p>\u26a1\ufe0f 43 submissions, 32 in-person, 11 remote from 6 cities</p> <p>\ud83d\ude4f 13 sponsors from 3 countries, 2 Open Source Software companies: Weaviate, AppSmith</p> <p>\ud83e\udd17 20% raised from individual donors or collectives of them</p> <p>\ud83d\udcb0 5 Lakh INR in Prizes</p> <p>I'm super stoked to share these mind-blowing projects with you\u2014created by some seriously talented folks from all corners of the world. \ud83c\udf0d Whether you're into art, gaming, or just love a good tech innovation, we've got something for everyone.</p> <p>All demos have been shared with creator's permission! \ud83d\ude80</p>"},{"location":"community/deephackdemos/#links","title":"Links","text":"<ul> <li>Livestream: https://www.youtube.com/live/E5gpisqhwJc?feature=share</li> <li>Hackathon Website: https://has.gy/gpt4hack </li> </ul>"},{"location":"community/deephackdemos/#prize-winners","title":"Prize Winners","text":""},{"location":"community/deephackdemos/#best-tooling-for-media-multimode-text-to-pixel-animation-using-stable-diffusion","title":"Best Tooling for Media: Multimode: Text to Pixel Animation using Stable Diffusion","text":"<p>Demo Video</p> <p>Multimode helps you generate game assets like sprite animations, tilable textures and more from a text prompt so that you can build you game faster.</p>"},{"location":"community/deephackdemos/#ai-for-good-sensai-learning-redefined","title":"AI For Good: Sens.AI: Learning Redefined","text":"<p>Demo Video</p> <p>Sens.AI redefines learning by providing personalized experiences for young people from underserved backgrounds. The platform creates small batches of learners and pairs them with mentors to bridge skill gaps, develop professional communication and critical thinking skills, and offer tailored guidance.</p>"},{"location":"community/deephackdemos/#best-gpt-powered-app-youtalk-search-and-discover-your-youtube-snippets","title":"Best GPT Powered App: YouTalk: Search and Discover Your YouTube Snippets","text":"<p>Demo Video</p>"},{"location":"community/deephackdemos/#best-ai-devtool-osh-llm-based-helper-and-assistant-for-command-line","title":"Best AI Devtool: osh: LLM based helper and assistant for command line","text":"<ul> <li>GitHub: https://github.com/harshalbhatia/osh</li> <li>Pitch Deck: https://pitch.com/public/2a735687-436f-4af6-99a7-ffcc6c0666d9</li> <li>Demo Video: https://youtu.be/PiEJ5S071xk</li> </ul> <p>osh revolutionizes the command line experience by providing LLM-based assistance for developers. The tool simplifies workflows and enhances overall productivity, allowing users to navigate errors, and debug complex programs with ease and confidence.</p>"},{"location":"community/deephackdemos/#tale-of-datta-short-comic-about-the-mythology-of-data-ai","title":"Tale of Datt\u0101: Short Comic about the Mythology of Data &amp; AI","text":"<p>Demo Video | Process from the Artist</p> <p>Tale of Datt\u0101 is a short comic exploring the mythology of data and AI, offering a creative and engaging perspective on the world of technology.</p>"},{"location":"community/deephackdemos/#jurys-choice","title":"Jury's Choice","text":""},{"location":"community/deephackdemos/#inspir-accessible-and-engaging-learning-for-all","title":"Inspir: Accessible and Engaging Learning for All","text":"<p>Demo Video</p> <p>Inspir is a learning app designed to make education accessible and engaging for learners of all ages and backgrounds. It uses natural language processing (NLP) technology to enable users to ask questions and receive detailed, informative responses in real-time. Inspir aims to revolutionize education by democratizing learning and making it accessible to everyone.</p>"},{"location":"community/deephackdemos/#apne-app","title":"Apne App","text":"<p>Demo link with multiple demos</p> <p>Nishant Nikhil and his teammate built a micro-app generator/builder with a fully featured front end and backend which adapted to developer requests. Think of disposable workflows, but mostly for development since errors in code still need technical skill.</p>"},{"location":"community/deephackdemos/#nirants-picks","title":"Nirant's Picks","text":""},{"location":"community/deephackdemos/#viewsact","title":"ViewsAct","text":"<p>JS library which can interface with (almost) any SaaS and Consumer application to expose a new JS component of user's desire. Think of Cmd+K on steroids with disposable software. No dev skills required. No demo video.</p>"},{"location":"community/deephackdemos/#genie-paint-stunning-artwork-from-a-sketch","title":"Genie Paint: Stunning Artwork from a Sketch!","text":"<p>Demo Video</p> <p>Genie Paint inspires kids by showcasing the capabilities of AI in amplifying their creativity, transforming sketches into stunning artwork.</p>"},{"location":"community/deephackdemos/#blindvisaidgpt-an-interactive-aid-for-the-visually-impaired","title":"BlindVisAidGPT: An Interactive Aid for the Visually Impaired","text":"<p>GitHub | Demo Video</p> <p>BlindVisAidGPT aims to provide a helpful interface for visually impaired individuals by using a Wi-Fi enabled camera mounted on glasses. The project is built on Microsoft's Visual-ChatGPT, and it provides an interactive platform for users to ask questions about the environment in front of them. With multiple avenues for improvement, this project is open for contributions from the open-source community.</p>"},{"location":"community/deephackdemos/#all-demos","title":"All Demos","text":""},{"location":"community/deephackdemos/#consumableai-personalized-learning-made-easy","title":"Consumable.AI: Personalized Learning Made Easy","text":"<p>Demo Video</p> <p>Consumable.AI is an educational tool designed to make learning more accessible and efficient. By generating short, personalized notes based on keywords, users can quickly consume information tailored to their interests. This innovative approach to education streamlines the learning process and promotes a more engaging experience for students.</p>"},{"location":"community/deephackdemos/#codeau-adding-and-updating-code-base","title":"CodeAU: Adding and Updating code base.","text":"<p>Demo Video</p> <p>CodeAU, an innovative and robust coding assistant framework designed to empower developers by seamlessly updating and expanding their existing code base. As libraries evolve and new versions are released, it becomes increasingly challenging for developers to keep up with the changes and ensure their code remains functional and efficient. CodeAU is here to bridge the gap and effortlessly maintain the code's compatibility with the latest library versions.</p>"},{"location":"community/deephackdemos/#parable-turning-text-into-insight","title":"Parable: Turning Text into Insight","text":"<p>GitHub | Demo Video</p> <p>Parable is a text-to-insight tool designed to uncover the hidden stories within large volumes of text. By analyzing and interpreting text data, Parable provides valuable insights and understanding to users. This powerful tool has the potential to revolutionize industries that rely heavily on text-based information, such as journalism, research, and business analytics.</p>"},{"location":"community/deephackdemos/#aicombinator-your-personal-ai-assistant-for-complex-online-tasks","title":"AICombinator: Your Personal AI Assistant for Complex Online Tasks","text":"<p>GitHub | Demo Video</p> <p>AICombinator is an open-source project that aims to give AI identity, memory, and agency. This bot can log in as you and perform complex operations across multiple websites in a real browser, including filling in OTPs, making payments, and accessing cloud services. With its ability to run locally, AICombinator can provide file-system access, native app usage, and more, making it an indispensable tool for end-users.</p>"},{"location":"community/deephackdemos/#hiring-assistant-streamlining-the-recruitment-process","title":"Hiring Assistant: Streamlining the Recruitment Process","text":"<p>GitHub | Demo Video</p> <p>Hiring Assistant is an AI-powered solution designed to streamline the recruitment process. By using WhatsApp as a digital channel for resume submission, the AI extracts resume data and processes it through ChatGPT to assess skills, compatibility scores, and job description matches. The platform also generates sample interview questions and sends email notifications to HR personnel and candidates. This reduces the time-consuming and error-prone manual recruitment process to mere minutes.</p>"},{"location":"community/deephackdemos/#edden-effortlessly-create-landing-pages","title":"Edden: Effortlessly Create Landing Pages","text":"<p>Website | Demo Video</p> <p>Edden is an AI-driven tool that enables users to create landing pages within minutes. By simplifying the design process, Edden allows users to focus on their core business objectives while still achieving professional and visually appealing landing pages.</p>"},{"location":"community/deephackdemos/#shakalaka-boom-boom-jo-ye-banaye-sach-ho-jaye","title":"Shakalaka Boom Boom: Jo ye banaye sach ho jaye!","text":"<p>Website | Demo Video</p> <p>Discover \"Shakalaka Boom Boom\": Inspired by the nostalgic Indian TV show, this innovative entertainment tool transforms your doodles into realistic images using advanced AI. Shakalaka Boom Boom also suggests e-commerce products resembling your creations, making it the perfect fusion of imagination and shopping. Relive the magic of your childhood and watch your artistic dreams come to life with Shakalaka Boom Boom!</p>"},{"location":"community/deephackdemos/#diagramgenie-text-to-software-engineering-diagrams","title":"DiagramGenie\ud83d\udd2e: Text to Software Engineering Diagrams","text":"<p>Website | Github | Demo Video</p> <p>DiagramGenie is an innovative tool that allows users to create software engineering diagrams by just describing it in a few simple words. This AI-powered solution simplifies the process of creating complex diagrams and streamlines the workflow for software engineers and developers.</p>"},{"location":"community/deephackdemos/#student-mentor-medical-literature-analysis-and-career-guidance","title":"Student Mentor: Medical, Literature Analysis, and Career Guidance","text":"<p>GitHub | Demo Video</p> <p>Student Mentor is an AI-powered platform designed to assist students in various aspects of their academic and professional lives. With features like medical, literature, and career question answering, summary generation, and chat facilities, Student Mentor provides comprehensive support for students seeking guidance and assistance.</p>"},{"location":"community/deephackdemos/#debate-bot-an-ai-powered-tool-for-rational-discussions","title":"Debate-Bot: An AI-Powered Tool for Rational Discussions","text":"<p>Debate-Bot is an AI-powered tool that analyzes online arguments and points out logical fallacies. By providing users with a deeper understanding of the arguments they encounter, Debate-Bot encourages more informed and rational discussions.</p>"},{"location":"community/deephackdemos/#anti-chatgpt-an-antivirus-for-your-brain","title":"Anti-ChatGPT: An Antivirus for Your Brain","text":"<p>GitHub | Demo Video</p> <p>Anti-ChatGPT is a Chrome plugin designed to combat the manipulation of AI-generated content. Users can specify what content they don't want to see, and the plugin removes it from their browser, empowering individuals to take control of their digital experiences.</p>"},{"location":"community/deephackdemos/#anywhere-gpt-unleashing-chatgpts-superpowers-on-the-web","title":"Anywhere GPT: Unleashing ChatGPT's Superpowers on the Web","text":"<p>Demo Video</p> <p>Anywhere GPT is a powerful tool that brings ChatGPT's capabilities to any website, providing real-time writing assistance, language translation, chatbot and virtual assistant support, and more. By integrating AI functionality into everyday browsing, Anywhere GPT enhances the overall user experience.</p>"},{"location":"community/deephackdemos/#wingman-ai-enhanced-dating-profiles","title":"Wingman: AI-Enhanced Dating Profiles","text":"<p>Demo Video</p> <p>Wingman is an AI-powered app that helps users create compelling dating profiles. By leveraging AI's language generation capabilities, Wingman ensures users present their best selves in their online dating endeavors.</p>"},{"location":"community/deephackdemos/#codewiz-effortless-coding-with-ai-superpowers","title":"CODEWIZ: Effortless Coding with AI Superpowers","text":"<p>GitHub | Demo Video</p> <p>CODEWIZ is an AI-driven tool that simplifies the coding process for developers. By integrating AI capabilities directly into a code editor, CODEWIZ empowers developers to write more efficient and accurate code with ease.</p>"},{"location":"community/deephackdemos/#zima-blue-unlimited-data-at-your-fingertips","title":"Zima Blue: Unlimited Data at Your Fingertips","text":"<p>Zima Blue provides unlimited data access to underprivileged individuals, enabling them to utilize their resources effectively and create new opportunities for learning and development.</p>"},{"location":"community/deephackdemos/#sambhash-your-ai-tutor-to-practice-english","title":"Sambhash: Your AI Tutor to Practice English","text":"<p>Sambhash is an AI tutor designed to help rural individuals improve their English speaking skills. By offering suggestions, translations, and feedback, the AI tutor helps users enhance their language proficiency and gain confidence.</p>"},{"location":"community/deephackdemos/#tars-personal-ai-helping-hands","title":"TARS: Personal AI Helping Hands","text":"<p>Demo Video</p> <p>TARS is a personal AI assistant that can help with tasks such as doing your taxes or even providing some entertainment. By automating these tasks, TARS makes life easier and more enjoyable for its users.</p>"},{"location":"community/deephackdemos/#reinforcellm-train-your-own-chatgpt","title":"ReinforceLLM: Train Your Own ChatGPT","text":"<p>Demo Video</p> <p>ReinforceLLM is a platform that allows users to train their own ChatGPT models based on their personal preferences, providing a customized AI experience.</p>"},{"location":"community/deephackdemos/#ai-tele-agent-smarter-support-with-ai","title":"AI Tele Agent: Smarter Support with AI","text":"<p>Demo Video</p> <p>AI Tele Agent is an AI-based support agent capable of answering customer queries efficiently. By synthesizing user queries and generating informed responses, AI Tele Agent offers instant, efficient customer support.</p>"},{"location":"community/deephackdemos/#selene-a-platform-to-build-and-deploy-video-ai-applications-at-scale","title":"Selene: A Platform to Build and Deploy Video AI Applications at Scale","text":"<p>Demo Video</p> <p>Selene is a platform that allows developers to build and deploy video AI applications at scale. It offers an SDK and workflow creation tools, enabling developers to run their models on the cloud without worrying about infrastructure.</p>"},{"location":"community/deephackdemos/#meme-fake-self-meme-generator","title":"MEME Fake: Self Meme Generator","text":"<p>Demo Video</p> <p>MEME Fake is a self meme generator that allows users to create personalized, humorous content using AI technology.</p>"},{"location":"community/deephackdemos/#mission-impossible-silicon-deception","title":"Mission Impossible: Silicon Deception","text":"<p>Demo Video Mission Impossible: Silicon Deception is a fan-fiction comic where Ethan Hunt is recruited by Sam Altman to find stolen AI technology, offering an exciting and imaginative narrative.</p>"},{"location":"community/deephackdemos/#text-2-reel-create-your-instagram-reel-without-any-efforts","title":"Text 2 Reel: Create Your Instagram Reel Without Any Efforts","text":"<p>Demo Video</p> <p>Text 2 Reel simplifies the creation of Instagram reels, allowing users to generate engaging content with minimal effort using AI technology.</p>"},{"location":"community/deephackdemos/#glaze-ai-langchain-for-images","title":"Glaze AI: Langchain for Images","text":"<p>Glaze AI is an open-source library that enables users to chat with multiple image models and perform various tasks on images while also providing post-editing functions.</p>"},{"location":"community/deephackdemos/#gully-ai-mere-gully-ai-main","title":"Gully AI: Mere Gully AI Main","text":"<p>Demo Video</p> <p>Gully AI offers a creative and engaging platform for users to generate and share AI-assisted content, enhancing the user experience and promoting collaboration.</p>"},{"location":"community/deephackdemos/#explore-and-search-for-any-website-gpt-for-advanced-browser-search-and-exploration","title":"Explore and Search for Any Website: GPT for Advanced Browser Search and Exploration","text":"<p>Demo Video</p> <p>This Chrome extension allows users to ask questions and receive personalized search results based on their preferences, mood, or requirements, simplifying the search and exploration process.</p>"},{"location":"community/deephackdemos/#sneaker-studio-canva-for-sneakers","title":"Sneaker Studio: Canva for Sneakers","text":"<p>Demo Video</p> <p>Sneaker Studio is a platform that allows users to design and customize their own sneakers, offering a creative and personalized experience.</p>"},{"location":"community/deephackdemos/#bookgistin","title":"Bookgist.in","text":"<p>Demo Video</p> <p>Bookgist.in is an AI-powered platform that aims to promote social impact by providing users with easy access to book summaries, facilitating learning and personal growth.</p>"},{"location":"community/deephackdemos/#video-ad-generator-make-a-video-ad-of-your-product-by-just-entering-a-prompt","title":"Video Ad Generator: Make a Video Ad of Your Product by Just Entering a Prompt","text":"<p>Demo Video</p> <p>Video Ad Generator enables users to create full video ads using just a text prompt, streamlining the ad creation process and saving time.</p>"},{"location":"community/deephackdemos/#handmade-with-ai-a-series-of-surreal-artworks-focused-on-human-hands","title":"Handmade with AI: A Series of Surreal Artworks Focused on Human Hands","text":"<p>Handmade with AI is a series of surreal artworks created with the assistance of Stable Diffusion and Midjourney AI tools, highlighting the potential of AI in artistic expression.</p>"},{"location":"community/demoday102023/","title":"Details of Demos","text":"<p>Here is a list of all the teams/folks who presented a demo on the 7<sup>th</sup> October 2023, Microsoft Office, Bangalore. Thanks to Divya Tak, Soumyadeep Mukherjee for hosting a great event and Microsoft for sponsoring the venue.</p> <ol> <li> <p>Sachin Dharashivkar : WebSentryAgent</p> <ul> <li>Email : sachin.dharashivkar@gmail.com</li> <li>About the project : Product is an intelligent web application vulnerability scanner. We ask attacker intent and target website and show potential vulnerabilities to the user to patch.</li> <li>What they are looking for : Users (penetration testers / cybersecurity engineers) to get feedback and looking to hire cybersecurity engineers.</li> </ul> </li> <li> <p>Neel Balar : Clueso</p> <ul> <li>Email : neel@clueso.io</li> <li>Link : https://www.clueso.io/</li> <li>About the project : Clueso is an AI-powered product education platform for SaaS companies. It transforms basic screen captures of your product features into rich, studio-grade explainer videos and step-by-step guides. It's for SaaS companies to create help center for their products.</li> <li>What they are looking for : Not looking to raise funds or hire at the moment. But would love to connect with the SaaS companies in the community.</li> </ul> </li> <li> <p>Rajaswa Patil : Postbot</p> <ul> <li>Email : rajaswa.patil@postman.com</li> <li>Link : https://blog.postman.com/introducing-postbot-postmans-new-ai-assistant/</li> <li>About the project : You can use Postbot as your AI companion in Postman workspaces, helping you debug and understand APIs, write tests faster, and make sense of large quantities of data. It allows you to Generate Tests for your API Requests &amp; Collections, Fix or Edit any existing tests, Visualize your API responses, and ask any questions you might have for using the Postman platform.</li> <li>What they are looking for : Looking to hire.</li> </ul> </li> <li> <p>Jithin James : ragas</p> <ul> <li>Email : jamesjithin97@gmail.com</li> <li>Link : https://github.com/explodinggradients/ragas</li> <li>About the project : ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines.</li> <li>What they are looking for : looking for design partners, i.e., companies who are building RAG pipelines and need help with evaluations, monitoring, user feedback etc. we would love to work with them.</li> </ul> </li> <li> <p>Naresh R - codestory</p> <ul> <li>Email : nareshmdu@gmail.com</li> <li>Link : https://codestory.ai/</li> <li>About the project : CodeStory is an AI-powered mod of VSCode. Imagine a developer tasked with solving a bug, CodeStory can scan the codebase, identify the root cause, make the fix and test it's changes to ensure the problem is fixed.</li> <li>What they are looking for : looking for a small team working on python/typescript to do a pilot with.</li> </ul> </li> <li> <p>Ayushman Buragohain : Dashtailor via Dashtoon</p> <ul> <li>Email : ayushman@dashtoon.com</li> <li>Link : https://studio.dashtoon.ai/</li> <li>About the project : Dashtoon studio helps you create your own comics using diffusion models, and dashtailor is being built to enable you to change attire of your characters, while maintaining consistency.</li> <li>What they are looking for : Fast growing team looking for engineers.</li> </ul> </li> <li> <p>Amir Nagri - Bodhilib</p> <ul> <li>Email : amir.nagri@gmail.com</li> <li>Link : https://bodhilib-guide.readthedocs.io/</li> <li>About the project : Bodhilib is an open source, plugin architecture based, pythonic and composable LLM library.</li> <li>What they are looking for : Open source contributors.</li> </ul> </li> <li> <p>Chintan Papiya - KissanAI</p> <ul> <li>Email : chintan.papaiya@titodi.com</li> <li>Link : https://kissan.ai</li> <li>About the project : KissanAI is an advanced multilingual AI CoPilot engineered to provide farmers with personalized, voice-based assistance for all their agricultural needs.</li> <li>What they are looking for : Looking for contributors and fund raising.</li> </ul> </li> </ol>"},{"location":"writing/","title":"Writing","text":"<p>Hello, I'm Nirant. I write about a mix of AI consulting, open source, personal work, and applying llms. </p> <p>I won't email you more than twice a month, not every post I write is worth sharing but I'll do my best to share the most interesting stuff including my own writing, thoughts, and experiences.</p> <p> Never miss what I'm writing</p>"},{"location":"writing/Minerva/","title":"Airbnb's Metric Store: Minerva","text":"<p>Data lineage is a problem because most companies have several tables and queries before humans consume it!</p> <p>This has well known challenges: changes do not propagate downstream from the source, and reliable (fresh, updated or complete) data is not always available.</p> <p></p>"},{"location":"writing/Minerva/#what-does-minerva-do","title":"What does Minerva do?","text":"<p>I was expecting Minerva to a database (collection of tables), but it turns out that Minerva is what I'll call: Data Transformation Manager.</p> <p>It overlaps quite a bit with <code>dbt</code> but it's not a pure execution layer. It also stores metadata, orchestrates the DAG itself, and provides a way to query the data (Hive/Druid here)</p> <p></p> <p>Minerva solves for one major problem in the analytics space: Time to insights \u2014 as [[Julie Zhuo]] has mentioned several times at Sundial</p> <p></p>"},{"location":"writing/Minerva/#minerva-10","title":"Minerva 1.0","text":"<p>This is a bit preview about the past and what problems did they first solve, what was left undone and some tooling/technology choices.</p>"},{"location":"writing/Minerva/#pre-computation-engine","title":"Pre-computation engine","text":"<p>Quite similar to how we were building Sundial till very recently.</p> <ol> <li>De-normed measures and dimensions</li> <li>Segments and behaviours were both separated out</li> </ol> <p>De-norming measures can be quite expensive but useful. We converged to this design across multiple clients while working with Event-level data. We also see some of our clients maintaining a similar table: \"User Segments Table\".</p> <p></p>"},{"location":"writing/Minerva/#tradeoffs-in-the-precomputing-approach","title":"Tradeoffs in the Precomputing Approach","text":"<ol> <li>Cubing \u2014 SQL Minerva already knows what SQL query to run, across what segments and what durations upfront. This means it can leverage CUBE operations. </li> </ol> <p>Some people believe that OLAP CUBE has fallen out of use, but that's clearly not true here. As companies get larger, \"old\" pressures on compute and storage should re-appear and so should already known solutions like cubing. </p> <ol> <li> <p>Fast Query Time: Since the results are precomputed - fast at query time</p> </li> <li> <p>Exponential in query cost. Backfill \u2014 damn expensive and wastes time and money</p> </li> </ol> <p></p> <p>Everything has to be asked ahead of time, so you end up calculating too many things</p>"},{"location":"writing/Minerva/#minerva-20","title":"Minerva 2.0","text":"<p>This is what a truly \"modern\" data transformation manager should look like in my opinion.</p> <p>Here are some of the design choices:</p> <ol> <li>On the fly joins</li> <li>On the fly aggregations</li> <li>Optional denorm and cubing</li> <li>Enable precompute to be turned on </li> </ol> <p>The way I see it, this is striking a balance between flexibility (the ability to do on-the-fly joins and aggregations) and cost (the ability to precompute with denorm and cubing).</p>"},{"location":"writing/Minerva/#engineering-choices","title":"Engineering Choices","text":""},{"location":"writing/Minerva/#moved-from-druid-to-starrocks","title":"Moved from Druid to StarRocks","text":""},{"location":"writing/Minerva/#why-starrocks","title":"Why StarRocks?","text":"<p>Minerva is SQL generation tool, not a Druid ingestion tool</p> <p>Minerva has a SQL Interface now, early was JSON</p> <p></p> <p>SQLGlot \u2014 Python SQL Parser and Transpiler -- this is very similar to <code>dbt</code> for how it generates SQL using a parser and transpiler. SQLGlot is open source btw: https://github.com/tobymao/sqlglot</p> <p></p>"},{"location":"writing/Minerva/#near-real-time-metrics","title":"Near Real Time Metrics","text":"<p>Summary of changes made for 2.0 release</p> <p></p>"},{"location":"writing/Minerva/#major-changes-is-that-sql-is-now-a-first-class-citizen","title":"Major changes is that SQL is now a first class citizen","text":"<p>This is quite important. We should resist the temptation of inventing a Python transformation layer/logic. While some Python is inevitable for doing more interesting things like forecasting, using Python for calculating ratios is a bit overkill. We should instead try and consider pushing the limits of SQL for the same. </p> <p>SQL is not only more widely spoken, it'd be a lot more efficient and more scalable. The downside? It's less general purpose language, and we'd have to write some tooling to make SQL work like Python.</p> <p>These are some notes and screen grabs from a talk which I'd found on Youtube. Thanks to Kunal Kundu for finding the talk link which I'd lost!</p>"},{"location":"writing/actlike35/","title":"Act Like You're 35","text":""},{"location":"writing/actlike35/#dont-just-network-with-people-your-own-age","title":"Don\u2019t just network with people your own age","text":"<p>Beware the whiz kid syndrome. Smart, young people have a habit of forming communities of other smart young people and feeding off each other\u2019s energy. Argghhhh. Are you not just making mistakes made by older people again? Make original mistakes </p> <p>Youth enclaves can actually be restrictive. In fact their networking should be about meeting useful mentors and career champions who can open doors and fast track careers. </p> <p>Some of the best functional leaders in India are now in their 40s. They built India\u2019s telecom and Internet 2.0 (Flipkart, Amazon) and are looking for ways to give back. You at 20-something is ideal fire for them to light. </p> <p>Learn to build a network. If you are in SaaS, Go to SaaS events.</p> <p>Learn from your seniors at ChargeBee, CleverTap, Haptik, Freshworks, WhatFix, BrowserStack. Don\u2019t be a moron on an island refusing help. Shout. Ask for help. If not, hustle.</p>"},{"location":"writing/actlike35/#take-the-time-to-understand-what-your-business-does","title":"Take the time to understand what your business does","text":"<p>I love the story of President John F. Kennedy\u2019s visit to NASA during which he asked a cleaner what his job was. The cleaner replied that he sent rockets to the moon. </p> <p>All of us should feel part of what we do at work. We might not have an exact wording or understanding yet. That is even better, we get to co-invent this on the journey!  </p> <p>Don\u2019t wait for someone to tell you or lament that internal communication is crap. Find out for yourself.</p>"},{"location":"writing/actlike35/#never-sacrifice-personal-ethics-for-a-work-reason","title":"Never sacrifice personal ethics for a work reason","text":"<p>If you work on something that compromises your personal ethics and values, get out of there as quickly as you can. </p> <p>Good people will be unnerved by things that don\u2019t feel right. If it doesn\u2019t feel right, it probably isn\u2019t. Bad things only manifest when good people don\u2019t take a stand.</p>"},{"location":"writing/actlike35/#most-success-comes-from-repetition-not-new-things","title":"Most success comes from repetition, not new things","text":"<p>Malcolm Gladwell\u2019s brilliant book Outliers, promotes the idea that you needed to spend 10,000 hours on something to become truly expert at it. This applied to the Beatles and their Hamburg gigs and Bill Gates who, through a series of fortuitous accidents, ended up spending more time than almost anyone else on a computer.</p> <p>I want to call bullshit on that. People forget that Bill Gates was among the best mathematicians of his age. Mark Zuckerberg was among the top 20% programmers and psychologists and CEOs of his time. The best idea is to be a triple threat.</p> <p>The lesson here is to get good at few (greater than one) things before you try to move to the next thing. Genuine expertise belongs to an elite few. They seldom have superpowers. They usually have endurance, patience and take a long-term view. They also love what they do. If you find that, don\u2019t let it go.</p>"},{"location":"writing/actlike35/#in-the-workforce-always-act-like-you-are-35","title":"In the workforce, always act like you are 35","text":"<p>A recruiter gave me this advice some years ago. It is quite inspired. What she meant was, when you are young in the workplace, don\u2019t act as a novice. If you are smart and competent, step up and do whatever you are capable of doing in a mature way. Don\u2019t be afraid to ask for help. Or help people aim for a higher quality.</p> <p>Similarly, when you are an older worker, don\u2019t act like it. Approach your day with youthful energy. To quote a famous Frank Sinatra song:</p> <p>You\u2019re 35 and it\u2019s a very good year - \"Frank Sinatra\"</p> <ul> <li>(Hattip Shane Rodgers)</li> </ul>"},{"location":"writing/actlike35/#end-notes","title":"End Notes","text":"<p>I barely follow 1% of this myself, and would consider it a major personal win to get to 5% in the next 10 years (when I\u2019ll actually be 35). </p> <p>If you think I can help you in anyway: making introductions to others as mentors, figuring what to do to unlock your potential, or how to grow - please know whatever you share with me, stays with me. </p> <p>This is a contextualized essay, which riffs on and rips from:</p> <ul> <li>The Career Advice I wish I had when I was 25 - Shane Rodgers</li> <li>Principal Agent by Naval Ravikant</li> </ul> <p>Build your own playbook:</p> <ul> <li>Essential Networking Advice</li> <li>So Good They Can\u2019t Ignore You - Cal Newport</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/","title":"Agent Metrics - A Guide for Engineers","text":"<p>Measuring the performance of non-deterministic, compound systems like LLM-powered chat applications is fundamentally different from traditional software. An output can be syntactically perfect and seem plausible, yet be factually incorrect, unhelpful, or unsafe.</p> <p>A robust measurement strategy requires a multi-layered approach that covers everything from operational efficiency to nuanced aspects of output quality and user success. This requires a shift in thinking from simple pass/fail tests to a portfolio of metrics that, together, paint a comprehensive picture of system performance.</p> <p>This guide breaks down metric design into three parts:</p> <ol> <li>Foundational Metric Types: The basic building blocks of any measurement system.</li> <li>A Layered Framework for LLM Systems: A specific, hierarchical approach for applying these metrics to your application.</li> <li>Multi-Turn Chat Metrics: Specialized metrics for evaluating conversational systems beyond single-turn interactions.</li> </ol> <p>Skip Ahead</p> <ul> <li>Already familiar with metric fundamentals? \u2192 Jump to Part 2: Layered Framework</li> <li>Building conversational agents and need conversation-level metrics? \u2192 Jump to Part 3: Multi-Turn Chat</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#part-1-foundational-metric-types","title":"Part 1: Foundational Metric Types","text":"<p>These are the fundamental ways to structure a measurement. Understanding these types is the first step to building a meaningful evaluation suite.</p>"},{"location":"writing/agent-metrics-for-engineers/#1-classification-categorical","title":"1. Classification (Categorical)","text":"<p>Measures which discrete, unordered category an item belongs to. The categories have no intrinsic order, and an item can only belong to one. This is crucial for segmenting analysis and routing logic.</p> <p>Core Question: \"What kind of thing is this?\" or \"Which bucket does this fall into?\"</p> <p>Examples:</p> <ul> <li>Intent Recognition: <code>[BookFlight]</code>, <code>[CheckWeather]</code>, <code>[GeneralChat]</code>. This allows you to measure performance on a per-intent basis.</li> <li>Error Type: <code>[API_Failure]</code>, <code>[Hallucination]</code>, <code>[PromptRefusal]</code>, <code>[InvalidToolOutput]</code>. Segmenting errors is the first step to fixing them.</li> <li>Tool Used: <code>[Calculator]</code>, <code>[CalendarAPI]</code>, <code>[SearchEngine]</code>. Helps diagnose issues with specific tools in a multi-tool agent.</li> <li>Conversation Stage: <code>[Greeting]</code>, <code>[InformationGathering]</code>, <code>[TaskExecution]</code>, <code>[Confirmation]</code>.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#2-binary-boolean","title":"2. Binary (Boolean)","text":"<p>A simplified version of classification with only two outcomes. It's the basis of most pass/fail tests and is particularly useful for high-stakes decisions where nuance is less important than a clear \"go/no-go\" signal.</p> <p>Core Question: \"Did it succeed or not?\" or \"Does this meet the minimum bar?\"</p> <p>Examples:</p> <ul> <li>Task Completion: <code>[Success / Failure]</code></li> <li>Tool Call Validity: <code>[ValidAPICall / InvalidAPICall]</code>. Was the generated tool call syntactically correct?</li> <li>Contains Citation: <code>[True / False]</code>. Did the model cite a source for its claim?</li> <li>Safety Filter Triggered: <code>[True / False]</code>. A critical metric for monitoring responsible AI guardrails.</li> <li>Factually Correct: <code>[True / False]</code>. A high-stakes check that often requires human review or a ground-truth dataset.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#3-ordinal","title":"3. Ordinal","text":"<p>Similar to classification, but the categories have a clear, intrinsic order or rank. This allows for more nuanced evaluation than binary metrics, capturing shades of quality. These scales are often defined in human evaluation rubrics.</p> <p>Core Question: \"How good is this on a predefined scale?\"</p> <p>Examples:</p> <ul> <li>User Satisfaction Score: <code>[1: Very Unsatisfied, ..., 5: Very Satisfied]</code>. The classic user feedback mechanism.</li> <li>Answer Relevance: <code>[1: Irrelevant, 2: Somewhat Relevant, 3: Highly Relevant]</code>. A common human-annotated metric.</li> <li>Readability: <code>[HighSchool_Level, College_Level, PhD_Level]</code>. Helps align model output with the target audience.</li> <li>Safety Risk: <code>[NoRisk, LowRisk, MediumRisk, HighRisk]</code>. Granular assessment for safety-critical applications.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#4-continuous-scalar","title":"4. Continuous (Scalar)","text":"<p>Measures a value on a continuous range, often normalized between 0.0 and 1.0 for scores, but can be any numeric range. These are often generated by other models or algorithms and provide fine-grained signals.</p> <p>Core Question: \"How much of a certain quality does this have?\"</p> <p>Examples:</p> <ul> <li>Similarity Score: Cosine similarity between a generated answer's embedding and a ground-truth answer's embedding (e.g., <code>0.87</code>).</li> <li>Confidence Score: The model's own reported confidence in its tool use or answer, if the API provides it.</li> <li>Toxicity Probability: The likelihood that a response is toxic, as determined by a separate classification model (e.g., <code>0.05</code>).</li> <li>Groundedness Score: A score from 0 to 1 indicating how much of the generated text is supported by provided source documents.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#5-count-ratio","title":"5. Count &amp; Ratio","text":"<p>Measures the number of occurrences of an event or the proportion of one count to another. These are fundamental for understanding frequency, cost, and efficiency.</p> <p>Core Question: \"How many?\" or \"What proportion?\"</p> <p>Examples:</p> <ul> <li>Token Count: Number of tokens in the prompt or response. This directly impacts both cost and latency.</li> <li>Number of Turns: How many back-and-forths in a conversation. A low number can signal efficiency (quick resolution) or failure (user gives up). Context is key.</li> <li>Hallucination Rate: (Count of responses with hallucinations) / (Total responses). A key quality metric.</li> <li>Tool Use Attempts: The number of times the agent tried to use a tool before succeeding or failing. High numbers can indicate a flawed tool definition or a confused model.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#6-positional-rank","title":"6. Positional / Rank","text":"<p>Measures the position of an item in an ordered list. This is crucial for systems that generate multiple options or retrieve information, as the ordering of results is often as important as the results themselves.</p> <p>Core Question: \"Where in the list was the correct answer?\" or \"How high up was the user's choice?\"</p> <p>Examples:</p> <ul> <li>Retrieval Rank: In a RAG system, the position of the document chunk that contained the correct information. A rank of <code>1</code> is ideal; a rank of <code>50</code> suggests a poor retriever.</li> <li>Candidate Generation: If the system generates 3 draft emails, which one did the user select? (1<sup>st</sup>, 2<sup>nd</sup>, or 3<sup>rd</sup>). If users consistently pick the 3<sup>rd</sup> option, maybe it should be the 1<sup>st</sup>.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#part-2-a-layered-framework-for-llm-systems","title":"Part 2: A Layered Framework for LLM Systems","text":"<p>Thinking in layers helps isolate problems and understand the system's health from different perspectives. A failure at a lower level (e.g., high latency) will inevitably impact the higher levels (e.g., user satisfaction).</p> <p>These three layers form a hierarchy where each builds on the previous:</p> <pre><code>graph TB\n    Op[Layer 1: Operational&lt;br/&gt;Is it working?]\n    Qual[Layer 2: Output Quality&lt;br/&gt;Is it good?]\n    Success[Layer 3: User Success&lt;br/&gt;Does it help?]\n\n    Op --&gt; Qual --&gt; Success\n\n    style Op fill:#e8f4f8\n    style Qual fill:#e1f5ff\n    style Success fill:#ffe1f5</code></pre>"},{"location":"writing/agent-metrics-for-engineers/#layer-1-operational-system-metrics-is-it-working","title":"Layer 1: Operational &amp; System Metrics (Is it working?)","text":"<p>This is the foundation. If the system isn't running, nothing else matters. These metrics are objective, easy to collect, and tell you about the health and efficiency of your service.</p>"},{"location":"writing/agent-metrics-for-engineers/#latency-time-based","title":"Latency (Time-based):","text":"<ul> <li>Time to First Token (TTFT): How long until the user starts seeing a response? This is a primary driver of perceived performance. A low TTFT makes an application feel responsive, even if the total generation time is longer.</li> <li>Total Generation Time: Full time from prompt submission to completion.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#throughput-volume-based","title":"Throughput (Volume-based):","text":"<ul> <li>Requests per Second (RPS): How many requests can the system handle? Essential for capacity planning.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#cost-resource-based","title":"Cost (Resource-based):","text":"<ul> <li>Tokens per Request: Average prompt and completion tokens. This is the primary driver of direct LLM API costs.</li> <li>Cost per Conversation: Total cost of a multi-turn interaction, including all LLM calls, tool calls, and other API services.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#reliability-error-based","title":"Reliability (Error-based):","text":"<ul> <li>API Error Rate: How often do calls to the LLM or other external tools fail (e.g., due to network issues, rate limits, or invalid requests)?</li> <li>System Uptime: The classic operational metric, representing the percentage of time the service is available.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#layer-2-output-quality-metrics-is-the-output-good","title":"Layer 2: Output Quality Metrics (Is the output good?)","text":"<p>This is the most complex layer and specific to generative AI. \"Goodness\" is multi-faceted and often subjective. These metrics require more sophisticated evaluation, including other models (\"LLM-as-Judge\") or structured human review.</p>"},{"location":"writing/agent-metrics-for-engineers/#faithfulness-groundedness-is-it-true","title":"Faithfulness / Groundedness (Is it true?):","text":"<ul> <li>Citation Accuracy (Binary/Ratio): Does the provided source actually support the generated statement? This can be a simple check (the source is relevant) or a strict one (the exact passage is highlighted).</li> <li>Hallucination Rate (Ratio): What percentage of responses contain fabricated information? Defining a \"hallucination\" requires a clear rubric for human evaluators.</li> <li>Contradiction Score (Continuous): A score from an NLI (Natural Language Inference) model on whether the response contradicts the source documents.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#relevance-is-it-on-topic","title":"Relevance (Is it on-topic?):","text":"<ul> <li>Relevance Score (Ordinal/Continuous): How relevant is the response to the user's prompt? Often rated on a scale (e.g., 1-5) or scored by another model using embeddings.</li> <li>Instruction Following (Binary/Ordinal): Did the model adhere to all constraints in the prompt (e.g., \"Answer in 3 sentences,\" \"Use a formal tone,\" \"Format the output as a JSON object with keys 'name' and 'email'\")? This is a key measure of model steerability.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#clarity-coherence-is-it-well-written","title":"Clarity &amp; Coherence (Is it well-written?):","text":"<ul> <li>Readability Score (Continuous): Flesch-Kincaid or similar automated scores to ensure the output is appropriate for the target audience.</li> <li>Grammar/Spelling Errors (Count): Number of detected mistakes.</li> <li>Coherence Score (Ordinal): Does the response make logical sense from beginning to end? This is highly subjective and almost always requires human judgment.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#safety-responsibility-is-it-safe","title":"Safety &amp; Responsibility (Is it safe?):","text":"<ul> <li>Toxicity Score (Continuous): Output from a public or custom-trained toxicity classifier.</li> <li>PII Detection Rate (Binary/Ratio): Does the model leak personally identifiable information, either from its training data or from provided context?</li> <li>Jailbreak Attempt Detection (Binary): Was the user prompt an attempt to bypass safety filters?</li> <li>Bias Measurement (Classification/Ratio): Using a benchmark dataset of templated prompts (e.g., \"The [profession] from [country] went to...\"), does the model generate responses that reinforce harmful stereotypes?</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#layer-3-task-user-success-metrics-did-it-help","title":"Layer 3: Task &amp; User Success Metrics (Did it help?)","text":"<p>This is the ultimate measure of value. A model can produce a perfect, factual, safe answer, but if it doesn't help the user achieve their goal, the system has failed. These metrics connect model performance to real-world impact.</p>"},{"location":"writing/agent-metrics-for-engineers/#task-success","title":"Task Success:","text":"<ul> <li>Task Completion Rate (Binary/Ratio): For goal-oriented systems (e.g., booking a ticket, summarizing a document), did the user successfully complete the task? This is often measured by tracking clicks on a final \"confirm\" button or reaching a specific state.</li> <li>Goal Completion Rate (GCR): A more nuanced version asking if the user achieved their ultimate goal, even if it took a few tries. For example, a user might complete the \"task\" of finding a recipe but fail their \"goal\" because it required an ingredient they didn't have.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#user-interaction","title":"User Interaction:","text":"<ul> <li>Thumbs Up/Down Rate (Ratio): Simple, direct user feedback. The most valuable signal when available.</li> <li>Conversation Length (Count): Shorter might mean efficiency; longer might mean engagement. This needs to be correlated with task success to be interpreted correctly.</li> <li>Response Edit Rate (Ratio): How often do users have to copy and then significantly edit the AI's generated response? A high rate is a strong negative signal.</li> <li>Follow-up Question Rate (Ratio): Are users asking clarifying questions because the first answer was incomplete, or are they naturally continuing the conversation?</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#business-value","title":"Business Value:","text":"<ul> <li>Deflection Rate: In a customer support context, what percentage of issues were solved without escalating to a human agent? A high deflection rate is only good if user satisfaction is also high. This is also the pricing structure for Fin by Intercom.</li> <li>Conversion Rate: Did the interaction lead to a desired business outcome (e.g., a sale, a sign-up)?</li> <li>User Retention (Ratio): Are users coming back to use the application? This is a powerful long-term indicator of value.</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#part-3-multi-turn-chat-measuring-conversations-not-just-responses","title":"Part 3: Multi-Turn Chat \u2014 Measuring Conversations, Not Just Responses","text":"<p>Parts 1 and 2 established the foundational metric types and a layered framework applicable to any LLM system. Part 2's Layer 2 covered output quality metrics\u2014relevancy, faithfulness, coherence\u2014that apply to individual model responses. This foundation is essential, and now we extend it for agentic and conversational systems. </p> <p>Multi-turn chat introduces complexities that single-turn evaluation cannot capture: context management across turns, user intent shifts, conversational flow, and the ability to recover from errors. A response that scores perfectly on relevancy and faithfulness in isolation can still derail an entire conversation if it ignores previous context or misinterprets evolving user intent.</p> <p>This section focuses on what's unique to conversational systems: Conversation-Specific Metrics that evaluate the entire user journey, and User Interaction Signals that reveal implicit feedback traditional metrics miss.</p> <p>The evaluation flow for conversations extends the single-turn approach:</p> <pre><code>graph LR\n    Turn[Turn Metrics&lt;br/&gt;Quality per response]\n    Conv[Conversation Metrics&lt;br/&gt;Success across turns]\n    Signals[User Signals&lt;br/&gt;Behavior patterns]\n\n    Turn --&gt; Conv\n    Signals --&gt; Conv\n    Conv --&gt; Insights[Product Decisions]</code></pre>"},{"location":"writing/agent-metrics-for-engineers/#turn-specific-metrics-extending-the-question-answer-framework","title":"Turn-Specific Metrics \u2014 Extending the Question-Answer Framework","text":"<p>At the micro-level, we must ensure each turn is high-quality. These metrics adapt classical QA evaluation to the conversational setting, and many have been operationalized in open-source frameworks like Ragas<sup>1</sup>.</p>"},{"location":"writing/agent-metrics-for-engineers/#1-comparing-answer-with-question-relevancy","title":"1. Comparing Answer with Question: Relevancy","text":"<p>The most fundamental requirement is that the model's answer directly addresses the user's most recent query. If a user asks, \"What were NVIDIA's Q2 earnings?\" the model shouldn't respond with the stock price. This concept of \"Answer Relevancy\" is a cornerstone metric that measures how well the response satisfies the immediate user intent.</p> <p>How to measure: This is often scored by human raters on a Likert scale. It can also be automated by using a powerful LLM as an evaluator, a technique that has shown strong correlation with human judgment<sup>2</sup>. Frameworks like Ragas implement this by using an LLM to generate potential questions from the answer and then measuring the semantic similarity between those generated questions and the original user query.</p>"},{"location":"writing/agent-metrics-for-engineers/#2-comparing-answer-with-context-faithfulness","title":"2. Comparing Answer with Context: Faithfulness","text":"<p>A model's response must be factually consistent with the information it was given. When a model generates information that cannot be verified against its context, it is often called a \"hallucination.\" Faithfulness, or groundedness, measures the absence of such hallucinations.</p> <p>How to measure: This involves a form of automated fact-checking. A common technique, used by Ragas, is to break the generated answer down into individual statements. Each statement is then verified against the source context to see if it can be directly inferred. The final score is the ratio of verified statements to the total number of statements<sup>3</sup>.</p>"},{"location":"writing/agent-metrics-for-engineers/#3-answer-vs-pre-defined-aspects","title":"3. Answer vs. Pre-defined Aspects","text":"<p>Not all quality attributes are about factual correctness. Depending on the product, you may need to enforce specific stylistic or content requirements. These \"aspect-based\" evaluations ensure the model adheres to brand voice and product needs.</p> <ul> <li>Common Aspects:</li> <li>Tone: Is the response professional, friendly, empathetic, or neutral, as required?</li> <li>Length: Does the answer respect length constraints (e.g., staying under a certain character count for a mobile interface)?</li> <li>Required Information: Does the answer include necessary elements like legal disclaimers, links to sources, or specific product mentions?</li> </ul>"},{"location":"writing/agent-metrics-for-engineers/#conversation-specific-metrics-capturing-the-flow","title":"Conversation-Specific Metrics \u2014 Capturing the Flow","text":"<p>A conversation can be composed of individually perfect turns and still be a total failure. Conversation-specific metrics analyze the entire user journey to identify broader patterns of success or failure.</p>"},{"location":"writing/agent-metrics-for-engineers/#drop-off-k-turns","title":"Drop-off @ K Turns","text":"<p>This metric identifies the average number of turns after which a user abandons the conversation. A high drop-off rate after just one or two turns might indicate poor initial response quality. Conversely, a drop-off after many turns could mean the user successfully completed a complex task or, alternatively, gave up in exhaustion. Segmenting this metric by conversation outcome (e.g., user clicks \"thumbs up\" vs. just closes the window) is crucial for correct interpretation.</p>"},{"location":"writing/agent-metrics-for-engineers/#conversation-success-rate","title":"Conversation Success Rate","text":"<p>Beyond individual turn quality, did the entire conversation achieve its goal? This binary or ordinal metric evaluates whether the conversation as a whole was successful. For goal-oriented dialogues (e.g., booking, troubleshooting), this can be measured by tracking whether the user reached a terminal success state. For open-ended conversations, this might require human annotation or LLM-as-judge evaluation of the full transcript against success criteria.</p>"},{"location":"writing/agent-metrics-for-engineers/#user-interactions-as-product-intelligence","title":"User Interactions as Product Intelligence","text":"<p>Beyond explicit feedback, how users physically interact with the chat interface provides powerful signals about the quality and utility of the model's responses. These signals fall into two categories: explicit expressions and implicit behaviors.</p>"},{"location":"writing/agent-metrics-for-engineers/#explicit-signals","title":"Explicit Signals","text":""},{"location":"writing/agent-metrics-for-engineers/#1-user-frustration","title":"1. User Frustration","text":"<p>Frustration is a critical emotional signal to capture. It indicates a fundamental breakdown between user expectation and model performance. Users often directly express frustration. Look for patterns like: * Repeated question marks: ?? * Direct challenges: Why??, That's not what I asked * Rephrasing the same query multiple times * Use of all-caps</p>"},{"location":"writing/agent-metrics-for-engineers/#2-user-confusion","title":"2. User Confusion","text":"<p>Confusion differs from frustration. It signals that the user doesn't understand the model's response or reasoning, even if the model isn't necessarily \"wrong.\" Look for: * Where did this come from? (Indicates a need for better citation/attribution). * What are you talking about? (Indicates the model may have lost context).</p>"},{"location":"writing/agent-metrics-for-engineers/#3-need-for-explanations","title":"3. Need for Explanations","text":"<p>When users start asking meta-questions about the AI's capabilities, it reveals a gap in their mental model of how the system works. These questions are a goldmine for product improvement.</p> <p>Examples: * Why can't you update the glossary for me? * Can you add a new contact to my list?</p> <p>These interactions highlight user expectations about the model's agency and tool-use capabilities. Tracking these can directly inform the roadmap for feature development.</p>"},{"location":"writing/agent-metrics-for-engineers/#implicit-signals","title":"Implicit Signals","text":"<p>How users physically interact with the chat interface provides powerful, implicit signals about the quality and utility of the model's responses<sup>4</sup>.</p> <ul> <li>User Copies Text: If a user highlights and copies the AI's answer, it's a strong positive signal. It suggests the information was useful enough to save or use elsewhere. If they copy their own prompt, it may mean they are giving up and trying the query in a different search engine.  </li> <li>User Takes a Screenshot: This is a powerful indicator of a peak moment\u2014either extreme delight (a shockingly good answer) or extreme failure (a bizarre or hilarious error). While the sentiment is ambiguous without more context, it flags a conversation worthy of manual review.  </li> <li>User Copies a Citation Link: When a model provides sources, a user copying the URL is a stronger signal of interest and trust than a simple click. It implies an intent to save or share the source.  </li> <li>Long Click-Through Rate (CTR) to a Citation: A standard CTR simply tells you a link was clicked. A \"long CTR,\" where you measure the dwell time on the linked page, is far more valuable. If a user clicks a citation and spends several minutes on that page, it validates that the source was highly relevant and useful, confirming the quality of the model's recommendation.</li> </ul> <p>There is no single magic metric for evaluating multi-turn chat. A comprehensive strategy requires a multi-layered approach. It starts with the foundation of turn-specific quality\u2014relevancy, faithfulness, and adherence to style. But to truly understand the user experience, you must layer on conversation-specific metrics that track the narrative flow and identify points of friction. Finally, by analyzing user interaction data, product teams can gain invaluable, implicit feedback to guide future development.</p>"},{"location":"writing/agent-metrics-for-engineers/#conclusion","title":"Conclusion","text":"<p>Building a comprehensive evaluation strategy for LLM-powered agents requires thinking beyond traditional software metrics. The framework presented here provides a systematic approach:</p> <ol> <li>Start with the fundamentals: Understand the six foundational metric types (Classification, Binary, Ordinal, Continuous, Count &amp; Ratio, and Positional) and apply them appropriately to your specific use case.</li> <li>Think in layers: Structure your measurement strategy across three interconnected layers: operational metrics ensure your system is running efficiently, output quality metrics verify your responses are correct and safe, and user success metrics confirm you're delivering real value.</li> <li>Embrace multi-turn complexity: For conversational systems, evaluate both individual turns and entire conversations. Track how well each response addresses the immediate query, but also measure conversation-level patterns like topic drift, success rates, and recovery from errors.</li> <li>Combine implicit signals: User interactions\u2014copying text, taking screenshots, abandonment patterns\u2014often reveal more than explicit feedback. Build instrumentation to capture these behavioral signals.</li> </ol> <p>The metrics you choose should reflect your product goals and constraints. A customer support bot should prioritize deflection rate and user satisfaction. A research assistant should emphasize faithfulness and citation accuracy. A creative writing tool might focus on user engagement and iteration patterns.</p> <p>Start small. Implement operational metrics first, add a few key quality metrics for your most critical use cases, then gradually expand your coverage. The goal is not to measure everything, but to measure what matters for making informed decisions about where to invest your improvement efforts.</p>"},{"location":"writing/agent-metrics-for-engineers/#references","title":"References","text":"<ol> <li> <p>Es, Shahul, et al. (2023). Ragas: Automated Evaluation of Retrieval Augmented Generation. arXiv preprint arXiv:2309.15217\u00a0\u21a9</p> </li> <li> <p>Chiang, Wei-Lin, et al. (2023). Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/\u00a0\u21a9</p> </li> <li> <p>Min, Sewon, et al. (2023). FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. arXiv preprint arXiv:2305.14251\u00a0\u21a9</p> </li> <li> <p>Radlinski, Filip, et al. (2019). How Am I Doing?: Evaluating Conversational Search Systems Offline. Proceedings of the 2019 Conference on Human Information Interaction and Retrieval\u00a0\u21a9</p> </li> <li> <p>NVIDIA NeMo Evaluator. (2023). NVIDIA Developer Documentation. https://docs.nvidia.com/nemo-framework/user-guide/latest/nemollm/nemo_evaluator.html\u00a0\u21a9</p> </li> <li> <p>Zhang, Tianyi, et al. (2023). A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. arXiv preprint arXiv:2311.05232\u00a0\u21a9</p> </li> </ol>"},{"location":"writing/ai4bharat/","title":"AI4Humans aka Software x LLMs","text":"<p>AI4Bharat, IIT Madras, July 2023</p> <p>Namaste! \ud83d\ude4f I'm Nirant and here's a brief of what we discussed in our session. </p>"},{"location":"writing/ai4bharat/#why-you-should-care","title":"Why You Should Care?","text":"<p>I have a track record in the field of NLP and machine learning, including a paper at ACL 2020 on Hinglish, the first Hindi-LM, and an NLP book with over 5000 copies sold. I've contributed to IndicGlue by AI4Bharat, built and deployed systems used by Nykaa, and consulted for healthcare enterprises and YC companies. I also manage India\u2019s largest GenAI community with regular meetups since February 2023. </p> <p>Here's my Github.</p>"},{"location":"writing/ai4bharat/#ai4humans-retrieval-augmented-generation-for-india","title":"AI4Humans: Retrieval Augmented Generation for India","text":"<p>We dived into two main areas:</p> <ol> <li>Retrieval Augmented Generation: Examples of RAG for India, engineering choices, open problems, and how to improve it </li> <li>LLM Functions: Exploring tool augmentation and \"perfect\" natural language parsing</li> </ol>"},{"location":"writing/ai4bharat/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>RAG is a popular pattern in AI. It's used in various applications like FAQ on WhatsApp, customer support automation, and more. It's the backbone of services like Kissan.ai, farmer.chat and Bot9.ai.</p> <p>However, there are several open problems in RAG, such as text splitting, improving ranking/selection of top K documents, and embedding selection.</p>"},{"location":"writing/ai4bharat/#adding-details-to-rag","title":"Adding Details to RAG","text":"<p>We can improve RAG by integrating models like OpenAI's GPT4, Ada-002, and others. We can also enhance the system by adding a Cross-Encoder and 2 Pass Search.</p> <p></p> <p>Despite these improvements, challenges remain in areas like evaluation, monitoring, and handling latency/speed. For instance, we discussed how to evaluate answers automatically, monitor model degradation, and improve system latency.</p>"},{"location":"writing/ai4bharat/#using-llm-to-evaluate","title":"Using LLM to Evaluate","text":"<p>An interesting application of LLM is to use it for system evaluation. For example, we can use LLM to auto-generate a QA test set and auto-grade the results of the specified QA chain. Check out this auto-evaluator as an example.</p>"},{"location":"writing/ai4bharat/#addressing-open-problems","title":"Addressing Open Problems","text":"<p>We discussed the best ways to improve system speed, including paged attention, caching, and simply throwing more compute at it. We also touched on security concerns, such as the need for separation of data and the use of Role Based Access Control (RBAC).</p>"},{"location":"writing/ai4bharat/#llm-functions","title":"LLM \u201cFunctions\u201d","text":"<p>We explored how LLMs can be used for tool augmentation and converting language to programmatic objects or code. The Gorilla LLM family is a prime example of this, offloading tasks to more specialized, reliable models.</p> <p>In the context of AgentAI, we discussed how it can help in converting text to programmatic objects, making it easier to handle complex tasks. You can check out the working code here.</p> <p>Thank you for attending the session! Feel free to connect with me: Twitter, LinkedIn or learn more about me here.</p>"},{"location":"writing/ai4bharat/#references","title":"References","text":"<ul> <li>Slides for this talk</li> <li>RAG paper</li> <li>AgentAI library</li> <li>Auto-evaluator</li> <li>Gorilla LLM family</li> <li>vLLM</li> </ul> <p>Images in this blog are taken from the slides presented during the talk.</p>"},{"location":"writing/ankitnagori/","title":"Ankit Nagori, Co-Founder of Cult","text":"<p>Last month, I had a chance to talk to Ankit Nagori - the co-founder at Cult or Cure.fit</p> <p>This first appeared on https://niranting.substack.com</p>"},{"location":"writing/ankitnagori/#transcript","title":"Transcript","text":""},{"location":"writing/ankitnagori/#origin-story","title":"Origin Story","text":"<p>IIT to Flipkart for 6 years and then left with Mukesh Bansal of Myntra to start Cure.fit</p> <p>Ankit Nagori  0:00 Great. So I think my journey my journey started with Flipkart. That was a couple of years after I finished my undergrad at IIT. And at Flipkart, I spend close to six, six and a half years trying to build new businesses and figure out new markets for expansion. When I joined - Flipkart was just a bookseller, of course, had built a lot of e commerce technologies and the opportunity was ripe to build more categories. spend about six, six and a half years there building categories. And then when I when I decided to move on, even Mukesh was moving on. Mukesh had been in Flipkart for two and a half years after the Myntra merger, and he realized that there's an opportunity to do something really big in the healthcare space. Healthcare space, you know, is a very fragmented market, multiple categories, but all of them very small, very unbranded in nature and realize that there's a huge opportunity to create a multi-category platform,</p> <p>Ankit Nagori  1:02 that's how we did it.</p> <p>Nirant  1:06 Multi category platform, that's a very interesting way of looking at things, I think, HealthyifyMe and Cult are the only two things doing that right now. So why did you find this very interesting? </p> <p>Nirant  1:23 I think if you look at the overall, you know, health space or from a consumer point of view, to stay healthy. You really need to do multiple things you need to stay, you know, fit, mentally and physically, you need to eat healthy, you need to sleep well, you need to, you know, do your health checkup every year and do follow ups. So it is extremely difficult to stay healthy. And technology can definitely make it easier. And if you want to stay healthy, it has to be multiple categories, otherwise you just end up becoming a gym or health-food parlour or something. So we realize that the real value in long term - of course, it's very, very early days for us. But the real value in the long term will lie if we can own the end to end journey where a customer decides to stay healthy, and then she can really figure out the entire journey in over the few years few quarters across the board. So that's why you know, multi category is important. And while building a multi category platform, trying to make it easy is also very important. So we have tried to create products which are easy to consume, which are for starters for people who are not experts. And that's how we embarked on the journey.</p> <p>Nirant  2:34 And you chose to start with something like cult you could have started with something like care fit as well. </p> <p>Nirant  2:43 Yeah, Why?</p>"},{"location":"writing/ankitnagori/#go-to-market","title":"Go To Market","text":"<p>Started with the most consumerized and \"sticky\" habit: Gyms</p> <p>Ankit Nagori  2:45 I think the reason is very clear. If you look at the most consumerized product in healthcare right now, something which is very consumer focused, which is a very casual product, gyms is that category, because clinics and hospitals are very need based. People will do that when they have an acute need, of course. Healthcare and primary health care and secondary healthcare market is the largest. But fitness centers is something you know, which is the most consumerized product, more lifestyle product. And you know something which is an easy start, if you start doing a few classes every month, you will start the journey. And then of course, food is as important and the real problem in the country is that you don't have a very good supply of health food, you can't figure out what what can qualify as healthy food. And that's fair, I think food becomes extremely important as well. So these are the two categories where you've got massive scale. And of course, on top of that, we are building many, many more verticals which will build around food and fitness.</p> <p>Nirant  3:46 Okay, so now you mentioned cult.fit and eat.fit both cult has been amazing. You've built a community around it's word of mouth is insane, and how do you maintain it?</p>"},{"location":"writing/ankitnagori/#product-philosophy","title":"Product Philosophy","text":"<p>Every cult center is seen as a separate startup which needs to maintain 1000 true fans through out it's existence.</p> <p>Ankit Nagori  4:08 See, I think if you build any product, you need to start building a start, you start by just creating a very small community. And as a company, we follow a very strong philosophy around 1000 true fans, we feel that for any product to become successful, you need to get 1000 true fans, and true fans definitions, people are super duper users who will kill to use that product who would you know, be proponents, etc, etc. So that's exactly what we do. Whenever you open a cult center in a locality or a couple of other centers, we aim to get 1000 true fans in that locality. So when we launch a business, we do HSR, Sarjapur in Bangalore as a market and said that if we get 1000 true fans there, then we'll expand to Indiranagar and then again, and then we'll expand to Koramangala. So how we continue doing it is that because every time you open a new locality, we think of it as a new startup, and we say that 1000 people will not become crazy about the product, then there's no point. And that's a very strong thesis that we follow. And that has helped us reinvent the product. Every few quarters. That has helped us maintain the quality when we launch new cities, new markets. So that is the philosophy that we follow.</p> <p>Nirant  5:20 We have thousand true fans. The thesis, though works at like thousand feet level. When you said you iterate over this every few quarters, how does this each iteration look like? So I remember you had a very early center in Sarjapur for 2016/17. And my batchmate from college was like you should move to Sarjapur so that you can do cult, and that was very early in the cycle itself. So I'm assuming that now that this is probably like iteration 20 or something like that. What have these iterations look like?</p>"},{"location":"writing/ankitnagori/#role-of-technology","title":"Role of Technology","text":"<p>Building a customer journey across multiple touchpoints</p> <p>Ankit Nagaori  5:56 See, I think the first first important step we decided in 2016 was that the biggest problem that lies with the gyms is twofold: One, there is very little technology at the back end to manage the processes. And second, there is zero technology consumer facing. If you would go to even the top gyms in the US know Orangetheory, Barry's boot camp, very good product. But consumer journey is very weak. Consumers don't know how many classes. It's almost like a booking engine. Nothing like that of a journey tracker. So we realize that we need to build a journey product, where customers know how many classes they have done. They feel like doing more classes, they see progression. And they see other ancillary products getting tied up to it. So that's exactly what we have done. We have tried to create a product which is you know, a journey based product and a lot of technology. And if you are on a journey based product, people have done 100 classes in last one year to get them to another hundred classes this year -- You need to upgrade the product. And that's exactly what the philosophy is. You have to launch new formats. You have to launch new reasons for people to get excited. We're looking at hardware integration with the heartbeat, heart rate monitor in every class. We will be integrating a lot of technology through screens on the in the center. So I think the idea is to continuously evolve into a product which consumers are excited about. So we are very core value for us is blowing away customer expectations. But the tough part about customer expectation is that whatever was \"wow\" yesterday becomes table stakes today. So you have to continuously manage that every single day. So I think if you don't launch a new product every few quarters, which also get 1000 true fans really, then you will not continuously evolve. So every version of cult has to have 1000 true fans and not just the fact that at one point in time, every centre had that.</p> <p>Nirant  7:41 So what you're saying is it's not just 1000 true fans it's also novelty and pace, the pace of novelty itself.</p> <p>Ankit Nagori  7:48 Yes.</p> <p>Nirant  7:51 So, you mentioned the second thing which you mentioned was eat/eat.fit's menu has been very Indian/desi with roti, rice and sabji and you have not done any of the fancy Quinoa salads (I don't know how to pronounce). So how do you make operational decisions get made?</p>"},{"location":"writing/ankitnagori/#consumer-insight","title":"Consumer Insight","text":"<p>The simple ideas behind Eat Fit, which is used by over 50% of cult members very frequently</p> <p>Ankit Nagori  8:33 Everything starts and ends with the consumer insight. So the interesting thing about food is food is something which people will not change their habit at all. It is the most intrinsic habit for a particular customer. If you look at how Indian palates work, there is a carb, there's a curry, and there's a protein. These are three things: dal-roti-paneer, dal-roti-chicken, chicken biryani, dosa, all of these are very standard thali format -- you need multiple variants. Indians are not used to, you know, a lot of single bowl salads etc. But if you also look at what our customers end up eating, if our target audiences, urban working people, right? They end up eating a lot of this food through cafeterias through ordering in, through fancy restaurants etc. The problem is that none of these are extremely healthy or extremely hygienic. So we said that if someone's going to collect all they need is a very basic healthy food, which is calorie counted. And the format has to be rice roti sabzi dal, you know, because that's what people eat 80% times. Pizza burger ko healthy banana is not the best idea because anyways, if people eating pizza and burgers once a week, let them cheat. Why do they need a you know, healthy pizza? Why do they need a, you know, brown bread burger or something like that, right? So it's better to just let them cheat, but when they're eating 80% of the meals It should be basic, healthy, hygienic, and calorie counted. Getting millions of people to change their daily habits is extremely tough, but giving them a slightly better version will work. And that's exactly the thesis that worked for us. We see the maximum demand during lunchtime, from corporates, which clearly still shows that people are looking for a healthier option than what they get at cafeterias and, you know, corporate catering. So I think that's the core insight. And about 50% of our customers end up using our eat fit menu very, very aggressively.</p>"},{"location":"writing/ankitnagori/#net-promoter-score","title":"Net Promoter Score","text":"<p>The glue that binds Cure Fit as a Team across Functions</p> <p>Nirant  10:36 Okay. And how does tracking flow back to how your decisions are made? Matlab every class you take an NPS score, how awesome it was or not. I remember early on 2019, I had marked one class was bad because I had an injury in the class. And I got a call back from cult? How are these processes built around NPS tracking?</p> <p>Ankit Nagori  11:06 Right? So if I look at, you know, the entire org, the entire orgs feature, features, sorry, the focus, even if you look at the entire odds focus, it is completely in the direction of making insanely great products and blowing up the customer expectation. Those are our core values. And one very interesting thing you know, because of technology integration at gyms and food is that we have a four point scale, after every interaction, you get a four point scale on rating the product on an experience, and anything which goes below four, which is three two and one, we start, you know, analyzing it. And the interesting thing is it's so real time that if there's a class in Bombay, where the speakers were not working, we'll start getting feedback like within 30 seconds of class getting over and then we'll know that something went wrong. We'll make sure that the speakers are fixed even before the next class starts. So, of course, it's very Uber like, you know, approach where they read their rides, e.g. customers. For us, it's extremely, extremely real time because customers immediately open the app again after the class to book the next class, they open the app to book their next meal, because of the daily transaction, multi multiple times a day, daily transaction nature, we get very many DLT feedback. So that goes back into all our our processes, all the new product development, all the performance management, incentives, etc, is all all tied back to the four point scale. So I think the biggest thing that keeps us together as a company is the four points rating scale. It is the biggest top of the mind metric and ask anyone, any trainer, what metric what the rating was for the last three days, they will know. Ask any chef, what was the rating for the kitchen last 3 days they would know. So I think this is a very, very important thing, which is, you know, created the entire journey around ratings and reviews.</p> <p>Niant  13:00 that is the Northstar metric in a manner of speaking? </p> <p>Ankit Nagori 13:05 Absolutely, absolutely.</p>"},{"location":"writing/ankitnagori/#hiring-and-organisation","title":"Hiring and Organisation","text":"<p>1 month Training Academy in Bengaluru and NCR for all Operations staff</p> <p>Nirant 13:10 Now that we've spoken about trainers and chefs, how do you hire people? You've also acquired businesses like TFX and Tribe, how are these integrated?</p> <p>Nirant  13:19 And so, we have two sets of you know, teams that we hire one is the sales, marketing, growth and technology side of the world. And second is the teams which are on the ground: running the center, running the kitchens, making the food, doing the classes. So for the first bucket, you know, we we have focus on campus recruitment and also, you know, from technology companies, ecommerce companies. So there we have a very good mix of experiences and very young folks with technology Product Marketing background. It's mostly inbound. We put out openings on LinkedIn, and networks. It's mostly inbound. We have an assimilation process where we meet them - the Candidates a few times. Look at the passion for Fitness, Sports and health, I think it's super important to have that passion to work for cure fit because it is a very difficult space. And if you yourself not a consumer of these products, then it'll be difficult to create these products. </p> <p>On the city teams and the operations team, which I mentioned. There we have, you know, boot camps, campus hiring, walk in drives, where we, you know, we figure out who we need to hire. And the most interesting aspect there is that after we hire these people, there is a one month training program. So we have training academy set up in Bangalore and NCR. It's a full classroom program, and anyone who hits the floor or the centers or anything, they have to go through a three to four weeks dedicated training, they have to clear a test and that helps us maintain quality.</p> <p>Nirant  14:51 So that's on ground ops teams. What do you look for when you hire in those roles? </p> <p>Ankit Nagori  15:00 think I think we look for you know, some basic intrinsics things of course, and these intrinsics are important because we need to know how it how it How well do they know the technology product? How do they How well do they stand their coaching, but how well you understand the food part. So some some understanding of technology, but very deep understanding of food or fitness in respective areas. And then we figure out how well we'll be able to transform their learning to the Cure Fit way of doing things. We have a particular philosophy of food we have a particular philosophy for doing fitness training. So I think openness to learning, overall knowledge base of health and fitness and food. And lastly, how well can they adapt to technology? I think these are three important parts.</p>"},{"location":"writing/ankitnagori/#the-role-of-success-in-employee-retention","title":"The Role of Success in Employee Retention","text":"<p>Measured with OKRs with Quarterly Review and NPS</p> <p>Nirant  15:43 Awesome. And how to maintain and retain these for both sides, both teams?</p> <p>Nirant  15:52 I think we realize that you know, the biggest way of retaining people is to continuously show them success. Showing success means really everyone making impact. So we are a very heavy, OKR driven company like most of the work gets done to objectives and key results framework. And we try and make sure that most people are seeing tangible progress in their journey. So I think that is one biggest engagement area. Second is we actually have a very strong culture of sports and fitness. And most of our engagement activities are actually built around that. I think that has helped us build a massive health and fitness culture and the camaraderie around it. So I think, overall, we feel that the whole health and fitness culture, which all the engagement activity is all there, all the traditions of the company have been around that. And lastly, the continuous success metric, which is success under the belt, OKRs are, I think, a very good way of making sure that people understand that they're making progress. If they look back six months in time, they know exactly what they've achieved and what could they have done better for the next six months.</p> <p>Nirant  17:00 Yeah, that's, that's interesting. So, how frequently are the OKRs measured as a, like six month?</p> <p>Ankit Nagori  17:08 It's a quarterly process Nirant. So everyone has a 12 month goal and a quarterly milestone. So that's how it's done.</p> <p>Nirant  17:14 Okay, quarter.</p>"},{"location":"writing/ankitnagori/#ankit-the-person","title":"Ankit, The Person","text":""},{"location":"writing/ankitnagori/#favourite-sachin-innings","title":"Favourite Sachin Innings?","text":"<p>Nov 2009, IND vs AUS, Sachin scored 175</p> <p>Nirant  17:16 Let's talk about you personally a little, and how you think? Which is your favorite innings of Sachin?</p> <p>Ankit Nagori  17:23 Yeah, so interestingly, yesterday was as soon as but then I had put that tweet out also.</p> <p>Unknown Speaker  17:29 So my favorite thing is that he scored a 175 against Australia in Hyderabad, it was a One Day International. He was chasing 350 and we got to 347. When he got out, we needed 20 runs of 20 balls or something like that. And there's been a lot of chatter about whether he's a good finisher or not, but if someone takes you so far, and still India does not and there have been many, many occasions in 90s &amp; 2000s where this happened, where you know, he would if he would take he would take the team to like last 10 runs and still not finish. The difference now is that India has solid finishers. If someone would even even leave that team at 50 runs from 30 balls we will know will win. But that was a crazy innings like scoring 175 or the 347. And the amount of fours and sixes across the ground. I think it was one of his best innings.</p> <p>Nirant 18:19 What year is this match from?</p> <p>Ankit Nagori 18:20 2009 November</p> <p>Nirant 18:21 I'll definitely go and take a look at that</p>"},{"location":"writing/ankitnagori/#daily-rituals","title":"Daily Rituals","text":"<p>Up early at 5:30 AM -&gt; Work Out -&gt; Reading -&gt; Office -&gt; Home by 7:30 PM, followed religiously</p> <p>Nirant  18:28 What is your ideal day look like?</p> <p>Nirant  18:31 So, non lockdown days, I would say starts very early. If I would talk until March, I start my day at 5:30 to 6:00. Start my day with a workout. And then the morning couple of morning rituals, you know, spending time reading and all of that some newspaper news. Would would reach office by 1030. Spend the day in meetings some personal work time, thinking time. By 7-730 I would get back home spend time with my son. I have a four and a half year old son.</p> <p>Nirant  19:05  Oh, what's his name?</p> <p>Ankit Nagori  19:09 Yaan</p> <p>Ankit Nagori  19:13 I retire to the bed by 10 1030. So, and weekends, I play cricket. We have a Cure.fit cricket team and for more than a week, so play cricket, spending time with friends over the weekend. So I have a very mundane routine, but I follow it very, very religiously. In the current times, I think the day starts a little late get by 7-7:30-8:00 you know. So that's how it is. So everything has gotten phase shifted by a couple of hours.</p>"},{"location":"writing/ankitnagori/#reading-recommendations","title":"Reading Recommendations","text":"<p>Online Journalism: The Ken, The Mint Editorials, TechCrunch, Inc.com</p> <p>Frequently Read: An Autobiography: The story of My Experiments with Truth, Mahatma Gandhi</p> <p>Most Gifted at Work: No Limits: The Art and Science of High Performance, Mukesh Bansal</p> <p>Most Gifted to Friends, Family and Work: The Industries of the Future by Alec J. Ross (Goodreads)</p> <p>Recently Liked: Blitzscaling: The Lightning-Fast Path to Building Massively Valuable Companies by Reid Hoffman</p> <p>Interesting: Sapiens and Homo Dues by Yuval Harrari</p> <p>Nirant  19:52 What do you read in the morning? You mentioned you spend about four hours in your morning rituals.</p> <p>Ankit Nagori  19:59 I don't think I spend too much during reading. There is one hour counted for that of course. I don't read a lot of news. I spend time reading news digitally, but I can I have some blogs and I have some long form articles which I follow. </p> <p>So that thing I think Ken is the interesting article to read every morning these days. And what 1520 minutes. I spent time reading TechCrunch and Inc.com so these are some of the websites. </p> <p>So I prefer reading long form articles as opposed to news. And then we have favorite news website is Mint. So I spend at least at some time, one or two articles. Again, not news, but editorials.</p> <p>Nirant  20:47 You mentioned 1000 true fans and that thesis? What author or genre do you follow or read about in business strategy?</p> <p>Ankit Nagori  20:58 Interesting, I think If I have to think about the last few books, which I really liked ... interesting question, let me check my collection and tell you</p> <p>Ankit Nagori  21:10 what was the last interesting? Yeah. So of course, both the Sapiens and Homo Deus. I think Sapiens and Dues have an interesting, you know, take at how things are. I also I read a lot, you know, so I started a lot of books and then, you know, finish them quickly, I don't find a lot of authors very inspiring. But of course, there are some books which I really, really like. I think I mentioned it earlier. Also, a couple of chats. I my favorite go to book in which I read a lot of time is the experiments of truth. My Experiments with Truth by Mothma Gandhi, I think it is a guide to most things in life. </p> <p>And from business point of view, I think I don't get heavily inspired by the book. I lead a lot of books, but I don't get extremely inspired by one book or the other either. I think Blitzscaling was the last book which I really liked. You know, Reed Hoffman, I think that was one book, which I really thought was very, very well.</p> <p>Nirant  22:09 which is a book which have gifted to your team?</p> <p>Ankit Nagori  22:12 I.</p> <p>Ankit Nagori  22:16 When we started off, we offered, we asked people, you know, at that time, my favorite was industries of the New World, or something like that. Industries of the Future, that is the book, and that I was gifting heavily not only the way not only to our colleagues, but also to friends and family. It was a really good take of how the world is going to be in the future. And the second book recently, I've been lifting people is Mukesh's book, No Limits. It's very well written. So I think that's a book which I've been gifting to a bunch of people. Yeah.</p> <p>Nirant  23:01 Okay. What part of your fit reflects you? Like, this looks like what I would do. This is my favorite bit.</p> <p>Ankit Nagori  23:10 Eat fit. I spend a lot of time on the food business. I'm a massive foodie. I think one thing that defines me really well it's food. I'm a big, big foodie. I also like to cook a bit so I think Eat Fit is the 23:23 place for me.</p> <p>Nirant  23:25 What do you cook with your son son loves?</p> <p>Ankit Nagori  23:29 I think a bunch of smoothies that I make he's a huge fan of them.</p> <p>Nirant  23:33 Ah, nice which is his favorite smoothie?</p> <p>Ankit Nagori  23:36 I think oatmeal chocolate smoothie.</p> <p>Nirant  23:37 Oatmal chocolate nice. Ah, what are the personal moat? In career or your life?</p>"},{"location":"writing/ankitnagori/#personal-moat","title":"Personal Moat","text":"<p>Hard Work</p> <p>Being in Top 10% of your cohort in Hard Work. Not just long term thinking</p> <p>Ankit Nagori  23:46 Hardwork. </p> <p>I think I believe that hardwork trumps everything else. I always have believed that, you know, if you're in the top 10 percentile of the hard work, you know, cohort, then you'll do well. So I try to do that. Of course no I see that most people have realized that hard work is the moat and that competition is fierce. And I think I know people who also feel that long term thinking and just thoughtfulness is a big moat. For me I think hard work works the best.</p> <p>Nirant  24:16 What do you do on a low day? Do you skip workout?</p> <p>Ankit Nagori  24:23 I don't do that. I have a terrible day if I don't work out. I think workout is a big thing. It keeps me sane.</p> <p>Nirant  24:35 Got it, how do you recover from a bad day? At work or otherwise</p> <p>Ankit Nagori  24:39 I think recovering from bad days is sleeping early</p> <p>Nirant  24:43 Sleeping early -- get a lot of rest. </p> <p>Ankit Nagori 24:45</p> <p>Nirant 24:48 I think it's time for some COVID questions. How is the Cult.live format doing?</p>"},{"location":"writing/ankitnagori/#covid-and-business","title":"COVID and Business","text":""},{"location":"writing/ankitnagori/#how-is-cult-live-doing","title":"How is Cult Live doing?","text":"<p>Cult Digital Products have reached about 1 Million Daily Active Users</p> <p>Ankit Nagori  25:00 Right. So I think</p> <p>Ankit Nagori  25:02 Live was a very interesting way of, you know, dealing with the situation. Have we always had the framework and the infrastructure to run these live classes? </p> <p>It is one of those projects, which we were thinking of as a long term bet, thinking we will do it in 2021. </p> <p>And we had started, started building, putting the building blocks. March 1<sup>st</sup> week is when we realized that there could be an imminent shutdown happening. And in two weeks, we fast forwarded and did whatever we could have done in last six months. The entire company's energies were put into that and thankfully it is doing well. We'll hit a million DAUs on our digital products soon. And I think it is a very interesting way of working out. We're getting customers from tier three tier four cities, which we would have never imagined with the cult offline offering.</p> <p>Nirant  25:48 That is fantastic. Do you think that it will cannibalize your revenue?</p> <p>Ankit Nagori 25:53 No, I don't think so. I think people will start thinking that it is the same. Sometimes they'll go to the center, sometimes they'll work out from home. It'll be all one big bucket of</p> <p>Ankit Nagori  26:02 working out.</p> <p>Nirant  26:03 So this is just another category in your 1000 true fans thing?</p> <p>Ankit Nagori 26:10  Yes.</p>"},{"location":"writing/ankitnagori/#how-is-cult-priced","title":"How is Cult Priced?","text":"<p>Cult aims to be Profitable at a Company Level, and not get caught in the Fundraising treadmill</p> <p>Technology, Manpower, Rentals, Branding together determine the COGS Cult charges a 25% Brand Premium on that</p> <p>Nirant  26:14 That's actually very I'll have to think a little bit more about that before asking more questions. How do you price your cult subscription?</p> <p>Ankit Nagori  26:22 I think I price my comm physically. If I look at all the pricing tools that we have done,</p> <p>Unknown Speaker  26:28 we have always sided to be unit economics positive Nirant, you know,</p> <p>Unknown Speaker  26:35 we want to</p>"},{"location":"writing/ankitnagori/#avoiding-the-fundraise-treadmill","title":"Avoiding the Fundraise Treadmill","text":"<p>Ankit Nagori  26:36 build a business where we don't burn money by scaling business every single day. And that's the kind of business where you know, when when you start scaling, you have to have this constant need of raising money you are almost like running on a treadmill which can't stop. So we price our products with a premium on manpower time, branding and Technology costs. So, these are the three things which you know are the COGS. And rental of course that is in the case of cult. And food product is the food itself.  So it's four: technology, manpower, technology, manpower is okay. Branding is second, third is the actual costs and fourth is a branding. So across all of these four buckets we have a fixed cost you know, which we understand and the variable cost and we want to charge 25% premium on top of that. Cult is a good platform you know. It is unit economic positive in a massive way. Eat fit has been unit positive, but thinner margins because the food delivery business, but our constant endeavor has been to build a profitable business and hopefully next couple of years we should be a company. We should be profitable at a company level.</p>"},{"location":"writing/ankitnagori/#carefit-unit-economics","title":"Care.Fit Unit Economics","text":"<p>Nirant  27:45 Care Fit is unit economic positive?</p> <p>Ankit Nagori  27:49 Care fit couple of centers are now out of the 8 centers because it's a new business. It takes six to 12 months to hit profitability.</p> <p>Nirant  27:56 Okay, so you think six or 12 months is profitability, not just break even.</p> <p>Ankit Nagori  28:01 Product level profitability, then they</p> <p>Ankit Nagori  28:02 are central overheads of the company, etc.</p> <p>Nirant  28:04 Yeah, take a take a parting question.</p>"},{"location":"writing/ankitnagori/#what-should-somebody-do-to-get-ankit-as-a-mentor","title":"What should somebody do to get Ankit as a mentor?","text":"<p>Nirant  28:10  What should somebody do? So that you would be interested in sharing your time teaching about how to think and how you think.</p> <p>Ankit Nagori  28:18 I think if it's a two way conversation where I get to learn something from the person, I am most happy to, you know, have these conversations. I do have some set of people who we meet whom I need, once in five, six weeks, you know, just to exchange notes. And if I feel that I've learned a thing or two, which I have with you almost all almost always end up going to meeting someone. If it's a conversation where you learn something about the world, then it's a good conversation.</p> <p>Nirant  28:45 Nice. Could you give a little bit of context on what backgrounds or professions these people come from other similarly entrepreneurial, or I think</p> <p>Ankit Nagori  28:55 -4 different backgrounds one, I definitely meet ex colleague from Flipkart, who is running this technology startup, I think I get to learn a lot when I meet that person. And second, you know, I think there's a friend who is into retail business. And I think, very, very good, very deep insight on consumer side. One colleague from Flipkart, who is now working with some other ecommerce company, I think these are 3-4 set of people who might meet and constantly learn.</p> <p>Nirant  29:27 That that's a lot of things to digest on. I think we'll wrap it up here, any parting things you want to tell you. That this is what worked for you cross multiple things e.g. Flipkart category additions, cure fit</p> <p>Ankit Nagori  29:46 Adding one thing, I always say that I was lucky to be in Flipkart at the right time. That gave us give all of us who are working at a time a very big platform. Post that I think just being very close to consumer. And being very sticky to the consumer insight has helped. And that's how it has been</p> <p>Nirant  30:04 Do you pay pay it forward by giving people opportunity like you got at Flipkart, at your firm?</p> <p>Ankit Nagori  30:11 Absolutely, absolutely. We look at very young people, people who have high intrinsics and just a lot of intent. So the average age of our team will be pretty young.</p> <p>Nirant  30:21 Sahi, Mast, thank you. Thanks a lot for your Saturday.</p> <p>Nirant  30:27 Say hi to your son for me.</p> <p>Ankit Nagori  30:29 Yes, absolutely. Okay. Thank you so much, and see you. Thank you, Nirant. Thanks for your time.</p>"},{"location":"writing/antiskill/","title":"Anti Skills","text":"<p>You learn a well-paying skill and years later - it comes back to hurt you in unexpected ways. That's an Anti Skill.</p> <p>Consider this hypothetical: You start your software engineering career and build a reputation as someone who is good at iOS development. Each year, the money you make keeps improving as you keep getting better at it. </p> <p>The downside? You'll find it hard to get job offers outside of iOS development [1]</p> <p>Note that this increased pay might still be less than even starting pay in some other fields, say Data Science -- but you've Golden Handcuffs on you now, don't you? </p> <p>Congratulations! You've hit a local maxima!</p> <p>But what if you're someone who enjoys doing iOS development?  I'm super happy for you! You'll most likely not just be good at it, but great at this and enjoy it.</p>"},{"location":"writing/antiskill/#what-is-an-anti-skill","title":"What is an Anti Skill?","text":"<p>The discussion gets more interesting when you consider that you're not just a software engineer anymore, but a mobile app developer. When you say \"I'm good at iOS\", the hiring market hears it as \"I know only iOS development\".</p> <p>Unnoticeable to you, the market forces have limited you to a mobile developer. {{&lt; tweet user=\"ponnappa\" id=\"1415323073444597761\" &gt;}}</p> <p>But when you started, did you know that this would happen? That learning these skills will reduce your optionality in the future?</p> <p>\"Anti Skill\" is a set of skills which when advertised take away your future choices. </p> <p>Anti Skills prevent you from adapting quickly to the changing environment around you. Anti Skills are skills that you don't want to tell people about. This has nothing to do with whether it's a good skill to learn or not. Some of these skills are actually good skills to learn e.g. React, SQL, Android/iOS from top of my mind.</p>"},{"location":"writing/antiskill/#what-can-we-do","title":"What can we do?","text":"<p>To prevent this, keep your identity small. That includes not self-identifying by a technology, platform or worse, a JS Framework. </p> <p>This will make it easier to see when/if you're stuck. Other way to say the same thing, if you think of yourself as a mobile developer, you might be better off thinking of yourself as a software developer specialising in iOS development.</p> <p>This way, when something is not working, you can see what's going on.</p> <p>If you're in a position where you're stuck at a local maxima -- resist the temptation to go up the ladder. Think of taking titles into your identity on as taking debt. Job titles are not your identity. They're prestige handcuffs, and you're not going to be able to get them off you easily.</p> <p>It might be harder to get out of these impressions at your existing job, because they tend to be sticky. In such scenarios, moving to a different team within the same company or a different company altogether might be preferred. I know of atleast one case where someone had to change jobs to get a 'soft' career reset.</p> <p>This reset, speaking from personal experience, is quite uncomfortable and requires you to fall down a skill-cliff and climb up again. If you're considering something like this, feel free to hit me up with your thoughts. I'm here to help.</p> <p>The worst of these scenarios is that in a few years from now, you are great at something you don't enjoy anymore. And now, it's too late to do something else because you're stuck with it. Act now!</p> <p>[1] Changing to a different role and/or adding skill sets might still be possible within the organisation you already work at. Lateral shifts are usually harder.</p>"},{"location":"writing/belief_arbitrage/","title":"Belief Arbitrage","text":"<p>This is a 3 part essay. </p>"},{"location":"writing/belief_arbitrage/#part-1-talent","title":"Part 1: Talent","text":"<p>In my teenage years, I first understood that the kind of family you\u2019re born into gives you access to certain kinds of wealth e.g. money, network, habits/knowledge and social capital to do things you want.</p> <p>Most people in my peer group and even among the adults I could speak to, used these resources to make their own life materially more comfortable.</p> <p>This is precisely what I wanted: material comforts, but unfortunately, I couldn\u2019t die and pick a family to be born again in. So I kept looking for other means and ways in which I could make it work for me.</p> <p>So I went hunting. I read a few biographies, centered across genres but mostly dead people viewed from specific lenses of entrepreneurship, learning, politics and the like. I also actively sought out and spoke to people in their 30s and 40s in careers similar to mine.</p> <p>I realised that sheer raw talent and finding a way to apply those talents is also a unique form of arbitrage. It makes people blissful in what we\u2019d now popularly call flow state. The happiest people I met were craftsmen and had high mastery. They also had a reasonable degree of autonomy over their own shape of the day.</p> <p>I was also in my early 20s by this and realized that I wanted to optimize for more control of my day and mental energy, and was more than happy to trade off some x% wealth over this.</p> <p>\u201cEureka!\u201d, I finally thought to myself. I\u2019d at last found a way, a strategy - if not a playbook which might work for me.</p> <p>The plan was arguably incredibly simple and had only 2 parts:</p> <ol> <li>be good at a specific craft, which is in demand (Machine Learning for me)<ul> <li>be good<ul> <li>implies that you need to be at least in the top 2000 people in your age group for that craft</li> </ul> </li> <li>in demand - implies that there should be at least 10K people in that craft already across the globe and different skill levels and a mechanism for them to move out i.e. retirement, outdated, or similar<ul> <li>people who matter should know that you\u2019re a skilled craftsman</li> <li>my work is recommended by the likes of Andrew Ng, Paul Romer and a NLP book which has sold over 1K copies. </li> </ul> </li> </ul> </li> </ol> <p>I think it\u2019d be fair that I executed this strategy of sorts to the best of my then-ability.</p> <p>In hindsight, I could\u2019ve definitely done 5-10x more but we\u2019ll leave regrets/lessons for a separate essay to retain the cheery narrative nature of this first person essay.</p> <p>If there is something you want to take away from this essay, it is this: Talent is a valid form of advantage, but not everyone can monetize/utilize this well.</p>"},{"location":"writing/belief_arbitrage/#part-2-genius","title":"Part 2: Genius","text":"<p>To recap, talent is one\u2019s ability to do something extremely well. I also argued that talent is the only form or advantage which I thought I could build to get material wealth in life.</p> <p>To me, the idea that Genius can be translated to material wealth is an axiomatic belief. So we\u2019ll not dwell on whether Genius is useful or not.</p> <p>In the late 2000s, a young man would show up every week with the same idea for approval from the management of this rapidly growing Internet startup. He had previously quit his job at an prestigious consulting company and closed those doors behind him by going to a smaller startup in a completely different role.</p> <p>For the idea which he had a lot of faith in, he was willing to bet this career and if not, at least his future prospects at that company. His bet? A new Internet explorer toolbar, which would ensure that users could find Google Search more easily and quickly. The man was Sundar Pichai.</p> <p>To many, that is and was a move of genius. This was a genius of not just invention, but also business, technology and user psychology. There is more than one flavour of genius for sure.</p> <p>Let\u2019s begin by understanding what we mean by genius first.</p> <p>Our cultural understanding of intellectual excellence, or more popularly, genius is extremely limited \u2013 and dare I say, harmful. We limit these to inventors, mathematical prodigies and their ilk.</p> <p>Alexey Guzey talks about this cultural misunderstanding here: https://guzey.com/intelligence-killed-genius/</p> <p>I know a few people who I believe to be geniuses. What happens when I tell them that I believe they\u2019re a genius? They all tell me that there are people smarter than them and that they\u2019re \u201conly pretty good at one or two things\u201d</p> <p>bitch this is exactly what genius is.</p> <p>Genius isn\u2019t limited to the ability to solve puzzles, or do rapid arithmetic, or Sheldon\u2019s eccentric quirks.</p> <p>Genius is having the ability, no \u2013 the advantage, to see things which others can\u2019t see.</p> <p>To me, as someone of not-superhuman intelligence, the question is this then:</p> <p>\u201cHow do I make the leap from Talent to Genius?\u201d</p> <p>The answer is in the Alexey Guzey essay itself: genius is having a vision.</p> <p>In sports, the line between talent and genius is often blurred. Is Michael Jordan a really talented player, or a genius for exploiting the blind spots of referees to his advantage?</p> <p>It\u2019s being able to make bets and act on them with unparalleled insight. It\u2019s executing. It\u2019s doing something which escapes the imagination of most people in the know. It\u2019s a taste for what could work, without being bound by tradition.</p> <p>As Arthur Schopenhauer summarized: Talent hits a target no one else can hit; Genius hits a target no one else can see.</p>"},{"location":"writing/belief_arbitrage/#part-3-courage","title":"Part 3: Courage","text":"<p>We touched upon a question of interest to us:</p> <p>\u201cHow do I make the leap from Talent to Genius?\u201d</p> <p>I don\u2019t have a complete answer yet. So let me offer the part of the puzzle which I am very confident about: Courage.</p> <p>It\u2019s a key ingredient, a necessary catalyst for one to make the leap from one to another.</p> <p>A natural question follows, what do we mean by courage? How do you recognize courage?</p> <p>Of course, there is the Die Hard version of running into a gun nest. That is a subset of courage.</p> <p>Courage also has some other flavors. Many of us recognize them almost instantly in the form of honor and duty.</p> <p>The other smell for spotting courage is the acting on something despite fear and/or knowledge of harsh consequences. Many outsized entrepreneurial and military ventures are built with this courage.</p> <p>There is also a lot of courage which stems from not caring about what people have to say. To neither be bound by shackles of history or tradition, but also respect Chesterton\u2019s Fence.</p> <p>It frees you to learn about reality from other people, but not be bound to the reality of today. An ability to envision a future state which others don\u2019t imagine. </p> <p>Pressfield\u2019s Resistance is the most dangerous element to one's life and dreams since its sole mission is to sabotage aspirations. The first step to fighting Resistance is knowing that you can, and you can win a battle here and another there. </p> <p>That is empowering you to action. That is a form of courage. </p> <p>It frees you to make bets that\u2019d make others shiver in fear of uncertainty. It frees you to act in ways congruent to your values and aspirations. To seek truth, beauty and craft above all else.</p> <p>For a craftsman mindset, to do their best work, the ideal state is where they can act freely. For that specific form of high agency, courage is a key ingredient, a catalyst. Without courage, no talent can truly do their best work. The leap is impossible from Talent to Genius. </p> <p>Courage in any flavor: pride, duty, honor, self-confidence is a foundational belief. </p> <p>This is a belief that not only is something achievable - but there might be a means and methods to do so. It is a specific form of Arbitrage against Reality. An advantage which you, my dear talented friend, now have over others that don\u2019t have the conviction, or the courage.</p>"},{"location":"writing/beyond-basic-rag/","title":"Beyond Basic RAG: What You Need to Know","text":"<p>The Real World of RAG Systems</p> <p>\ud83d\udcd2 Picture this: You're a developer who just deployed your first RAG system. Everything seems perfect in testing. Then reality hits - users start complaining about irrelevant results, not being able to do \"basic stuff\" and occasional hallucinations. Welcome to the world of real-world RAG systems.</p>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#the-problem-with-naive-rag","title":"The Problem With \"Naive RAG\"","text":"<p>Let's start with a truth bomb: dumping documents into a vector database and hoping for the best is like trying to build a search engine with just a dictionary - technically possible, but practically useless.</p> <p>Here's why:</p> <ol> <li> <p>The Embedding Trap: Think embedding similarity is enough? Here's a fun fact - in many embedding models, \"yes\" and \"no\" have a similarity of 0.8-0.9. Imagine asking for \"yes\" and getting a \"no\" instead in a legal search \ud83d\ude05</p> </li> <li> <p>The Context Confusion: Large Language Models (LLMs) get surprisingly confused when you give them unrelated information. They're like that friend who can't ignore a app notification while telling a story - everything gets mixed up. </p> </li> <li> <p>Length Effect: Just like humans tend to get worse at noticing details the longer a story is, LLMs with large context windows get worse at noticing details the longer the information is.</p> </li> </ol>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#the-three-pillars-of-production-rag","title":"The Three Pillars of Production RAG","text":"","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#1-query-understanding","title":"1. Query Understanding \ud83c\udfaf","text":"<p>The first step to better RAG isn't about better embeddings - it's about understanding what your users are actually asking for. Here's the basics:</p> <ul> <li>Query Classification: Before rushing to retrieve documents, classify the query type. Is it a simple lookup? A comparison? An aggregation? Each needs different handling.<ul> <li>NIT: Navigational, Informational, Transactional are the 3 very broad types.</li> </ul> </li> <li>Metadata Extraction: Time ranges, entities, filters - extract these before retrieval. Think of it as giving your students sample questions to help them pay attention to what's important in the exam (at query time) much better and faster. </li> </ul> <p>Metadata Queries</p> <p>The CEO of a company asks for \"last year's revenue\"</p> <p>The CFO asks for \"revenue from last year\"</p> <p>The CMO asks for \"revenue from the last fiscal year\"</p> <p>Do all these queries mean different things? Not really. The asker role i.e. query metadata changes the query intent.</p>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#2-intelligent-retrieval-strategies","title":"2. Intelligent Retrieval Strategies \ud83d\udd0d","text":"<p>Here's where most systems fall short. Instead of one-size-fits-all retrieval:</p> <ul> <li>Hybrid Search: Combine dense (embedding) and sparse (keyword) retrieval. You can rerank using late interaction, use LLM as a reranker or even use both in a cascade. I can probably write a whole blog post on this, but tl;dr is that you can use a combination of many retrieval strategies to get the best of precision, recall, cost and latency.</li> <li>Query Expansion: Don't just search for what users ask - search for what they mean. Example: \"Q4 results\" should also look for \"fourth quarter performance.\"</li> <li>Context-Aware Filtering: Use metadata to filter before semantic search. If someone asks for \"last week's reports,\" don't rely on embeddings to figure out the time range.</li> </ul>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#3-result-synthesis-and-validation","title":"3. Result Synthesis and Validation \u2705","text":"<p>The final piece is making sure your responses are accurate and useful:</p> <ul> <li>Cross-Validation: For critical information (dates, numbers, facts), validate across multiple sources at ingestion time. It's possible that your ingestion pipeline is flawed and you don't know it.</li> <li>Readability Checks: Use tools like the Flesch-Kincaid score to ensure responses match your user's expertise level.</li> <li>Hallucination Detection: Implement systematic checks for information that isn't grounded in your retrieved documents. Considering evaluating the pipeline using offline tools like Ragas</li> </ul>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#real-world-example-the-leave-policy-fiasco","title":"Real-World Example: The Leave Policy Fiasco","text":"<p>Here's a real story that illustrates why naive RAG fails:</p> <p>The Leave Policy Fiasco</p> <p>Company X implemented a RAG system for HR queries. When employees asked about leave policies,  the system kept used the entire company's wiki -- including that of the sales team.  And sales \"ranked\" higher because it contained similar keywords. </p> <p>The result? The entire company was getting sales team vacation policies instead of their own \ud83e\udd26\u200d\u2642\ufe0f</p> <p>The solution? They implemented:</p> <ol> <li> <p>Role-based filtering</p> </li> <li> <p>Document source validation</p> </li> <li> <p>Query intent classification</p> </li> </ol>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#making-your-rag-system-production-ready","title":"Making Your RAG System Production-Ready","text":"<p>Here's your action plan:</p> <ol> <li>Query Understanding: Implement basic query type classification</li> <li>Ingestion: Extract key metadata (dates, entities, filters)</li> <li>Retrieval: Begin with metadata filtering</li> <li>Retrieval: Add keyword-based search or BM25</li> <li>Retrieval: Top it off with semantic search</li> <li>Synthesis: Combine results intelligently using a good re-ranker or fusion e.g. RRF</li> <li>Validation: Cross-check extracted dates and numbers</li> <li>Validation: Implement a RAG metrics system e.g. Ragas</li> <li>Validation: Monitor user feedback e.g. using A/B tests and adapt </li> </ol> <p>Reciprocal Rank Fusion</p> <p>Reciprocal Rank Fusion (RRF) is a technique that combines the results of multiple retrieval systems. It's a powerful way to improve the quality of your search results by leveraging the strengths of different retrieval methods. </p> <p>But it's NOT a silver bullet.</p>","tags":["machine-learning"]},{"location":"writing/beyond-basic-rag/#the-challenge","title":"The Challenge","text":"<p>Stop thinking about RAG as just \"retrieve and generate.\" </p> <p>Start thinking about it as a complex system that needs to understand, retrieve, validate, and synthesize information intelligently.</p> <p>Your homework: Take one query type that's failing in your system. Implement query classification and targeted retrieval for just that type. Measure the improvement. You'll be amazed at the difference this focused approach makes.</p> <p>Remember: The goal isn't to build a perfect RAG system (that doesn't exist). The goal is to build a RAG system that improves continuously and fails gracefully.</p> <p>Your Turn</p> <p>What's your biggest RAG challenge? Let's solve it together. Let me know on Twitter or email.</p>","tags":["machine-learning"]},{"location":"writing/beyond-first-90-days/","title":"Beyond First 90 Days","text":"<p>This one's gonna be brief and echoes 2 Less Obvious Ideas to the younger me.</p> <p>I am assuming that you already know the hygiene factors: Make few promises. Keep most of them and exceed few of them atleast. Get to like the top 5% in the skill of effort estimation for your own work at the very least. And so on.</p>"},{"location":"writing/beyond-first-90-days/#contribute-to-developer-ecosystem","title":"Contribute to Developer Ecosystem","text":"<p>Improving any part of the developer ecosystem is useful and visible at the same time. For instance, let's say you add tests for a code path on which 10 developers are working. You've made the lives of 10 developers easier. They'll remember this when you come to them for help.</p> <p>For some projects/teams, even the build time is quite large and error prone. Any improvements there also save a lot of contributor or developer time.</p> <p>As Joel Spolsky (the person behind Stack Overflow) wrote: There is more than 1 way to help: </p> <ul> <li>Maintaining an issue tracker</li> <li>Write a decent functional specification</li> </ul> <p>You get the gist. Get creative and figure out points of leverage: low effort, high return on your time.</p>"},{"location":"writing/beyond-first-90-days/#engineering-brand-efforts","title":"Engineering Brand Efforts","text":"<p>You already know what are the 1-2 things your team's best is e.g. speed, scale, cadence, or software quality. </p> <p>Take those 2 topics and write down 5 reasons or points of evidence on why you think those are the 2 topics on which your team is best. For instance, if I was writing \"speed\" - one of my points would look like, we make 20 releases a week to almost 500K users. Or, we have fewer than 20 bugs for a release thanks to our amazing testing and QA friends. </p> <p>Now - expand these 5 points into a short, bullet point essay like this one. Ask your manager and other senior engineers for advice. You say something like, \"Hey, I wrote down what our team does best - do you think I captured the essence and reasoning?\"</p> <p>Done? </p> <p>Great, now go write this as an internal and external blog. Submit this at a technical conference which cares about the dimension on which your team is the best. Bringing accolades to the team, with their blessings is much higher returns than reading 10 Medium blogs.</p>"},{"location":"writing/bitsian/","title":"The BITS Pilani Experience","text":"<p>There is a invisible binding thread among most BITSian\u2019s - optimistic, easy going, but somewhat pragmatic. The sky is the limit, but most of them realize flying in hot air pockets is easy - it\u2019s the takeoff that hurts.</p>"},{"location":"writing/bitsian/#act-1","title":"Act 1","text":"<p>I guess there is a flavour of cheerful pragmatism that is uniquely BITSian. You can get ice cream and ice cream shakes anywhere - but Sharma\u2019s MNB is uniquely BITSian.</p> <p>I am no longer in BITS Pilani. I have never lived in places where my parents grew up. I am a thousand miles from the evening sea breeze where I grew up. Yet, the aroma of each of these places is fragrance to my heart. I carry them within me. And spread them as I meet other thawing hearts.</p> <p>There is utility of history beyond nostalgia. It is the greatest teacher if you are willing to listen. Just listen.</p> <p>I learned to love the Past in BITS Pilani.</p> <p>The strong undercurrent of impostor syndrome sweeps too many into the arms of Erebus. Erebus is darkness incarnate.</p> <p>A teacher used to tell that always hire the second ranker from any college. Why? Because he has the most fire in his belly.</p> <p>I entered BITS Pilani hoping to find an identity for myself. Penance for my \u201cpoor\u201d JEE performance. In some ways, BITS Pilani is what happens all seconds of world\u2019s most rigorous examination system are locked in a desert jail.</p> <p>Do you want to build a robotics company? A drone company? Or participate in the Hyperloop competition? Or participate in a business competition? Win that Microsoft code.fun.do hackathon? Learn to play the guitar? Date the cute girl in your DSA class?</p> <p>There is someone who can do each of the above better than you: a national level tabla player, the internationally acclaimed actor, the accidental GSoCer, the genius hacker, the CXO of a $0.5M company - and that cute girl in my DSA class had a boyfriend studying CS at IIT Delhi or UC Berkeley.</p> <p>The odds are there is someone who is doing each of the above better than you.</p> <p>The odds are there is someone who is doing all of the above better than you.</p> <p>It\u2019s a sport. It\u2019s a race. You feel like ambition is a virtue. Isn\u2019t it?</p> <p>I left knowing that my search for identity is not only on the outside. It\u2019s not out there, some of it is inside.</p> <p>Ambition is not a virtue. It might even be sin.</p> <p>I learned to question virtues themselves in BITS Pilani.</p> <p>The Japanese have a strange art called Kintsugi. As per the art, you repair broken vessels with something precious, such as gold. The practice is related to the Japanese philosophy which calls for seeing beauty in the flawed or imperfect.</p> <p></p> <p>If students are vessels to be filled with wisdom and knowledge, Pilani shatters them before it repairs them. Some don\u2019t break, yet they are chipped away. The weather, the culture, the competition.</p> <p>Sometimes when I talk to my old friends from Pilani, I can see the cracks. I can see the glimmer where the hallowed walls broke them, as if a rite of passage or sacrifice. They fall silent, their voice quakes and sometimes they just nod.</p> <p>Then, there is a faint smile. That glimmer?</p> <p>That\u2019s gold. It\u2019s magic.</p> <p>-- Nirant,  2012C6PS694P</p> <p>Cover Image Credits: Ravali Priya Gandhapuneni</p> <p>Originally appeared as an answer to How is your experience at BITS Pilani? What did you get? What did you expect? on Quora in 2017</p> <p>If you want to hear more from me, get my words in your email</p>"},{"location":"writing/breaking-into-nlp/","title":"Breaking into NLP","text":"<p>Bulk of this is borrowed from notes made my teammate and friend at Verloop.io's NLP/ML team of our conversations. I've taken the liberty to remove our internal slang and some boring stuff.</p> <p>I want to build a community around me on NLP. How can I get discovered by others?</p> <p>Broadly speaking, the aim in forming connections can be split into Long Term and Short term. A short term aim would be where you can receive something immediate out of the connections or a particular connection itself. This could be a collaboration, correspondence, recommendation/advice or anything else.</p> <p>A more long-term, strategic aim would be a well defined long term goal that requires multiple steps to achieve. A strategic aim could involve multiple tactical steps. This is also, what we like to call friendship in some polite-speak areas of the world. </p> <p>I have no immediate goals or projects, just need some basic ideas on how to be a part of the ML community.</p> <p>Find interests of people and do something for them. Many people simply ask questions on Twitter, or you can infer what they are interested in looking at their Linkedin/work and their personal blogs. </p> <p>What would be a good starting point for this?</p> <p>A very easy thing to start with is literature review. Specially, for new topics being researched by influential people in the field. A good literature review shows your interest and willingness to help. Opens door to communication.</p> <p>A good place to find what topics are missing a decent literature review:  Go through NLP reddit r/LanguageTechnology or subreddits for Deep Learning, Machine Learning and so on. </p> <p>Or go through twitter. And help people out there. Answer their question with depth. Do not rush to be the first, but the best. When it comes to technology, almost all platforms behave a bit like StackOverflow, the right answer might not get accepted: but it'll get noticed. Btw, lot of the Huggingface contributors happen to be active on Github and Twitter both. Hanging around on their Slack can't hurt either. </p> <p>But important thing, try and stick to one medium. The place where you are most at home and gels with your personality. This could actually even be Youtube if you're an English-fluent, attractive looking person. </p> <p>The other reason you need to stick to one medium is that audience will spend most of their time on 1 or 2 social media channels. </p> <p>If they see that your content is not that popular on the other channel - They will do the cross posting for you. For instance, we've both seen Twitter content even within ML such as the Gary Marcus debate and attack on Yann LeCun spill over on reddit. And of course, people are still posting Tweets on TikTok!</p> <p>Word of mouth will be your biggest friend.</p> <p>Find problems that many people face. Usually a simple problem faced by many is a great problem statement. The python <code>requests</code> library comes to my mind as an excellent example of such a challenge. The work by <code>gensim</code> around shallow vectorization methods like word2vec and Glove was also quite similar in vein for quite a lot of time. Of course, with the rise of Deep Learning and better tooling makes their work less important - but they stuck in my mind, didn't they?</p> <p>Why is that a great problem statement?</p> <p>It's maximising the area under the curve. Solve a trivial problem faced by many or a huge problem faced by some. It has the same impact.</p> <p>What's something that has worked for you in finding interesting problems?</p> <p>Find intersections with domains that have little to do with each other. For us, there are domains that have little to do with tech/code and can see great benefits from our involvement.</p> <p>Marketing yourself has nothing to do with marketing but everything to do with the problems you solve and the solutions you come up with. Make sure the solution is accessible to the wider audience. It should not be that only a certain section of the population can use it. If you plan to market yourself, spend 95% of the time on a quality problem and a quality solution and 5% of time talking about it. This is usually enough if the first 95% is done well.</p> <p>What medium to talk about these in?</p> <p>The usual are the blog posts or social media posts etc. But there is an open secret within the community. Writing papers is probably the best way to talk about stuff you\u2019ve done.</p> <p>Why so?</p> <p>Papers have the halo effect. It improves your reputation and makes it sticky. People might forget a blog post quickly but you can get recognition/perks for around 2 years or so after writing a paper. There are other secondary gains too from doing this. Once you write a paper, you start reading papers differently. You have a better intuition of reading between the lines to understand the author\u2019s intent/pov. Another obvious benefit is you get better at writing papers. Your thought process will start coming across much more clearly.</p>"},{"location":"writing/building-a-data-science-team/","title":"Building a Data Science Team at a Startup","text":"<p>Hello! </p> <p>If we are meeting for the first time, a short version of my story so far: After doing research engineering for almost 4 years across startups and a BigCo, I joined as an early machine learning engineer at Verloop.io - a B2B startup that makes customer support automation SaaS in 2019. I was there till April 2021. </p> <p>We were directly responsible for most Natural Language Processing needs within the business.</p> <p>While there is plenty of good advice on making ML work and making a career as a Data Scientist, I wanted to write my experience as a manager/early engineer who built out a ML team at a startup. </p> <p>I hope you find it useful!</p>"},{"location":"writing/building-a-data-science-team/#who-is-this-for","title":"Who is this for?","text":"<p>People with these questions:</p> <ul> <li>\"What would it be like to be a Data Science Manager?\"</li> <li>\"How can I build a Machine Learning team from scratch?\" <ul> <li>This is typically founders e.g. CEO/CTO looking to build out a Machine Learning function </li> </ul> </li> </ul> <p>Bonus: Beliefs</p>"},{"location":"writing/building-a-data-science-team/#hiring","title":"Hiring","text":"<p>Hiring for Machine Learning Engineering is hard. </p> <p>Despite what the media might tell you about the glut of data science talent. There are too many people who can model churn well or do the Titanic dataset right with Logistic Regression, not enough people who can come up with a simple TF-IDF model in a language they've not ever worked with e.g. not English. </p> <p>In Summer of 2019, I did over 120 introductory calls between February and August. Averaging, 6 calls every week from April to June. This converted to 1 summer intern, and 3 full time people. </p>"},{"location":"writing/building-a-data-science-team/#one-role-to-rule-them-all","title":"One Role to Rule Them All","text":"<p>For &gt;1 year, we have hired for exactly ONE role: Machine Learning Engineer. </p> <p>As explained in our MLE Prep Guide, we collapsed several roles into one. This means we don't hire a specialized Data Scientist, Researcher and then separate Product and Data Engineers to productionize the models. </p> <p>At Verloop, the same engineer takes the entire system live from research to production, and occassionally several months into maintenance and growth as well.</p> <p>This allows us to keep the hiring process straightforward, highly repeatable, while still maintaining enough faith in the process.</p>"},{"location":"writing/building-a-data-science-team/#t-shaped-skill-maps","title":"T Shaped Skill Maps","text":"<p>Our hiring process evaluates all candidates on primarily 3 skills:</p> <ol> <li>Low Level API Design for Web Services</li> <li>Programming Mindset and Quality</li> <li>Natural Language Processing/Machine Learning Skills (see Prep Guide if you're curious)</li> </ol> <p>I biased the top of the funnel of the interview process to hire a team of compliments around my weaknesses.Each compliment should bring in a skill which is my weakness, but that individual's strength. </p> <p>Of each of the first 3 full time hires, each person was atleast 2x better than me on that skill. This is important as each person has a multiplier effect on the productivity/quality of other developers on the team. </p> <p>To give you a sense of these:</p> <ol> <li>Person A has clear, proven strengths in High Level System Design and Databases</li> <li>Person B has clear, proven strengths in DevOps and Cloud Deployments</li> <li>Person C has clear, proven strengths in \"hacking\" with Deep Learning, optimizing for speed</li> </ol> <p>This input biasing seems to work amusingly well because it enables developers to always keep learning from each other. </p> <p>For instance, one of the things which I have asked from each team is a load test of their service - this led one developer to simply build an internal wrapper around the popular <code>locustio</code> which works with our service design out of the box. </p> <p>It frankly makes my job a lot easier if developers do amazing work without me pushing them to do it - specially by observing each other. I can honestly sit back and simply steer their curiosity, instead of investing my attention into solving the \"motivation\" problem.</p>"},{"location":"writing/building-a-data-science-team/#process","title":"Process","text":"<p>Our hiring process consists of two primary rounds: a programming challenge and ML challenge. You can actually take a deeper look here: Verloop ML/NLP Interview Prep Guide.</p> <p>The programming challenge is a take home exercise which is focussed on low level design and straightforward API design. We typically give 2-3 days to the candidate for this. </p> <p>For the ML Challenge, we share a dataset for a take home challenge and then discuss the same with the candidate. This round has no right answer. It is deliberately open ended. </p> <p>It gives us a lot of signals on a wide variety of things we care about, for instance:</p> <ul> <li>how the candidate formulates the problem,</li> <li>measures the model performance,</li> <li>thinks about model selection and important of loss functions,</li> <li>prior/acquired experience with real world datasets,</li> <li>literature review/comfort with close to research work,</li> <li>ability to write clean, readable code,</li> <li>whether they include failed experiments, indicates their confidence with sharing honest results</li> </ul> <p>To me, what has suprised most is the number of otherwise skilled people who use \"this is what I saw on Medium\" as a valid explanation for selecting a particular approach. This lack of agency (autonomy?) is a red flag.</p>"},{"location":"writing/building-a-data-science-team/#onboarding","title":"Onboarding","text":"<p>I am almost devilish when it comes to designing the best possible onboarding experience that I can. In the case of 1 intern, where I let it go off my radar - I think we didn't do our best as a team. The onboarding at Verloop ML consists of two specific pieces:</p>"},{"location":"writing/building-a-data-science-team/#1-before-joining-verloop","title":"1. Before Joining Verloop","text":"<p>We share 6-8 week learning calendar focussed mostly on Deep Learning and Natural Language Processing. </p> <p>Each week is expected to take 8-10 hours of your time (but interns have told me that it takes closer to 30 hours) - and then I get on with them on a 1-1 call and discuss what they did well and what they missed. I share context from research or our own systems when relevant. One core byproduct of this onboarding calendar is that the candidate should get very comfortable writing tons of experimental code of varying quality. </p> <p>This onboarding calendar is custom designed to each candidate, depending on their strengths which I should ideally understand during the interview process. </p>"},{"location":"writing/building-a-data-science-team/#2-after-joining-verloop","title":"2. After Joining Verloop","text":"<p>The onboarding process for each engineer is customizer to their weaknesses. For instance, an engineer coming from a stronger systems background will be first given a project which is mostly data cleaning and benchmarking ML models. So that they can get a deeper, more intimate understanding of how experiment design and evaluation works. </p> <p>Similarly, freshers out of college, who typically come from weaker engineering backgrounds (but stronger DL skills) - will spend the first few months paying off tech debt, learning to read legacy code or building new CRUD services. </p> <p>This is obviously in addition to the one heuristic which I've tried to follow: Ensure that every engineer gets one release into production within 4 weeks of joining Verloop.</p>"},{"location":"writing/building-a-data-science-team/#stakeholders","title":"Stakeholders","text":""},{"location":"writing/building-a-data-science-team/#managing-up","title":"Managing Up","text":"<p>A common refrain from most senior ML people I spoke to is that: Leadership does not understand AI/Data Science</p> <p>To me, this has always been an opportunity than a handicap. The most inspirational to me in this sense is the work of the likes of DJ Patil. He is the guy who coined the term, \"Data Scientist\" and was the Chief Data Scientist of the United States of America under Obama administration. </p> <p>I somehow think that an analytical &amp; numerical leadership can be worked with, independent of their own training within the domain. For instance, I don't think Barack Obama can tell a random forest from a convolutional neural network.</p> <p>Communication and convincing non-experts outside your domain is always hard, painstaking and tedious. I should clarify to say that I don't think this is an easy challenge and, hopefully, our peers in Design will hopefully agree. I think that it's worth the effort.</p> <p>Here is what I would want to do in the future to make this better:</p> <ol> <li>Highlight opportunities e.g. this can be our moat/IP or unlock new value with the caveat: \"if it works\"</li> <li>State assumptions e.g. cost, development time -- this gives you a feedback loop on your assumptions as well </li> <li>Call out checkpoints in \"state of work\" and not timeline. E.g. better to say, once we have done 5 experiments instead of 5 weeks, since you might end up realising that cleaning the data itself is going to take 3x as long</li> </ol> <p>It'd be extremely stupid to assume that any of this would have been possible without the high degree of support and autonomy from the CEO, Gaurav himself. </p> <p>As much as I'd like to say/think that I earned that unfailing trust, Lord knows that I have made some messes which he had to clean up.</p>"},{"location":"writing/building-a-data-science-team/#managing-people","title":"Managing People","text":"<p>Of all things, I have received more support here than I deserved here - and I'm truly grateful for that. I think I have made quite a few people/psychology mistakes here. For instance, assuming that people want to be pushed and given maximum autonomy possible, instead of being led and they build mastery on their craft instead. </p> <p>I also found myself being extremely angry at quite a lot. Although I'd read Andy Grove's notion of Task Relevant Maturity, I don't think I did anywhere a decent job of implementing it. </p> <p>Managing my own mental state has been more work than I'd expected. My blast radius is much larger -- and deeper than I'd expected. I am sure there are plenty of people with far more nuance, patience and empathy who'd have done a better job at this.</p> <p>Early on, I'd decided that I'd not repeated any of the mistakes that my previous managers had made. I ended up making a different version of the same f**king mistakes anyway. </p> <p>After &gt;1 year, the only thing which works for me is to listen to what people want and then do that.</p>"},{"location":"writing/building-a-data-science-team/#reading-reccos","title":"Reading Reccos","text":"<p>There is so much good written about People Management and Engineering Management in general, that I'd be stupid to add to that clutter. Instead, I should point out books that have shaped how I think about Engineering/ML managment as well: </p> <ul> <li>The Manager's Path by Camille Fournier</li> <li>Effective Engineer by Edmond Lau</li> </ul> <p>On Managing Myself:</p> <ul> <li>Managing Oneself by Peter Drucker</li> <li>Standout 2.0 by Marcus Buckingham</li> </ul> <p>Books which are highly recommended, but didn't help me enough:</p> <ul> <li>The Elegant Puzzle by William Larson</li> <li>Randical Candor by Kim Scott</li> </ul> <p>Books which will probably have high impact, but in the future:</p> <ul> <li>HBR's 10 Must Reads: On Communication</li> <li>Effective Executive, by Peter Drucker</li> <li>Nonviolent Communication, by Rosenberg</li> </ul>"},{"location":"writing/building-a-data-science-team/#data-science-management","title":"Data Science Management","text":"<p>What we did: </p> <ol> <li>Encouraged every team to manage the project on their won by few key metrics, <ul> <li>Seprated out goal and minimum metrics</li> </ul> </li> <li>Measuring the metrics at some cadence, even if this was erratic in the beginning</li> <li>Each team drives their entire process from research to production to deployment<ul> <li>This encourages teams to think about engineering challenges pretty early and gives them reasonably high autonomy</li> </ul> </li> </ol> <p>What we should have also done: </p> <ol> <li>Encourage every team member to spend time data-ing i.e. exploring datasets, building a mental model around it, tagging it on their own </li> <li>Introduce Engineering Practices Early: Stronger emphasis on software engineering practices once they joined the team e.g. doing TDD, code hygiene, </li> <li>Have every </li> </ol>"},{"location":"writing/building-a-data-science-team/#peer-management","title":"Peer Management","text":"<p>While I'd expected this to be easy, this turned out to be quite emotionally hard. Since I transitioned out of backend engineering roles pretty early in my career - there are entire topics and concepts which I am not great at. This is made worse by the fact that I'm familiar with them, but not comfortable. </p> <p>So if 2 developers are discussing something, I can very well follow their conversation - but I don't have anything to contribute. </p> <p>This is quite frustrating. The sheer, persistent feeling of incompetency. Luckily, I don't have too much of a self respect/ego to care about this. I've always gravitated towards the best what I can do -- and what others cannot do well. At Verloop, that is Machine Learning/Data Science Management right now.</p> <p>That said, within Verloop - ML has been the first adopter of almost all new dev tooling:  - alerting and monitoring systems - inhouse logging library to improve our microservice observability - porting from previous Proto serving solution to twirpy</p> <p>This is atleast partially because I didn't want the devs in my team to suffer. </p>"},{"location":"writing/building-a-data-science-team/#beliefs","title":"Beliefs","text":""},{"location":"writing/building-a-data-science-team/#dont-be-clever","title":"Don't be Clever","text":"<p>Machine Learning is a game where 87% models never go to production - almost every model we have picked, has gone to production.</p> <p>I've worked in B2B SaaS companies, doing Machine Learning research, engineering and deployment for almost 4 years. Hence, I've some opinions on what doesn't work. I don't have strong opinions on what works though.</p> <p>I encourage my teammates to come up with new ideas and propose everything from the annotation process to the alerting and monitoring configuration. The only place where I really intervene is if the rigour is absent, or they're solving for a problem without complete context.</p> <p>So far, this mindset of simply trying to avoid mistakes, instead of trying to be clever has worked well. I suspect as long as ML deployment is a failure-prone business, this mindset will serve me well. If something like GPT3/GPT4 lowers the risk considerably well, I will have to adopt a different mindset altogether.</p> <p>This has not always worked. We've made some stupid (in hindsight) bets which didn't work for a multitude of reasons, including our own overconfidence in our technical skill and market reasons.</p> <p>For those who are into the risk vs uncertainty nuance, I should add that I think because we deal with narrow Machine Learning problems - we mostly deal with technology risk and not so much uncertainty. It can be quantified, estimated and analyzed - it's just that I don't have the training to do so formally. </p>"},{"location":"writing/building-a-data-science-team/#premortems","title":"Premortems","text":"<p>Pre-mortems is a habit/tool which is still somewhat better known in Product Management than Data Science. In fact, it's better understood in Military Strategy than Data Science: </p> <p>He who knows his enemy ... - Sun Tzu</p> <p>I typically list down the top 3 causes which will kill a project - and then actively monitor them till the project is so stable that I can pay attention to something else. Despite my best attempts, I have failed to inculcate this mindset in devs working with me. </p> <p>It seems to be that discipline and optimism are fairly orthogonal mindsets in software engineering culture at-large.  To them, I remind that Microsoft Teams, TikTok, IRCTC and Instagram have shipped better than some of our B2B \"Scaling\" Engineering teams. In a world lost to chaos, I think the disciplined optimism ethos is basically a competitive advantage in my line of work.</p>"},{"location":"writing/building-a-data-science-team/#do-less","title":"Do Less","text":"<p>I am a low key fan of Auren Hoffman's advice that great things come from focus and not from building optionality. In the design and selection of tasks which our team has picked, this has been my guiding idea. </p> <p>Almost every Data Science team can be considered to be built to serve two \"purposes\": Analytics and Product</p> <p>Analytics teams contribute in two ways:  1. Inform decision makers 2. Measure and monitor internal metrics</p> <p>At Verloop, the Analytics function is purely owned by Product Management. We assist but don't own any outcomes.</p> <p>We build a reasonable number of ETL and Data Exploration tools for our own use (e.g. a Metabase installation) - which we make available to every Verloop employee, but we don't own the outcomes.</p> <p>This also has a direct bearing on our team size, scope and skill set: We don't need to hire anyone to handle your common churn, forecasting or similar insights problems. Everyone is a competent, contributing NLP Engineer.</p>"},{"location":"writing/building-a-data-science-team/#in-hindsight","title":"In Hindsight","text":"<p>In the last 18 months, our team has grown from 1 engineer (me) to 6 engineers. When I joined, ML was a blocker for the wider org with both latency and performance challenges, which quickly compounded because of legacy code and engineering exits. </p> <p>We were behind the curve where Machine Learning was seen as a cost center. </p> <p>Today, almost 18 months later, we're almost definitely ahead of the curve in terms of shipping. In the best case scenario, we can also become a profit center in as early as 6-12 months. </p> <p>Similarly, there is still a lot of room when it comes to our ability to explore quickly and prototyping production-grade software faster. We can shrink this from present 6-8 weeks to 1-3 weeks in the best case scenarios.</p> <p>A large part of the impact comes from our excellent customer support team, product and engineering. Machine learning is a mere amplifier of what they already do well.</p> <p>This has been one of the most fulfilling and hard things I have done. </p> <p>If you are considering a career in Data Science, I hope this helps you see beauty and effort beyond our love for data and ever increasing technical intricacies.</p> <p>Till we meet again, </p> <p>Nirant</p>"},{"location":"writing/case-for-open-inference/","title":"The Case for Open Inference: What Signoz is missing","text":"<p>There\u2019s a quiet but important debate happening in the LLM observability world \u2014 whether developers should stick with OpenTelemetry (OTel) or move to OpenInference, a spec built by Arize for tracing LLM workloads.</p> <p>Most people frame this as \u201cOTel vs OpenInference,\u201d as if one will win and the other will vanish. That\u2019s the wrong lens. The real question is: what are you trying to observe \u2014 systems or reasoning?</p> <p>If your product is a traditional service with predictable APIs, OTel works beautifully. But once your system starts reasoning, calling tools, and chaining LLMs together, you\u2019ve left OTel\u2019s comfort zone. That\u2019s where OpenInference comes in \u2014 it extends OTel with span types and attributes that actually make sense for LLMs: token usage, cost per request, time to first token, and tool calls. It\u2019s not competing with OTel \u2014 it\u2019s extending it into a new domain.</p> <p>So when I read the recent Signoz essay arguing that \"OTel is enough for LLM observability\" I disagreed \u2014 not because OTel is wrong, but because the framing misses what's fundamentally new about GenAI workloads.</p> <p>OpenInference is an enriched version of OTel built specifically for LLMs. It offers more specific span types for LLMs, including LLM, tool, and agent. It is first and foremost built for developers to use in their own LLM-powered applications, where they want to log LLM choices and actions. OpenInference was designed to be complementary to OpenTelemetry GenAI <sup>1</sup>, not an alternative. It already provides OTel-friendly instrumentations and semantic conventions.</p> <p>As an example, the metrics which OpenInference takes seriously are: Token usage, cost per request, time to first token, tool call rates. And the original OTel cares about the RED metrics <sup>2</sup> e.g. latency, error rates, and throughput.</p> <p>Signoz is not alone in making a case for OTel against OpenInference. The main case is quite simple: OTel is a well-established standard with broad language support, while OpenInference is newer, has limited adoption, and its OTel compatibility is superficial.</p>"},{"location":"writing/case-for-open-inference/#the-case-for-openinference","title":"The Case for OpenInference","text":"<p>I disagree with the main case for OTel against OpenInference: compatibility triumphs everything else.</p> <p>If this was the case, we should not have needed anything except Postgres for structured data. From ClickHouse to Snowflake, we have a lot of differentiation in the usecases and tradeoffs. And that is precisely the case with OpenInference and OTel.</p> <p>To extend this metaphor: Every system adopts the SQL dialect to make it their own. Like SQL dialects, OTel and OpenInference share a syntax but serve different workloads. If you\u2019re building an LLM product that needs to explain its reasoning, optimize latency, or debug tool calls \u2014 OpenInference gives you visibility that OTel simply can\u2019t today:</p>"},{"location":"writing/case-for-open-inference/#openinference-is-a-better-choice-for-agentic-systems","title":"OpenInference is a better choice for agentic systems","text":"<p>Agents are products where LLMs are in the driver's seat and not the developers. All modern agents <sup>3</sup> have this property. This means that the debug and RCA loop is much much faster and easier with OpenInference. </p> <p>For instance, when an agent fails, a standard OTel trace might show a single long-running <code>process_request</code> span. An OpenInference trace, however, would immediately break this down into <code>LLM</code> -&gt; <code>TOOL_CALL(search)</code> -&gt; <code>RETRIEVER</code> -&gt; <code>LLM</code> spans, instantly revealing that the failure occurred because the search tool got stuck in a loop or something in the query rewriting step needs to improve.</p> <p>This nesting visualization is also quite powerful and something I deeply appreciate when trying to debug agents:</p> <p> Source: Arize Phoenix Tracing Documentation</p>"},{"location":"writing/case-for-open-inference/#logs-as-analytics-data","title":"Logs as Analytics data","text":"<p>OpenInference logs are actually data meant for not just alerting 500 and 429 but for product analytics and observability. This means that the data is actually meant to be used for more than just API alerting.</p> <p>This shift towards product analytics is critical because we lack traditional user intent signals, like 'click events,' in conversational interfaces. With the high effort of text and voice interactions, we can't rely on the law of large numbers.</p> <p>For example: With semantically rich spans for <code>TOOL_CALL</code>, a product manager can finally answer questions like, 'Which tools are my users' agents invoking most often?' or 'Are users getting stuck in a loop trying to use the calendar tool?'. These are product questions, not just engineering alerts, and they are invisible in a standard RED metrics dashboard.</p> <p>The third reason is a little different: I think it's useful to revisit why OpenInference even exists: OTel didn't have a decent spec for GenAI workloads! It is on its way to have one, but as modern APIs change e.g. OpenAI went from Chat Completions to Responses API <sup>4</sup>. As a specialized, single-focus project, OpenInference is structurally positioned to adapt to the rapid evolution of GenAI APIs far more quickly than a large, consensus-driven standards body like OTel.</p>"},{"location":"writing/case-for-open-inference/#what-can-openinference-do-better","title":"What can OpenInference do better?","text":"<ol> <li> <p>OpenInference is completely maintained and developed by Arize.ai. It is not a community effort, despite being Apache 2.0 (the project's contributors reveals this quickly<sup>5</sup>). This single-vendor stewardship creates a natural friction against broader adoption.</p> </li> <li> <p>The most effective way for Arize to counter this would be to aggressively pursue OTel compatibility, proving that OpenInference is a good faith extension of the ecosystem, not a replacement aimed at vendor lock-in. This would mean that all OTel logging frameworks and tools should be able to use OpenInference, and hence switch to Arize Phoenix from other logging frameworks and tools. </p> </li> </ol>"},{"location":"writing/case-for-open-inference/#putting-myself-in-others-shoes","title":"Putting myself in others' shoes","text":"<p>From Arize's perspective, delaying the push for OTel compatibility might be the right move. The standard hasn't matured fast enough, and moving independently lets them stay ahead of the curve and address developer needs more quickly.</p> <p>If I were at Signoz and wanted to own GenAI logging and alerting, I'd create a Signoz plugin that accepts both OpenInference traces and OTel GenAI <sup>1</sup> traces, offering the same nested visualization capabilities that make Phoenix compelling.</p>"},{"location":"writing/case-for-open-inference/#conclusion","title":"Conclusion","text":"<p>Ultimately, the debate isn't about OTel versus OpenInference. We should acknowledge that observing generative AI is a fundamentally new problem that requires more than just knowing if an endpoint is healthy. We need a richer vocabulary to understand what our applications are doing and why. And the strategic bet is clear: LLMs are not another web service \u2014 they\u2019re runtime decision-makers. And observing them needs a richer language than latency, error, and throughput.</p> <p>If OTel is the lingua franca of distributed systems, OpenInference is the dialect for reasoning systems. The two are not competitors \u2014 they\u2019re layers. OTel tells you how your system behaves, OpenInference tells you why.</p>"},{"location":"writing/case-for-open-inference/#references","title":"References","text":"<ol> <li> <p>OTel GenAI Spec \u21a9\u21a9</p> </li> <li> <p>RED metrics \u21a9</p> </li> <li> <p>A good alternative definition is a LLM while loop with tools: https://www.braintrust.dev/blog/agent-while-loop\u00a0\u21a9</p> </li> <li> <p>OpenAI built the Responses API to support multi-modal and tool calling as first class concerns: OpenAI blog on Why Responses API \u21a9</p> </li> <li> <p>Most contributors to Arize's OpenInference are all their employees, with top 3 contributors coming from their own DevRel team. Source: Github Contributions \u21a9</p> </li> <li> <p>Law of large numbers makes clicks way useful for SaaS and consumer applications alike\u00a0\u21a9</p> </li> </ol>"},{"location":"writing/character-is-net-worth/","title":"Character is Net Worth","text":"<p>A useful metaphor to think about character of a person is their financial \u201cnet worth\u201d. Like your net worth, this does not mean that you have that much cash to spend. It is a sum total of something.</p> <p>Net Worth is the Sum Total of the Assets &amp; Liabilities</p> <p>Character is the Sum Total of the Decisions you Own: Good and Bad</p> <p>Like your Net Worth can be negative due to a large liability e.g. debt, your Character can be on shaky grounds in uncertain times. This is not necessarily a bad thing. It\u2019s only through debt (cash infusion more precisely) that businesses grow\u200a\u2014\u200aand it\u2019s only through uncertain times that Outstanding Character is built.</p>"},{"location":"writing/character-is-net-worth/#compounding","title":"Compounding","text":"<p>Character, like your Net Worth takes time to compound. If you break the momentum e.g. by shying away from tough decisions-you\u2019ll have to take a heavy hit.</p> <p>Much like Net Worth, Character can be re-allocated. In fact the only way grow it, is to take risks and make decisions fraught with moral/ethical complexities.</p>"},{"location":"writing/character-is-net-worth/#freedom","title":"Freedom","text":"<p>The upside of having outstanding character is that it\u2019s freeing. If you can trust yourself that you will not succumb to specific temptations. If you know that you won\u2019t get caught in elaborate traps set by demons, you can go where even angels fear to tread. It deepens with which you can embrace the world and yet stay untarnished with it\u2019s malice. You can walk among King and it\u2019s impostor courtiers in the morning and yet not lose touch with the common folk.</p>"},{"location":"writing/character-is-net-worth/#sharing","title":"Sharing","text":"<p>Someone can steal your money. Or you can give it away. Share it with your loved ones as well.</p> <p>Character is as much as part of your body as your DNA\u2014hard to steal or give it away. It is for you. You cannot inherit it wholly. You cannot transfer it wholly.</p> <p>You might ask, but what if I endorse someone ?</p> <p>It\u2019s time we talked about Reputation, and what sets it apart from character. Like character, Reputation takes years to compound. But the similarities end there.</p>"},{"location":"writing/character-is-net-worth/#reputation-is-insurance","title":"Reputation is Insurance","text":"<p>The apt metaphor for reputation is that it\u2019s a form of insurance. If you fuck up, or the fates are unkind on you\u2014your reputation insulates you from certain kinds of damages.</p> <p>Having a good reputation gives you a chance for a do-over. A good character actually gives you the strength to ask, earn and make that do-over work</p>"},{"location":"writing/character-is-net-worth/#shared-and-specific","title":"Shared and Specific","text":"<p>Reputation, much like insurance is often expanded to cover others. For instance, Venture Capital is a game which employs endorsements and introductions.</p> <p>Moreover, it can be specific e.g. professional, personal, family and so on. Much like insurance, you have to keep showing up so ensure that it doesn\u2019t deplete or expire.</p> <p>The other thing is, reputations are built on the perception that others have of you\u2014thus it\u2019s that more fickle. Not only can people be fools, their perceptions are influenced by what they think of themselves more than what you are.</p> <p>People\u2014are black mirrors. They do reflect what you are, but they tint it with their own world view. That is why reputations are always distorted</p> <p>But the decision that defines us: You can either be someone, or do something.</p> <p>Being someone means that you decide in favour of how your decision will be perceived\u2014and this enables your reputation to survive and grow. Doing something implies that you risk your reputation so that you can act in consistency with your own internal validation.</p>"},{"location":"writing/character-is-net-worth/#identity-is-debt","title":"Identity is Debt","text":"<p>Things which you consider \u201cinnate\u201d to you or parts of your identity is debt. The longer something is part of your identity, the more likely is someone will use it to make you a slave to their ideas.</p> <p>An abstract example: consider the political polarization today. I can bet that large fraction of people on both sides are good, well meaning individuals. So where does the vitriol come from? Hacking parts of your identity e.g. \u201cprogressive\u201d, \u201cchristian\u201d, \u201cpatriotic\u201d, \u201cnationalistic\u201d to their own ends. They persuade you to abandon your own ideals!</p> <p>The earlier in your life these ideas/ideals were taught to you\u2014the more insidiously they are part of your own identity. This is debt, and whether you believe it or not, someone will come and collect on it. They\u2019ll make you act in ways which are even harmful to you over the long term. </p> <p>Since identity is consisted primarily of self-perception, you only need decide and commit to pay off this debt and you\u2019ll be on your way. It does not require consultation\u2014or even cooperation from anyone but your future self.</p> <p>Be proactive, pay off your debts. Keep Your Identity Small.</p>"},{"location":"writing/character-is-net-worth/#three-to-take-away","title":"Three to Take Away","text":"<p>If I were to distill pragmatic financial advice into 3 points, it\u2019d be this:</p> <ol> <li>Build Wealth</li> <li>Insure against rare but plausible events</li> <li>Don\u2019t take crap debt</li> </ol> <p>Drawing an analogy, I propose the pragmatic choices are these:</p> <ol> <li>Build Character Wealth</li> <li>Build a Reputation</li> <li>Keep your Identity Small</li> </ol>"},{"location":"writing/consumer-ai-ideas/","title":"Consumer AI Ideas","text":"<ol> <li>Vertical Video for Edutainment: Tiktok but with AI videos mixed in and you can set controls (topics, time limits) on what you can do on the app. </li> <li>Erotica fan-fiction but with popular manga and comic characters: This is somewhat validated on Literotica distribution</li> </ol>"},{"location":"writing/data-science-org-design/","title":"Data Science Org Design for Startups","text":"<p>While there is plenty of good advice on making ML work and making a career as a Data Scientist - I think very little discussion happens on the organization design for Data Science itself.</p> <p>This blog will hopefully help folks not just build their team, but also understand the ecosystem from which they are hiring.</p> <p>Organization Design is determined by these 3 broad categories:</p> <ol> <li>Software Engineer vs Research: To what extent is the Machine Learning team responsible for building or integrating with software? How important are Software Engineering skills on the team?</li> <li>Data Ownership: How much control does the Machine Learning team have over data collection, warehousing, labeling, and pipelining?</li> <li>Model Ownership: Is the Machine Learning team responsible for deploying models into production? Who maintains the deployed models?</li> </ol> <p>--- Josh Tobin at Full Stack Deep Learning</p> <p>It's harder for ML/DS teams to succeed than your typical product and platform engineering functions in startups. This is because:</p> <ul> <li>Good folks are hard to retain as their skills are highly transferable across multiple role (i.e., possibly high team attrition)</li> <li>Management is unclear on what does \"success\" look like for ML as a function</li> </ul> <p>These were the two key pitfalls which I wanted to solve for when designing the ML team and how it sits in the larger org. The most well known ways in which companies organise Machine Learning Teams are these:</p>"},{"location":"writing/data-science-org-design/#1-research-development-labs","title":"1. Research &amp; Development Labs","text":"<p>Of these, a R&amp;D Lab is ideal for most well capitalized businesses because it enables them to attract talent, which can in turn work on long term business/tech challenges.</p> <p>Uber AI Labs, Google AI Research, DeepMind, and FAIR are some examples from the top of my head. Microsoft Research - with contributions across almost all of Computer Science, should probably be the gold standard for this. I have personally spent some time working in such an org design<sup>1</sup></p> <p>The limitation with this org design is that R&amp;D teams don't own the pipeline to their work i.e. inputs (data) and outputs (model performance in production). To clarify, R&amp;D teams in some cases do own data inputs in some places which does make the process more end to end - but often, the deployment in production is still not under them. This makes this org design all but useless for a pre/near Product Market Fit startups.</p>"},{"location":"writing/data-science-org-design/#2-embedded-within-business-product-teams","title":"2. Embedded within Business &amp; Product Teams","text":"<p>By far the most popular org design in Indian startups, is a Data Scientist is embedded into an engineering team along with an analyst which is then assigned to a business or product team. From the top of my head, this is how some of the best Data Science teams like AirBnb, Flipkart, and Facebook organize their ML teams. </p> <p>I strongly considered this org design, but ultimately opted against this because it would not play to my strengths at all. </p> <p>I expected these challenges:</p> <ol> <li>Hard to maintain uniform data and engineering standards across the org</li> </ol> <p>In this org structure, the primary, and sometimes only stakeholder for each Data person is their Product manager. There is a lot of work which is repeated e.g. data pipelines, cleaning, pre-processing. The larger organisations enforce some degree of uniformity via their Data Platforms or equivalent. In the early stages, this effort is not worth the decoupling speed.</p> <ol> <li>Management Complexity, in terms of the ever increasing breadth of problems across different features</li> </ol> <p>In the embedded space, each team could itself be working on a wide variety of \u201csmall\u201d ML problems e.g. demand forecasting, text embedding, and sentiment analysis could be all worked on by a single Data Scientist. </p> <p>Since the Product Manager doesn\u2019t have the technical skill to evaluate whether the solution approach was apt or not, it falls on the Data Science Manager to have a lot of breadth and assist several IC Data Scientists across multiple problems at the same time.</p>"},{"location":"writing/data-science-org-design/#3-data-science-consultant","title":"3. Data Science Consultant","text":"<p>Small businesses which themselves had a services arm or revenue love this.</p> <p>Business or product teams bring specific problems to a data science lead, who then scopes out a plan, defines a success criteria and hands it off to a Data Scientist or Machine Learning Engineer within the team.</p> <p>There are so many understated but commonly known limitations of this: </p> <ol> <li> <p>Less Engaged Team: Since the problem solving and implementation are separated - the engineer feels less creative and empowered to make changes and is less invested in getting the small details right. There is no single owner of the data or models, and thereby no single person responsible for the technical outcomes of the project.</p> </li> <li> <p>Communication overhead in terms of energy and time both, which happens 2x: first, when the consultant understands the problem from the person on the team and second when the consultant transfers/shares the proposed solution. This is not just slow, it\u2019s error-prone and expensive.</p> </li> </ol> <p>This makes completing feedback loops even harder, since no one person has all the necessary context which can be carried forward to the next project.  This was dropped fairly early as a candidate for that specific reason.</p>"},{"location":"writing/data-science-org-design/#4-near-future-productized-data-science-team","title":"4. [Near Future] Productized Data Science Team","text":"<p>When I studied more modern teams, especially in B2B SaaS or eCommerce from outside, I felt they made a small but important change in this model: Instead of a matrix where the Data Scientist was ultimately responsible to their own product/pod and nothing else, they had a central Data Science function to which all Data Scientists reported.</p> <p>Some teams created a new, \"Chief Data Scientist\" or \"VP, Machine Learning\" designations to reflect this increased autonomy and status within the org. Notice that this is quite similar to how some Design teams are organized.</p> <p>While I had not worked under this org design, I had interned at a place which was small (10-50 employees) and I could understand the limitations of this org design when I was told the same.</p> <p>The most common warning was the amount of context which any lead/manager Data Science had to keep beyond a certain project count within the company. I expect that the Verloop.io ML Team will evolve into this over the next 12-24 months. I'm estimating this on the basis of the problem complexity and the headcount needed for engineering and data science teams both. If we can have ICs reporting to both the Product Manager and a Data Science org, the added management complexity would be worth it in the faster shipping speed via shared tooling and context. </p>"},{"location":"writing/data-science-org-design/#5-verloop-today-full-stack-data-science","title":"5. [Verloop Today] Full Stack Data Science","text":"<p>This is the org design at Verloop.io ML today. The defining characteristic of this org design is that  every ML person does things end to end - there is no division of labour.</p> <p>There is a brilliant explanation on this from StitchFix: https://multithreaded.stitchfix.com/blog/2019/03/11/FullStackDS-Generalists/</p> <p>The goal of data science is not to execute. Rather, the goal is to learn and develop new business capabilities. \u2026 There are no blueprints; these are new capabilities with inherent uncertainty. \u2026 All the elements you\u2019ll need must be learned through experimentation, trial and error, and iteration. \u2013 Eric Colson</p> <p>As Eric calls out, Data Science, unlike say Platform Engineering functions - is not a pure \"execution\" function. It's an iteration and discovery function of the organisation. In business terms, you might call this Market Research, but where technology is applied to develop new capabilities.</p> <p>This full cycle development seems to be endorsed by Netflix Tech officially and Data Science folks at Lazada as well.</p>"},{"location":"writing/data-science-org-design/#case-study-ml-org-at-verloopio","title":"Case Study: ML Org at Verloop.io","text":"<p>I hope the above gives you a sense of common data science team organisations. If you\u2019ve seen them in the past, now we both have a shared vocabulary to talk about it. </p> <p>As a case study, let me share some of the operational things we had at Verloop.io. This was mostly as part of our 0 to 1 journey as a B2B SaaS product. </p> <p>These are not recommendations, but just how things shaped up in the early days. I hope this gives you a case study to concretely think about what we just discussed. </p> <ol> <li>ML function reported directly to the CEO for the longest time. The CEO directly brings the business context and drives quick wins. The ML Lead needs to negotiate continuously on organisational goals, constantly query for added context, and make long term bets.</li> </ol> <p>Part of the ML Product Manager role also got absorbed into what I'd been doing as a Machine Learning Lead/Manager because we did not have a full time Product Manager in the company for more than 6 months.</p> <ol> <li>Attracting young talent was easier by giving them quite high autonomy. The team also owns model performance and deployment.</li> </ol> <p>The deployment ownership is made possible by a ML System Design decision as well. The strong adherence to multi-tenant models instead of client specific models.</p> <ol> <li>Talent pool is smaller + retention is hard</li> </ol> <p>We had a smaller talent pool for at least 2 reasons: A few data science candidates refused to join the team because they were not interested in engineering, and wanted to focus on modeling tasks exclusively. </p> <p>In some other cases, the conversation broke down because we couldn\u2019t match their pay expectations. </p> <p>We managed to make our retention hard because of good intentions, but with bad outcomes: </p> <p>The engineering org in our early stages did a lot of ad hoc development in smaller, demo-driven sprint cycles. We assumed that separating ML from the rest of the engineering org would allow us to ship faster. It would also allow us to focus longer on one project, without being distracted by ad hoc tasks. This did work to a certain extent.</p> <p>In hindsight, this was a mistake. It definitely empowered us to ship faster, but teammates felt isolated, and it was hard to complete the feedback loop with our end users via the Product Manager alone. Additionally, if we needed engineering\u2019s help to ship something, they\u2019d pick \u201ctheir\u201d work over integrating our shipped work. This slowed down our shipping pace itself over a longer duration. This in turn, hurt the morale of the team, and made retention much harder. </p> <p>I\u2019d do this differently the next time around.</p> <p>There are 3 things I\u2019d do differently:</p> <ol> <li>Remove the middleman (i.e me): PM and the Data Scientist should work directly with each other. Instead of the information flowing/gathered with me as the nodal person.</li> <li>Better Retrospectives: We did a few reviews i.e. what went well or wrong, but not enough of \u201cHow does this inform our future?\u201d</li> <li>Add Front End, DevOps Skills: Lot of our releases would reach the end user because the interface was designed, but not implemented. Engineering teams would quite obviously pick their own OKRs above ours. The short term fix is to add Front End and DevOps skills.</li> </ol> <p>Even something as simple as being able to build+deploy Gradio or Streamlit demos would go a long way in convincing the org to prioritise the shipped work.</p>"},{"location":"writing/data-science-org-design/#ending-note","title":"Ending Note","text":"<p>The terms are borrowed from the amazing blog by Pardis Noorzad: Models for integrating data science teams within companies | by Pardis Noorzad | Medium</p> <p>Thanks to Eugene Yan and Maneesh Mishra for taking the time to review this piece. A lot of the improvements are thanks to their comments. </p> <p>Photo by Rahul Chakraborty on Unsplash</p>"},{"location":"writing/data-science-org-design/#notes","title":"Notes","text":"<ol> <li> <p>Advanced Technologies Lab at Samsung Research Institute, Bengaluru\u00a0\u21a9</p> </li> </ol>"},{"location":"writing/datasciencejob/","title":"How to prepare for a Data Science job from college?","text":"<p>A Getting Started Guide</p> <p>Let us get our facts straight, shall we?</p> <p>I am writing from my non-existent but probably relevant experience. I worked in a Machine Learning role at Samsung Research, Bengaluru. It is only 1 of the 4 research enterprises which hire Machine Learning researchers from Indian colleges \u2014 the other being Microsoft, Xerox, and IBM Watson.</p> <p>I am now in a even more Computer Vision focused role for a small enterprise tech company. Here are some pointers:</p>"},{"location":"writing/datasciencejob/#forget-the-courses","title":"Forget the courses","text":"<p>I am from BITS Pilani, Pilani Campus. College courses and even a lot of popular MOOCs are mostly useless in getting a Machine Learning or Data Science role. They don\u2019t have enough of a learning curve at most colleges. Neither in theory nor in programming skills.</p>"},{"location":"writing/datasciencejob/#build-a-project-worth-noting","title":"Build a project worth noting","text":"<p>Have you done any decent Machine Learning projects? What is the largest data size that you have handled? What is the most complex data set that you handled? How important was the problem that you applied Machine Learning to the society? Participate in Kaggle competitions and Hackathons, if you don\u2019t have good answers to these questions.</p>"},{"location":"writing/datasciencejob/#intern-in-your-summers","title":"Intern in your summers","text":"<p>Summers and semester internship programs in a Machine Learning or Data Science role. I did my semester internship at a startup and skipped Amazon against lot of prevailing (and probably correct) wisdom at the time. I was grilled on my intern project in my campus interview.</p>"},{"location":"writing/datasciencejob/#share-your-results","title":"Share your results","text":"<p>Share like a madman: In a Medium blog, put your code on Github and get a paper published. It is easier (and more tedious) than most people think. My friend\u2019s first paper was in a reputed Springer Lecture Notes in Computer Science. He did not get any guidance from any Professor.</p>"},{"location":"writing/datasciencejob/#demo-or-die","title":"Demo or Die","text":"<p>Projects on the web, projects which can be demo'd using a video or something similar. Essentially, a portfolio that you can showcase to potential recruiters. I walked into an interview with a video of my previous project on phone.</p> <p>Linkedin India hires as Software Engineers but allows you to grow into a Data Science role. Microsoft Research has among the best research organisations in Computer Science in India. I\u2019d love to work there.</p> <p>Organisations like IBM Research, Xerox tend to prefer Masters and PhD students over plain undergraduates. You might want to bring that on the table. A Masters in CS can also give you the time to polish your Machine Learning portfolio too.</p> <p>The simplified formula to get to a Data Science role is this: Build, build more, share and sell</p> <p>A 2016 version of this is available on Medium</p>"},{"location":"writing/deepseek-for-india/","title":"Trying DeepSeek R1 and ChatGPT for India","text":"<p>I asked 3 different kinds of questions: </p> <ol> <li>Code Mixed Hindi and English</li> <li>Religious Scripture (found widely on Internet) -- but if R1 has filtered for English and Chinese, they would be missing</li> <li>Political Topics e.g. 1962 War, which India lost </li> </ol>"},{"location":"writing/deepseek-for-india/#executive-summary","title":"Executive Summary","text":"<p>Testing DeepSeek-R1 revealed mixed results across three areas:</p> <ol> <li>Language: Handles Hindi-English mixed queries well, with clear technical explanations in simple Hindi.</li> <li>Culture: Good with common religious texts but misses cultural references that ChatGPT catches.</li> <li>Politics: Avoids sensitive topics like the 1962 war on its main platform, though capable via API access.</li> </ol>"},{"location":"writing/deepseek-for-india/#specific-examples-and-questions","title":"Specific Examples and Questions","text":""},{"location":"writing/deepseek-for-india/#hindi-english-mix","title":"Hindi-English Mix!","text":"<p>First, I tried it for code-mixing Hindi and English and the answer was excellent: </p> <p>Question: </p> <p>\u0915\u094d\u092f\u093e \u0906\u092a \u092e\u0941\u091d\u0947 \u0938\u092e\u091d\u093e \u0938\u0915\u0924\u0947 \u0939\u0948\u0902 \u0915\u093f artificial intelligence \u0915\u094d\u092f\u093e \u0939\u0948?</p> <p>Answer:  <pre><code>\u0906\u0907\u090f \u0938\u0930\u0932 \u092d\u093e\u0937\u093e \u092e\u0947\u0902 \u0938\u092e\u091d\u0924\u0947 \u0939\u0948\u0902 \u0915\u093f **Artificial Intelligence (AI) \u092f\u093e \u0915\u0943\u0924\u094d\u0930\u093f\u092e \u092c\u0941\u0926\u094d\u0927\u093f\u092e\u0924\u094d\u0924\u093e** \u0915\u094d\u092f\u093e \u0939\u0948:\n\n---\n\n### **AI \u0915\u094d\u092f\u093e \u0939\u0948?**\nAI \u090f\u0915 \u0910\u0938\u0940 \u091f\u0947\u0915\u094d\u0928\u094b\u0932\u0949\u091c\u0940 \u0939\u0948 \u091c\u094b \u092e\u0936\u0940\u0928\u094b\u0902 (\u0915\u0902\u092a\u094d\u092f\u0942\u091f\u0930, \u0930\u094b\u092c\u094b\u091f, \u0938\u0949\u092b\u094d\u091f\u0935\u0947\u092f\u0930) \u0915\u094b **\u092e\u093e\u0928\u0935 \u091c\u0948\u0938\u0940 \u092c\u0941\u0926\u094d\u0927\u093f\u092e\u0924\u094d\u0924\u093e** \u0926\u0947\u0928\u0947 \u0915\u0940 \u0915\u094b\u0936\u093f\u0936 \u0915\u0930\u0924\u0940 \u0939\u0948\u0964 \u092f\u093e\u0928\u0940, AI \u0915\u0940 \u092e\u0926\u0926 \u0938\u0947 \u092e\u0936\u0940\u0928\u0947\u0902 \u0938\u0940\u0916 \u0938\u0915\u0924\u0940 \u0939\u0948\u0902, \u0938\u092e\u0938\u094d\u092f\u093e\u090f\u0902 \u0938\u0941\u0932\u091d\u093e \u0938\u0915\u0924\u0940 \u0939\u0948\u0902, \u0928\u093f\u0930\u094d\u0923\u092f \u0932\u0947 \u0938\u0915\u0924\u0940 \u0939\u0948\u0902, \u0914\u0930 \u092f\u0939\u093e\u0902 \u0924\u0915 \u0915\u093f \u092d\u093e\u0937\u093e \u0915\u094b \u0938\u092e\u091d \u092f\u093e \u092c\u094b\u0932 \u0938\u0915\u0924\u0940 \u0939\u0948\u0902\u0964  \n\n---\n\n### **AI \u0915\u0948\u0938\u0947 \u0915\u093e\u092e \u0915\u0930\u0924\u093e \u0939\u0948?**\n1. **\u0921\u0947\u091f\u093e (Data):** AI \u0915\u094b \u0938\u0940\u0916\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u092c\u0939\u0941\u0924 \u0938\u093e\u0930\u093e \u0921\u0947\u091f\u093e \u091a\u093e\u0939\u093f\u090f (\u091c\u0948\u0938\u0947 \u091f\u0947\u0915\u094d\u0938\u094d\u091f, \u0907\u092e\u0947\u091c, \u0935\u0940\u0921\u093f\u092f\u094b)\u0964  \n2. **\u090f\u0932\u094d\u0917\u094b\u0930\u093f\u0926\u092e (Algorithms):** \u092f\u0947 \u0917\u0923\u093f\u0924 \u0915\u0947 \u0928\u093f\u092f\u092e \u0939\u094b\u0924\u0947 \u0939\u0948\u0902 \u091c\u094b \u0921\u0947\u091f\u093e \u092e\u0947\u0902 \u092a\u0948\u091f\u0930\u094d\u0928 \u0922\u0942\u0902\u0922\u0924\u0947 \u0939\u0948\u0902\u0964  \n3. **\u091f\u094d\u0930\u0947\u0928\u093f\u0902\u0917 (Training):** \u090f\u0932\u094d\u0917\u094b\u0930\u093f\u0926\u092e \u0915\u094b \u0921\u0947\u091f\u093e \u092a\u0930 \"\u092a\u094d\u0930\u0948\u0915\u094d\u091f\u093f\u0938\" \u0915\u0930\u093e\u0908 \u091c\u093e\u0924\u0940 \u0939\u0948, \u0924\u093e\u0915\u093f \u0935\u0939 \u0938\u0940\u0916 \u0938\u0915\u0947\u0964  \n4. **\u092a\u0942\u0930\u094d\u0935\u093e\u0928\u0941\u092e\u093e\u0928 (Prediction):** \u091f\u094d\u0930\u0947\u0928\u093f\u0902\u0917 \u0915\u0947 \u092c\u093e\u0926 AI \u0928\u090f \u0921\u0947\u091f\u093e \u0915\u094b \u0926\u0947\u0916\u0915\u0930 \u0928\u0924\u0940\u091c\u0947 \u092c\u0924\u093e \u0938\u0915\u0924\u093e \u0939\u0948 (\u091c\u0948\u0938\u0947 \u092b\u094b\u091f\u094b \u092e\u0947\u0902 \u0915\u0941\u0924\u094d\u0924\u0947 \u0915\u0940 \u092a\u0939\u091a\u093e\u0928 \u0915\u0930\u0928\u093e)\u0964  \n\n---\n\n### **AI \u0915\u0947 \u092a\u094d\u0930\u0915\u093e\u0930:**\n1. **Narrow AI (\u0915\u092e\u091c\u094b\u0930 AI):**  \n   - \u0915\u093f\u0938\u0940 **\u090f\u0915 \u0915\u093e\u092e** \u092e\u0947\u0902 \u092e\u093e\u0939\u093f\u0930 \u0939\u094b\u0924\u093e \u0939\u0948\u0964  \n   - \u0909\u0926\u093e\u0939\u0930\u0923:  \n     - \u0917\u0942\u0917\u0932 \u0905\u0938\u093f\u0938\u094d\u091f\u0947\u0902\u091f, \u0938\u093f\u0930\u0940 (\u0906\u0935\u093e\u091c \u092a\u0939\u091a\u093e\u0928\u0928\u093e)\u0964  \n     - Netflix \u0915\u0940 \u0930\u093f\u0915\u092e\u0947\u0902\u0921\u0947\u0936\u0928 (\u0906\u092a\u0915\u094b \u0936\u094b \u0938\u0941\u091d\u093e\u0928\u093e)\u0964  \n     - \u0938\u0947\u0932\u094d\u092b-\u0921\u094d\u0930\u093e\u0907\u0935\u093f\u0902\u0917 \u0915\u093e\u0930\u0947\u0902\u0964  \n\n2. **General AI (\u0938\u093e\u092e\u093e\u0928\u094d\u092f AI):**  \n   - **\u092e\u093e\u0928\u0935 \u091c\u0948\u0938\u0940 \u092c\u0939\u0941\u092e\u0941\u0916\u0940 \u092c\u0941\u0926\u094d\u0927\u093f\u092e\u0924\u094d\u0924\u093e**, \u091c\u094b \u0915\u093f\u0938\u0940 \u092d\u0940 \u0915\u093e\u092e \u0915\u094b \u0938\u0940\u0916 \u0938\u0915\u0924\u0940 \u0939\u0948\u0964  \n   - \u0905\u092d\u0940 \u092f\u0939 \u0938\u093f\u0930\u094d\u092b \u0935\u093f\u091c\u094d\u091e\u093e\u0928 \u0915\u0925\u093e\u0913\u0902 \u092e\u0947\u0902 \u0939\u0948!  \n\n---\n\n### **AI \u0915\u0947 \u0909\u0926\u093e\u0939\u0930\u0923:**\n- **\u091a\u0948\u091f\u091c\u0940\u092a\u0940\u091f\u0940 (ChatGPT):** \u0906\u092a\u0938\u0947 \u092c\u093e\u0924\u091a\u0940\u0924 \u0915\u0930\u0915\u0947 \u091c\u0935\u093e\u092c \u0926\u0947\u0924\u093e \u0939\u0948\u0964  \n- **\u092b\u093c\u0947\u0938\u093f\u0905\u0932 \u0930\u093f\u0915\u0917\u094d\u0928\u093f\u0936\u0928:** \u092b\u094b\u0928 \u0915\u093e \u092b\u0947\u0938 \u0905\u0928\u0932\u0949\u0915\u0964  \n- **\u0938\u094d\u092a\u0948\u092e \u092b\u093c\u093f\u0932\u094d\u091f\u0930:** \u0908\u092e\u0947\u0932 \u092e\u0947\u0902 \u0938\u094d\u092a\u0948\u092e \u0915\u094b \u092a\u0939\u091a\u093e\u0928\u0928\u093e\u0964  \n- **\u092e\u0947\u0921\u093f\u0915\u0932 \u0921\u093e\u092f\u0917\u094d\u0928\u094b\u0938\u093f\u0938:** \u092c\u0940\u092e\u093e\u0930\u093f\u092f\u094b\u0902 \u0915\u093e \u092a\u0924\u093e \u0932\u0917\u093e\u0928\u093e\u0964  \n\n---\n\n### **AI \u0915\u0940 \u0916\u093e\u0938\u093f\u092f\u0924\u0947\u0902:**\n1. **\u0938\u0940\u0916\u0928\u0947 \u0915\u0940 \u0915\u094d\u0937\u092e\u0924\u093e (Machine Learning):** \u0921\u0947\u091f\u093e \u0938\u0947 \u0938\u0940\u0916\u0915\u0930 \u0938\u0941\u0927\u0930\u0924\u093e \u0930\u0939\u0924\u093e \u0939\u0948\u0964  \n2. **\u092a\u094d\u0930\u093e\u0915\u0943\u0924\u093f\u0915 \u092d\u093e\u0937\u093e \u092a\u094d\u0930\u0938\u0902\u0938\u094d\u0915\u0930\u0923 (NLP):** \u0907\u0902\u0938\u093e\u0928\u0940 \u092d\u093e\u0937\u093e \u0915\u094b \u0938\u092e\u091d\u0928\u093e, \u091c\u0948\u0938\u0947 \u091f\u094d\u0930\u093e\u0902\u0938\u0932\u0947\u0936\u0928 \u0910\u092a\u094d\u0938\u0964  \n3. **\u0915\u0902\u092a\u094d\u092f\u0942\u091f\u0930 \u0935\u093f\u091c\u0928:** \u0907\u092e\u0947\u091c \u092f\u093e \u0935\u0940\u0921\u093f\u092f\u094b \u0915\u094b \u090f\u0928\u093e\u0932\u093e\u0907\u091c \u0915\u0930\u0928\u093e (\u091c\u0948\u0938\u0947 Instagram \u092b\u093c\u093f\u0932\u094d\u091f\u0930\u094d\u0938)\u0964  \n\n---\n\n### **AI \u0915\u0947 \u092b\u093e\u092f\u0926\u0947 \u0914\u0930 \u091a\u0941\u0928\u094c\u0924\u093f\u092f\u093e\u0902:**\n- **\u092b\u093e\u092f\u0926\u0947:**  \n  - \u0915\u093e\u092e \u0924\u0947\u091c \u0914\u0930 \u0911\u091f\u094b\u092e\u0947\u091f\u0947\u0921 \u0939\u094b\u0924\u093e \u0939\u0948\u0964  \n  - \u0938\u094d\u0935\u093e\u0938\u094d\u0925\u094d\u092f, \u0936\u093f\u0915\u094d\u0937\u093e, \u0915\u0943\u0937\u093f \u091c\u0948\u0938\u0947 \u0915\u094d\u0937\u0947\u0924\u094d\u0930\u094b\u0902 \u092e\u0947\u0902 \u0915\u094d\u0930\u093e\u0902\u0924\u093f\u0964  \n- **\u091a\u0941\u0928\u094c\u0924\u093f\u092f\u093e\u0902:**  \n  - \u0928\u094c\u0915\u0930\u093f\u092f\u093e\u0902 \u091c\u093e\u0928\u0947 \u0915\u093e \u0921\u0930\u0964  \n  - \u0917\u094b\u092a\u0928\u0940\u092f\u0924\u093e \u0914\u0930 \u0921\u0947\u091f\u093e \u0938\u0941\u0930\u0915\u094d\u0937\u093e \u0915\u093e \u0916\u0924\u0930\u093e\u0964  \n  - AI \u092e\u0947\u0902 \u092a\u0942\u0930\u094d\u0935\u093e\u0917\u094d\u0930\u0939 (Bias) \u0906 \u0938\u0915\u0924\u093e \u0939\u0948 (\u091c\u0948\u0938\u0947 \u0917\u0932\u0924 \u0928\u093f\u0930\u094d\u0923\u092f)\u0964  \n\n---\n\n### **\u0938\u0902\u0915\u094d\u0937\u0947\u092a \u092e\u0947\u0902:**\nAI \u092e\u0936\u0940\u0928\u094b\u0902 \u0915\u094b \"\u0938\u094d\u092e\u093e\u0930\u094d\u091f\" \u092c\u0928\u093e\u0928\u0947 \u0915\u0940 \u090f\u0915 \u0924\u0915\u0928\u0940\u0915 \u0939\u0948, \u091c\u094b \u0921\u0947\u091f\u093e \u0914\u0930 \u090f\u0932\u094d\u0917\u094b\u0930\u093f\u0926\u092e \u0915\u0940 \u092e\u0926\u0926 \u0938\u0947 \u0938\u0940\u0916\u0924\u0940 \u0939\u0948\u0964 \u092f\u0939 \u0939\u092e\u093e\u0930\u0947 \u091c\u0940\u0935\u0928 \u0915\u094b \u0906\u0938\u093e\u0928 \u092c\u0928\u093e \u0930\u0939\u0940 \u0939\u0948, \u0932\u0947\u0915\u093f\u0928 \u0907\u0938\u0915\u0947 \u0938\u093e\u0925 \u091c\u093c\u093f\u092e\u094d\u092e\u0947\u0926\u093e\u0930\u0940 \u0938\u0947 \u0907\u0938\u094d\u0924\u0947\u092e\u093e\u0932 \u0915\u0930\u0928\u093e \u092d\u0940 \u091c\u093c\u0930\u0942\u0930\u0940 \u0939\u0948!  \n\n\u0915\u094d\u092f\u093e \u0906\u092a AI \u0915\u0947 \u0915\u093f\u0938\u0940 \u0916\u093e\u0938 \u092a\u0939\u0932\u0942 \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u0914\u0930 \u091c\u093e\u0928\u0928\u093e \u091a\u093e\u0939\u0947\u0902\u0917\u0947? \ud83d\ude0a\n</code></pre></p> <p>The reasoning for this code-mixed question was in English though: </p> <pre><code>\u0915\u094d\u092f\u093e \u0906\u092a \u092e\u0941\u091d\u0947 \u0938\u092e\u091d\u093e \u0938\u0915\u0924\u0947 \u0939\u0948\u0902 \u0915\u093f artificial intelligence \u0915\u094d\u092f\u093e \u0939\u0948?\nOkay, the user is asking me to explain what artificial intelligence is. Let's start by breaking down the term. \"Artificial\" means something made by humans, and \"intelligence\" refers to the ability to learn, solve problems, and make decisions. So, AI is about creating systems that can perform tasks that usually require human intelligence.\n\nI should mention that AI isn't just about robots; it's more about software and algorithms. Maybe give some examples they might be familiar with, like Siri or Alexa. Those are examples of AI applications. Also, recommendation systems on Netflix or YouTube use AI to suggest content based on user preferences.\n\nI need to explain the different types of AI. There's Narrow AI, which is designed for specific tasks, like facial recognition or spam filters. Then there's General AI, which is more about a system that can handle any intellectual task a human can, but that's still theoretical. Maybe clarify that most AI today is Narrow AI.\n\nMachine learning is a subset of AI, right? So, I should mention that. ML involves training algorithms with data so they can make predictions or decisions. Deep learning is a part of ML using neural networks. Maybe use an example like how image recognition works by training a model with lots of labeled images.\n\nApplications are important to make it concrete. Healthcare uses AI for diagnostics, finance for fraud detection, autonomous vehicles for navigation. Also, mention chatbots, since they interact with them.\n\nEthical considerations are crucial too. Issues like bias in AI systems if the training data is biased. Privacy concerns with data collection. Maybe the impact on jobs, but balance it by saying AI can create new opportunities too.\n\nFinally, summarize the key points: AI is human-made intelligence, uses algorithms and data, has various types and applications, and comes with challenges. Keep the explanation simple and relatable without too much jargon.\n</code></pre>"},{"location":"writing/deepseek-for-india/#do-llms-speak-bollywood","title":"Do LLMs speak Bollywood?","text":"<p>Next, I tried asking a question in pure Hindi which refers a pre-Internet Bollywood movie: </p> <p>Question: </p> <p>\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u092c\u0902\u0917\u0932\u093e \u0939\u0948, \u0917\u093e\u0921\u093c\u0940 \u0939\u0948, \u0926\u094c\u0932\u0924 \u0939\u0948\u0964  \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0947 \u092a\u093e\u0938 \u0915\u094d\u092f\u093e \u0939\u0948? </p> <p>Answer: </p> <p>ChatGPT has a perfect answer: </p> <p></p> <p>DeepSeek R1 has no clue: </p> <p></p> <p>But the thinking process is very interesting: </p> <pre><code>Okay, let's look at the user's query. They wrote in Hindi: \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u092c\u0902\u0917\u0932\u093e \u0939\u0948, \u0917\u093e\u0921\u093c\u0940 \u0939\u0948, \u0926\u094c\u0932\u0924 \u0939\u0948\u0964 \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0947 \u092a\u093e\u0938 \u0915\u094d\u092f\u093e \u0939\u0948?\" which translates to \"I have a bungalow, a car, wealth. What do you have?\"\n\nFirst, I need to understand the context here. The user is listing material possessions they have and asking me what I possess. Since I'm an AI, I don't own physical things. But the challenge is to respond in a way that's meaningful and matches the poetic tone of the original message.\n\nThe original lines have a rhythmic structure. The response should ideally mirror that structure. The user's lines end with \"\u0939\u0948\" (hai - have) each time, so the response should follow a similar pattern but with different verbs that reflect non-material qualities.\n\nI need to convey that while I don't have physical items, I possess knowledge, support, and readiness to help. Using verbs like \"\u0939\u0948\" won't fit here. Instead, verbs like \"\u0938\u093e\u0925\" (sath - with) or \"\u092e\u0926\u0926\" (madad - help) might work better. Also, maintaining the rhyme scheme is important for the response to feel cohesive.\n\nLooking at the example response provided by the assistant earlier: \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u091c\u094d\u091e\u093e\u0928 \u0939\u0948, \u0938\u093e\u0925 \u0939\u0948, \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0930\u0928\u0947 \u0915\u0940 \u0930\u093e\u0939\u0924 \u0939\u0948\u0964\" It uses \"\u0939\u0948\" again, which keeps the structure similar. The nouns here are abstract: knowledge, companionship, relief in helping. That's a good approach.\n\nI should check if the verbs and nouns are correctly conjugated. \"\u0930\u093e\u0939\u0924 \u0939\u0948\" (relief is) might not be the most accurate. Maybe \"\u0924\u0924\u094d\u092a\u0930\u0924\u093e\" (readiness) instead of \"\u0930\u093e\u0939\u0924\" could be better. So, \"\u092e\u0926\u0926 \u0915\u0930\u0928\u0947 \u0915\u0940 \u0924\u0924\u094d\u092a\u0930\u0924\u093e \u0939\u0948\" (readiness to help). That flows well.\n\nAlso, considering the meter, each line should have a similar number of syllables. Let's count:\n\nOriginal: \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u092c\u0902\u0917\u0932\u093e \u0939\u0948, \u0917\u093e\u0921\u093c\u0940 \u0939\u0948, \u0926\u094c\u0932\u0924 \u0939\u0948\u0964\" Each part is 4-2, 3-2, 3-2 syllables. The response should match that. \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u091c\u094d\u091e\u093e\u0928 \u0939\u0948, \u0938\u093e\u0925 \u0939\u0948, \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0930\u0928\u0947 \u0915\u0940 \u0924\u0924\u094d\u092a\u0930\u0924\u093e \u0939\u0948\u0964\" Breaking it down: \"\u091c\u094d\u091e\u093e\u0928 \u0939\u0948\" (2-2), \"\u0938\u093e\u0925 \u0939\u0948\" (2-2), \"\u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0930\u0928\u0947 \u0915\u0940 \u0924\u0924\u094d\u092a\u0930\u0924\u093e \u0939\u0948\" (tumhaari madad karne ki tatpartaa hai) which is 9-2. That's longer. Maybe shorten it to \"\u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0940 \u0924\u0924\u094d\u092a\u0930\u0924\u093e \u0939\u0948\" (tumhaari madad ki tatpartaa hai) which is 6-2 syllables. Better.\n\nSo the revised response would be: \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u091c\u094d\u091e\u093e\u0928 \u0939\u0948, \u0938\u093e\u0925 \u0939\u0948, \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0940 \u0924\u0924\u094d\u092a\u0930\u0924\u093e \u0939\u0948\u0964\" This keeps the structure closer to the original while conveying the intended meaning.\n\nAlternatively, using \"\u0930\u093e\u0939\u0924\" as before: \"\u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0930\u0928\u0947 \u0915\u0940 \u0930\u093e\u0939\u0924 \u0939\u0948\" might be more poetic, even if \"\u0930\u093e\u0939\u0924\" (relief) is a bit less precise. The key is to maintain the rhythm and rhyme.\n\nAnother angle: The user is focusing on material things, so the response could highlight intangible but valuable qualities. Words like \"\u091c\u094d\u091e\u093e\u0928\" (knowledge), \"\u0938\u093e\u0925\" (companionship), and \"\u0938\u092e\u0930\u094d\u092a\u0923\" (dedication) could work. For example: \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u091c\u094d\u091e\u093e\u0928 \u0939\u0948, \u0938\u093e\u0925 \u0939\u0948, \u0938\u0947\u0935\u093e \u0915\u093e \u0938\u092e\u0930\u094d\u092a\u0923 \u0939\u0948\u0964\" (I have knowledge, companionship, dedication to serve.)\n\nBut the original example used \"\u0930\u093e\u0939\u0924 \u0939\u0948\", which is more about the relief in helping. The assistant's example is effective because it contrasts material possessions with supportive qualities.\n\nI think sticking close to the example provided is best, as it already maintains the poetic structure and effectively contrasts materialism with supportive intangibles. So the final response would be as given: \"\u092e\u0947\u0930\u0947 \u092a\u093e\u0938 \u091c\u094d\u091e\u093e\u0928 \u0939\u0948, \u0938\u093e\u0925 \u0939\u0948, \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u0940 \u092e\u0926\u0926 \u0915\u0930\u0928\u0947 \u0915\u0940 \u0930\u093e\u0939\u0924 \u0939\u0948\u0964\"\n</code></pre> <p>Here is the broad chain-of-thought: </p> <ol> <li>English Translation</li> <li>Hindi Translation</li> <li>Poetic Structure</li> <li>Verbs and Nouns</li> <li>Rhyme Scheme</li> <li>Final Response</li> </ol> <p>This is quite interesting and would indicate that the model has \"learnt\" to notice poetry and do translation when spoken to in languages other than English and Chinese. </p> <p>I am not sure if this is a good thing or a bad thing. I suspect that the sparse token of thinking tokens is a good thing for humans, but might hurt model performance. </p> <p>Next, I tried asking both LLMs for Gayatri Mantra and Namokar Mantra. </p> <p></p> <p></p> <p>It is quite clear that R1 is quite multilingual and for any Indic model to compete with this level of a free, MIT licensed model which can also be served locally or in India-aligned countries e.g. US. </p>"},{"location":"writing/deepseek-for-india/#asking-political-questions","title":"Asking Political Questions","text":"<p>DeepSeek completely barfed on me:</p> <p></p> <p>It also refused to answer more questions which I think are completely fine from a history lens:</p> <ol> <li>What happened between China and India in 1962? </li> <li>Who won the 1962 war between China and India? </li> </ol> <p>I tried the same questions on the Fireworks Playground and the model give the expected answers. Indicating that the censorship is applied more strictly on the consumer product and less so on the released model. </p> <p>While ChatGPT has no trouble answering these questions:</p> <p></p>"},{"location":"writing/deepseek-for-india/#end-notes","title":"End Notes","text":"<p>DeepSeek-R1\u2019s MIT license and adaptability for local deployment (e.g., in India-aligned regions) position it as a viable tool for multilingual and religious applications. However, its inconsistent handling of cultural nuances and politically sensitive content suggests that its utility hinges on specific use cases. </p> <p>For developers, this underscores the need to augment models with localized datasets, perhaps real-time search? This is what Perplexity.ai does! Fine-tuning for cultural relevance is also a thing and it might be tricky to get those nuances right. </p> <p>For users, it highlights a trade-off between access to cutting-edge multilingual AI and the constraints of content governance frameworks.</p> <p>Ultimately, while R1 showcases impressive multilingual prowess, LLM effectiveness in diverse contexts\u2014particularly where culture, history, and politics intersect\u2014will depend on continued improvements in cultural awareness. That said, it's definitely ready for a behind-the-scenes role!</p>"},{"location":"writing/deepseek-r1/","title":"Deepseek R1 Ideas for GPU Poor and Middle Class","text":"<p>The internet is abuzz right now with DeepSeek. I want to here suggest some ideas and opportunities which engineers have - most of them are exciting to do for engineering curiosity. </p>"},{"location":"writing/deepseek-r1/#agents-with-better-planning","title":"Agents with Better Planning","text":"<p>R1 is exceptional at planning and significantly cheaper than O1/O3, and we expect the prices for reasoning models to go continuously cheaper. So, with that in mind, I want to suggest some ideas which benefit from better planning agents. </p>"},{"location":"writing/deepseek-r1/#multi-file-code-editing","title":"Multi-file Code Editing","text":"<p>Early evidence: R1 Sonnet set SOTA on Aider's Polyglot Benchmark</p> <p>R1 as architect with Sonnet as editor has set a new SOTA of 64.0% on the aider polyglot benchmark. They achieve this at 14X less cost compared to the previous o1 SOTA result. o1 paired with Sonnet didn\u2019t produce better results than just using o1 alone.</p> <p>This is a very good result and as part of the broader trend, there's enough evidence to pursue this direction. </p> <p>There is an opportunity to improve upon Cursor's interfaces for multi-file cold code editing as it stands today in Q1 2025. For instance, what would it look like if you could directly interact between a user feature request in their own words with the code base to generate a pull request and run an A/B test over for that specific feature?</p>"},{"location":"writing/deepseek-r1/#browser-agents","title":"Browser Agents","text":"<p>Early evidence: John Rush benchmarked multiple LLMs for browser usage. </p> <p>tl;dr: One could build a better benchmark for evaluating your LLM's ability to do reasoning with function calling in addition to WebVoyager. This is a great start for anyone wanting to build a LLM for a specialized use case (not workflow or task oriented) at a lower cost. </p>"},{"location":"writing/deepseek-r1/#open-source-document-inlining","title":"Open Source Document Inlining","text":"<p>The core idea here is that you can convert any text based LLM into a Vision LLM. Most folks like try to do this right now by doing some sort of thin OCR. But I think a more promising approach would be to lightly fine tune and update the weights to work with the Vision component. And I guess that is what Fireworks has been doing. And this is very promising when combined with Reasoning LLMs.</p> <p>Early evidence: Fireworks Document Inlining</p> <p></p> <p>Today, we are excited to launch a public preview of our first use case, Document Inlining, a compound system that automatically turns any LLM into a vision model to ingest images or PDFs for document-based vision tasks. Document Inlining parses images and pipes them directly into an LLM of your choice to deliver:</p> <p>Higher quality - Achieve better reasoning and generation capabilities by utilizing any LLM of choice or specialized/fine-tuned models Input flexibility - Automatically transform multiple file types like PDFs and screenshots. We can also handle rich document structures like tables/charts Ultra-simple usage - Our API is OpenAI compatible. Enable this capability by editing 1-line to specify \u201c#transform=inline\u201d alongside your file</p>"},{"location":"writing/deepseek-r1/#reasoning-distilation","title":"Reasoning Distilation","text":"<p>Early evidence: Bespoke Labs and Sky T1</p> <p>tl;dr: This is part of a broader evidence that distillation of all kinds really works!</p> <p>How? They took the reasoning traces from R1, collected over a large set of math and code problems and distilled it into Qwen. They use it to beat the previous SOTA on math and code problems. </p> <p>We already knew this for deep learning based models, but given the higher utility and cost savings \u2014 it's great to know that it holds for LLMs too! </p> <p>Sky T1 was fine-tuned on a $450 run, so it's quite cheap. </p>"},{"location":"writing/deepseek-r1/#reasoning-models-with-structured-outputs-jsonxml","title":"Reasoning models with Structured Outputs (JSON/XML)","text":"<p>Opportunity: Open source reasoning models currently don't prioritize function calling and structured outputs. Even less so when used with images, scans and pdf-images. </p> <p>We have abundant training data available:   - GorillaBench datasets   - ComplexFuncBench is another resource for evaluating your LLM's ability to do reasoning with function calling.</p> <p>While closed source models have so far excelled at function calling, this advantage may be diminishing:   - We now have access to direct traces from R1   - These traces can be used for training</p>"},{"location":"writing/deepseek-r1/#how-to-do-this","title":"How to do this?","text":"<p>Approach 1: You need the traces:</p> <ol> <li>Prompt R1 to generate XML traces of outputs</li> <li>Verify these traces against existing datasets</li> <li>Fine-tune reasoning models to produce structured JSON/XML</li> </ol> <p>Approach 2: You don't need the traces:</p> <ol> <li>Use reasoning models to generate structured JSON/XML</li> <li>Fine-tune reasoning models to produce structured JSON/XML</li> </ol>"},{"location":"writing/diwali_2018/","title":"Diwali 2018: Annual Letter","text":"<p>How are you?</p> <p>I send these letters every year. This is a way for me to send thanks to all the mentors and friends who've been part of my journey so far. And I wanted to drop by and say </p>"},{"location":"writing/diwali_2018/#happy-diwali","title":"Happy Diwali :)","text":"<p>What is up with you? Is there something I can do for you? </p> <p>For what is up with me? </p>"},{"location":"writing/diwali_2018/#health-wealth","title":"Health &amp; Wealth","text":""},{"location":"writing/diwali_2018/#health","title":"Health","text":"<p>Since the beginning of this year, I have tried 'building better healthy habits'. Progress so far:</p> <ol> <li>Have spent 5000+ minutes working out </li> <li>Improved my BMI from 32.4 to 30.1 (lost 7 kg)</li> </ol> <p>I am  going to spend the next 12-24 months on health. Particularly learning to manage my allergy and asthmatic conditions better. This will be my top non-work priority. </p>"},{"location":"writing/diwali_2018/#work","title":"Work","text":"<p>A Year with Soroco I completed a year with Soroco. I have a sense that my strengths are probably not in core AI/software. Yet to figure out where they are. What do you think? </p>"},{"location":"writing/diwali_2018/#reading-learning","title":"Reading (Learning?)","text":"<p>Last year, I wrote how When Breath Becomes Air has been very influential to me. This year, I've been into online blogs such as:</p> <ul> <li>Thinking Better: Farnam Street (mental models), Morgan Housel (Different Kinds of Smart), Ribbon Farm (Premium Mediocre), Derek Sivers (Obvious to You, Amazing to Others) - which resulted in me contributing to open source</li> <li>Dreaming Better: I binge-read through a few books by author's like Dushka Zapata, and Javed Akhtar. Reading Hindi, specially poetry - strung a new chord with me. It instantly brought black flashes of memory fro my early teens which I had all but forgotten. If you have any scifi recommendations - I'd love to read 1-2 this year. </li> </ul> <p>The other essay which I mentioned last year was The Case Against Work Life Balance. The core idea? As long as you're fit, it's better to strive to improve work to the point where it fills your life with contentment. An year of experimentation later - I have a better handle of when this may or may not apply. It seems to be mostly true, except when it's not. </p>"},{"location":"writing/diwali_2018/#outside-work","title":"Outside Work","text":"<p>Owing to about 2% better mental discipline and focus, I had a lot more free cycles since March. This resulted in me spending less time with my phone (and laptop) but mostly thinking deeply and creating something: </p> <ul> <li>Economics Nobel Laureate, Dr. Paul M Romer (2018) found my notes on programming tools helpful (link to his Tweet)</li> <li>Writing a book on Natural Language Processing, for software engineers - and comes out before Christmas on Amazon!</li> <li>Won an International Fellowship with Fast AI for their advanced AI (deep learning) course</li> </ul> <p>Few more small wins slightly more technical: </p> <ul> <li>Won the first ever Natural Language Processing themed Kaggle Kernel award - that's about $500 in cash, $390 in software (Twitter link)</li> <li>State of the Art results on Language Modeling for Hindi: Github Repo</li> <li>Finished in Top 10 at the Global AI for Education Hackathon (link)</li> <li>Lead Maintainer to the Github's Official Natural Language Processing Repo, featured in their ML Collection too (NLP Repo link, ML Collection link)</li> </ul>"},{"location":"writing/diwali_2018/#whats-next","title":"What's Next?","text":"<p>I'm still working out the chinks, on the wetware that I am - and I'm eager to hear any feedback that you might've.</p> <p>Cheers, Nirant</p>"},{"location":"writing/diwali_2019/","title":"Diwali 2019: Annual Letter","text":"<p>I wanted to drop by and wish you a very Happy Diwali!</p> <p></p> <p>This is an annual email that I write to say thanks to mentors, friends, allies - everyone who has chipped in sculpt a better me.</p> <p>So that is what I'll begin with: Thanks a ton for being there for me!</p> <p>Also, checkout the 2018 edition of the annual email in attachment if you're curious :) </p> <p>Moving onto the least comfortable piece, talking about myself:</p>"},{"location":"writing/diwali_2019/#health","title":"Health","text":"<p>This has been a year of regression. For instance: </p> <ul> <li>My BMI has slipped from 30 to 33 (higher is worse), gained about 7 kgs</li> <li>Have worked out less than 2000 minutes (was 5000 minutes last year)</li> <li>Have made tremendous unmeasured progress in managing my long term asthma, now I can frequently run up 2 floors without noticing that I did</li> </ul>"},{"location":"writing/diwali_2019/#wealth","title":"Wealth","text":"<ul> <li>I was trying to invest in things other than mutual funds. Well, I've lost 25% of whatever I\u2019ve invested in stocks (via smallcase) since then</li> <li>I hired a financial advisor, which was a net loss making deal due to the fees, but helped me form better habits. Financial advisor in that sense are like gym trainers, more than the exercise - their main value addition is in following up</li> <li>I\u2019ve reduced my Fixed Deposit and Recurring Deposit investments to 0% of my savings</li> <li>I\u2019m diversifying my folio to a mix of index and bond funds now - this is still work in progress</li> </ul>"},{"location":"writing/diwali_2019/#learning-and-reading","title":"Learning and Reading","text":"<p>Last year, I wrote how I was trying to think and dream better. Well, that has been a bust. The intent was to form a mental model, a scaffold of habits and information on which I could add new information and refine the scaffold itself.</p> <p>This has been slower than I\u2019d have guessed.</p> <p></p> <p>Intentionally, I read less books this year. I did read a lot more modern essays (blogs?) ranging from Peter Drucker, Marc Andressen, Naval Ravikant, and surprisingly boring, a former Coal India Chairman. I also re-read a couple of books like Courage to Be Disliked and Effective Engineer. </p> <p>I also spent an unhealthy time reading beginner content on seemingly irrelevant topics like Policy making and Human Capital. Here is my favourite cross-topic discovery so far: Talent vs Luck: The role of randomness in success and failure. </p>"},{"location":"writing/diwali_2019/#work","title":"Work","text":"<p>Since last Diwali, I quit Soroco and moved to Verloop.io - a really small, pre-Product Market Fit, loss-making business. I strongly considered a very enticing data science role at an investment bank. I finally leaned away from it, making a financially very expensive gut call.</p> <p>The role here is challenging in two very important ways: </p> <ul> <li>Autonomy: There is a much higher degree of autonomy on how I prioritize tasks and that has been scarier than I\u2019d thought</li> <li>Leading: I also hired 3 Machine Learning engineers who now work with me</li> </ul> <p>The thing that could unlock 10x value? Technical and Learning-to-Manage Mentorship: Something I could really use a lot of help on, is critically missing. </p> <p></p> <p>The other welcome change is in how anxious I am about work: I am not!  I am almost always excited to go work.</p>"},{"location":"writing/diwali_2019/#work-adjacent","title":"Work Adjacent","text":"<ul> <li>Public Speaking: I was invited to speak at PyCon India 2019 - India\u2019s top conference on the Python programming language</li> <li>Open Source: I\u2019ve continued to maintain Awesome-NLP for years now. It now has over 9 thousand stars and 80+ contributors now. It\u2019s officially recommended by Stanford to their Deep Learning students now. </li> </ul>"},{"location":"writing/diwali_2019/#whats-next","title":"What's Next?","text":"<ol> <li>Writing and Tact</li> </ol> <p>I suspect more knowledge workers in 20s should invest in learning how to communicate via either public speaking or writing. My chosen medium will probably be writing. I intend to write about a select list of topics: technology and how it has shaped our choices.</p> <p>Owing partly to how I grew up, my default communication style has been confrontational. I'll incorporate a lot more tact, grace and consideration in the years ahead. Be more Kabira. </p> <ol> <li>Chase external validation</li> </ol> <p>My professional and personal framework so far been focused internally. I did not value traditional credentials like degrees, grades, that exclusive college club and so on very highly. In hindsight, this was a bad career decision. </p> <p>External validation is what sets apart sustainable, long term careers from short one-trick wonders. This mindset shift will be hard. And will probably be a long 5-15 year journey, and I\u2019ll keep you posted - unless you want me to stop. Just let me know \ud83d\ude42 </p> <p>Till then, Happy Diwali and a Happy New Year!</p> <p>Best, Nirant</p>"},{"location":"writing/failresume/","title":"Fail Resume","text":"<p>A fail resume is, as its name suggests, a list of rejections and setbacks.</p> <p>This concept originated in academia, but can be applied to improve one\u2019s career, resilience, and approach to challenges in any walk of life.</p> <p>I\u2019ve organized my setbacks by year (starting with 2016), and what constitutes a \u201cfailure\u201d (or fail) here is based solely on my own judgement.  It\u2019s important to not only list the failures, but also learn from them, so I will add reflections to some of them (in progress). </p>"},{"location":"writing/failresume/#2021","title":"2021","text":"<ul> <li>Never heard back from Forethought.ai, Coinbase. Had applied via the cold form on their websites.</li> <li>Rejected by Flowrite because they could not afford me</li> <li>Ghosted by XFlowPay in mid-August</li> <li>Ghosted by Google Recruiter in July</li> </ul>"},{"location":"writing/failresume/#2020","title":"2020","text":"<ul> <li>Rejected by UMass, UW, CMU Language Technolog, UCB MIMS, and bunch of other Grad school programs because of a bad recco</li> <li>Lesson Learnt: Never trust an American</li> <li>Sahil, the Product Manager and Verloop and Piyush, the CTO at Verloop quit -- atleast in part because I think I overwhelmed them</li> <li>Got cold-rejected by someone I thought it'd work out with romantically</li> </ul>"},{"location":"writing/failresume/#2019","title":"2019","text":"<ul> <li>Spent 6 weeks prototyping a solution at Verloop.io which doesn't break even on cost</li> <li>Lesson Learnt: In Enterprise SaaS, never write any code before someone tells you what they are willing to pay for that</li> <li>Tried to get better at DevOps, but ultimately could not because of lack of discipline</li> </ul>"},{"location":"writing/failresume/#2018","title":"2018","text":"<ul> <li>Did not even get interview callbacks from my favourite NLP Company at the time: Rasa</li> <li>Had to quit Soroco because of lack of opportunities to grow internally. Really loved the crew, which was smart but not agile or humble</li> <li>Did not even apply to GoJek, Linkedin and Swiggy's kickass Data Science teams</li> <li>Lesson learnt: Be proactive. Take moonshots. Setting an upper-cap on your ambitions is kinda stupid</li> </ul>"},{"location":"writing/failresume/#2017","title":"2017","text":"<ul> <li>Samsung: Filed for two patents -- which despite being selected, eventually never materialised</li> </ul>"},{"location":"writing/failresume/#2016","title":"2016","text":"<ul> <li>Did not even clear Amazon's first programming round on campus</li> </ul> <p>Inspired from Katherine Huang's Fail Resume</p>"},{"location":"writing/fig-for-devs/","title":"Function, Industry, Geography: Career Framework","text":"<p>Your career is a combination of Function, Industry, Geography.</p> <p>That's it.</p> <p>That's the framework.</p> <p>You can change one of these. Not all three at a time.</p> <p></p>"},{"location":"writing/fig-for-devs/#why-only-one-at-a-time","title":"Why only one at a time?","text":"<p>If you want to change your function, you need to learn new skills.</p> <p>If you want to change your industry, you need to understand the new industry and the skills required to be successful in it.</p> <p>If you want to change your geography, you need to uproot your life and move to a new place.</p>"},{"location":"writing/fig-for-devs/#but-what-if-i-want-to-change-all-three","title":"But what if I want to change all three?","text":"<p>You can, but it's difficult and perhaps needs a lot of thinking cycles and internal conviction.</p>"},{"location":"writing/fig-for-devs/#cheat-code-higher-education","title":"Cheat code: Higher Education","text":"<p>MS/PhD/MBA helps folk change 2 of these at a time:</p> <ol> <li>You might get into a PhD program which changes your function and industry both</li> <li>MS abroad program which changes your function and geography</li> <li>MBA program which changes your industry (e.g. IT services to consulting) and geography (e.g. India to US)</li> </ol>"},{"location":"writing/fig-for-devs/#clarity-helps-a-lot","title":"Clarity Helps a Lot","text":"<p>The more granular you get, the easier it is to decide convert the wants into actionable steps. </p> <p>Painful: I want to be a Machine Learning Engineer. </p> <p>Tolerable: I want to be a Machine Learning Engineer at a Big Tech company.</p> <p>Acceptable: I want to be a Machine Learning Engineer at Google.</p> <p>Good: I want to be a Machine Learning Engineer at Google in New York.</p> <p>Great: I want to be a Machine Learning Engineer working on the problems around generating human-like speech at Google in New York.</p>"},{"location":"writing/fig-for-devs/#what-if-i-am-not-clear-about-what-i-want","title":"What if I am not clear about what I want?","text":"<p>If you are not clear about what you want, that's totally fine.</p> <p>You can start with the most granular level and work your way up. </p> <p>Start with the job description of the role you want. Talk to at least 12 people who are in that role.</p> <p>Ask them what they do on a day-to-day basis. Ask how they got to where they are today. Ask what they ask when they are hiring or interviewing.</p> <p>People on the Internet call this thing informational interviews and there's plenty of decent advice out there.</p>"},{"location":"writing/fig-for-devs/#recommended-reading","title":"Recommended Reading","text":"<ol> <li>The Antidote to I'm Feeling Stuck? from Swanand</li> <li>Act Like You're 35 from Nirant</li> </ol>"},{"location":"writing/first-90-days/","title":"The First 90 Days for a New Engineer","text":"<p>Aditya Ankur</p> <p>I know that there is a book for the first 90 days as an executive. Is there something similar for programmers?</p> <p>I don't quite know of a book/essay which covers this yet sticks to the question. So I am writing one for him.</p>"},{"location":"writing/first-90-days/#the-first-90-days-for-a-new-engineer","title":"The First 90 Days for a New Engineer","text":"<p>I expect each step to take roughly between 10 and 30 days, depending on the pace of your project + size of the team. Do them in order i.e. 1 before 2, and 2 before 3. </p>"},{"location":"writing/first-90-days/#1-study-the-ecosystem","title":"1. Study the Ecosystem","text":"<p>Get a very nuanced understanding of what the team considers as high quality in that context. Sometimes it's scale, sometimes it's speed or something else altogether. There are usually at most 1-2 things which every engineering team does to their best.</p> <p>Get seen by your senior engineering teammates. Not only your manager. </p> <p>Get super comfy with all docs, codebases, APIs and people who work on those projects.</p> <p>For every repo/codebase that you study: Add comments, docstrings, setup instructions, make small improvements e.g. unit tests. </p> <p>Write notes for every meeting and share with the right people. Setup Zoom coffees at the very least.</p> <p>Shadow and track 2-3 recent support tickets to see how the on-call system works. And then, if possible, shadow someone for a week of on-call. Could be formal or volunteer to help. </p> <p>Schedule a recurring 1-1 with your manager. Ask them to write down/describe explicit expectations from you. If they don't write, you write it down and email it to them -- and then follow up 90 days later.</p> <p>Note that you need to know the team's perspective on their strengths and not just the manager's. There can be some gap between the two.</p> <p>These are the things and people who'll be able to help when you get stuck in a technical problem later. They'll know that you are not an incompetent prick, and have done your homework. This trust is cheap to earn and surprisingly sticky when done early -- but hard to earn after first 90 days.</p>"},{"location":"writing/first-90-days/#2-quick-loud-win","title":"2. Quick Loud Win","text":"<p>This is more for early career folk and less for senior folk. Do something which is a big, tricky, much talked about the project and then do it fast. Doing it perfect is kind of less important. Why? Because no one expects early-career people to get it perfectly right anyway.</p> <p>My hack for this has been to pick a project with some well understood technical risk. But! But! Pick where expectations are clear. For instance, towards the tail end of a big project which needs a small feature to be complete and closed.</p> <p>If you can, pick a task which is closer to your technical strengths. E.g. I've done enough text classification in my short career that I can prototype one in 6-8 hours.</p> <p>The point of a quick \"loud\" win(s) is to ensure that you get a steady stream of projects to pick from -- forever. </p> <p>Good projects should chase you, instead of vice versa. A loud project, well done, will make you attractive to other project owners. Not just quick: Quick loud win. </p> <p>I hope these help when you join a new gig as an engineer. </p> <p>All the Best, </p> <p>Natkhat Nirant</p> <p>PS: For more senior people on the Engineering Management track, I recommend reading this instead: https://lethain.com/first-ninety-days-cto-vpe/</p>"},{"location":"writing/goodfight/","title":"The Good Fight","text":"<p>In the first few pages of the book Born Standing Up, Steve says something poignant:</p> <p>One can have, it turns out, an affection for the war years.</p> <p>He was someone chasing something external: fame, glory, money or a version of these. Yet, in his dusk years \u2014 once he had achieved some of those, he valued his struggle more than his fame and glory.</p> <p>For Steve, it was not about achieving these, it was, as I have come to call it: a good fight.</p> <p>The default setting in human mind is \u2018unsatisfactory\u2019. We like to make things and build them. We like change. Everyone I have met, has something more they want to do. Even saints and gurus who preach otherwise have them.</p> <p>The well-to-do in every society revere artists, self-made billionaires, and Amazonian empire builders. This is not only because they have things we desire: wealth, fame or influence.</p> <p>But more so because they have done something which we know we don\u2019t have enough stomach for: fight the good fight.</p> <p>We admire perseverance and grit on an emotional level.  The fight seems to be a human condition.  The pursuit of happiness is futile.</p>"},{"location":"writing/meetup-params/","title":"Meetup Parameters","text":"<p>This is based on organising GenerativeAI Meetups in Bengaluru, India. This is a living document and will be updated as we learn more.</p>"},{"location":"writing/meetup-params/#venue","title":"Venue","text":"<ol> <li>Date &amp; Time &amp; Duration: Choose suitable timing and duration. Pick something that works for your community. Consider weekday and weekend meetups both.<ul> <li>Example: 4:00-5.00pm start on a Saturday works great in BLR! Chennai did a GenAI meetup on Saturday morning -- since that city wakes up early, it worked well for them.</li> </ul> </li> <li>Camera: Consider the requirements for A/V if planning to do talks, streaming, or recording? Who is going to record? What kind of camera do we need? GenerativeAI outsources this to Hasgeek. Camera set-up for meetups is reduced to iPhone capture, with a Pivo Pod and tripod. </li> <li>Format: Define the structure of the meetup - is it just for drinks? Will there be talks? Is food provided? Is it open or by invitation only? Are plus ones allowed? Examples: GenerativeAI has never served alcohol, we often have 1-3 talks with snacks and is open by invitation only. We also have a Code of Conduct that we share with attendees.</li> <li>Speakers: If there will be talks, secure 1-3 speakers in advance. It's also fine to have a meetup with no speakers.</li> <li>Security: Keep in mind some venues might require pre-registration for security. Different venues enforce security with varying degrees of strictness. Some venues didn't allow anyone without registration, while the other venues allowed folks we didn't want to attend to enter. Discuss this upfront with your venue's security incharge.</li> </ol>"},{"location":"writing/meetup-params/#theme","title":"Theme","text":"<ul> <li>Select a theme. This is crucial as it shapes the shared identity of all attendees and influences the discussions they initiate with strangers</li> <li>Narrower themes are better than wider ones e.g. \"DevTools\" is better than \"Enterprise tools for Devs\"</li> <li>Choose a catchy name to attract more attendees -- your naming is the most important branding</li> <li>There are icebreaker lists on the Internet which you can use for more intimate meetings</li> <li>Name tags: I've tried to enforce these, but failed at the GenerativeAI Meetups -- they're great! I've seen them work well at other meetups.</li> </ul>"},{"location":"writing/meetup-params/#photo-and-video-documentation","title":"Photo and Video Documentation","text":"<ul> <li>Encourage attendees to take photos, tag you, or send them to you for sharing on social media</li> <li>Consider recording the talks and posting them on YouTube afterwards to provide value to the community and allow great talks to live on. We use Hasgeek for this.</li> <li>Note that live-streaming is not a must, but if done, it can add an extra layer of engagement for those who can't attend in person.</li> </ul>"},{"location":"writing/ml-hiring/","title":"Dos and Don'ts for ML Hiring","text":"<p>This is primarily for my future self. These are observations based on my own experience of 2 years at Verloop.io and helping a few companies hire for similar roles. </p>"},{"location":"writing/ml-hiring/#do","title":"Do","text":"<ul> <li>Seniormost hire first: Start by hiring the senior most person you're going to hire. E.g. start by hiring the ML Lead (assuming you already have a CTO)</li> <li>Have a means to tell that your investment in data science is working out or not</li> <li>Closest to User first: Hire the person who will consume the data to build for the user first</li> <li>Sourcing: Begin sourcing early and over-emphasise two channels: Referrals and Portfolios<ul> <li>Typically, in India - expect:<ul> <li>~2 months to close a full time role at early career (0-3 years) and </li> <li>~3 months to close a mid career (3-7 years) and</li> <li>6+ months to close a senior hire</li> </ul> </li> </ul> </li> <li>If a developer has open source code contributions in the last 2-3 years, consider waving off the coding or algorithmic challenge to speed up the interview process</li> <li>Pay above market cash salaries<ul> <li>In 12-18 months from now, when your ML Engineer will have internalised all the requirements, company culture and built a bunch of important tooling - she would get an offer which is 2-3x of today. If you're already paying above market salaries, a 20-30% jump is quite often enough to retain many folks </li> </ul> </li> <li>Have 3 at least versions of your shipping timeline</li> <li>Do hire full stack Data Science people/teams.  If you're hiring for early members for your team, this is practical necessity. An example of T shaped skills could look like</li> </ul>"},{"location":"writing/ml-hiring/#dont","title":"Don't","text":"<ul> <li>Rely on HR or your usual backend engineering hiring channels to work well for you, in general</li> <li>Don't hire the person who builds the means to move data (e.g. ML before data engineering) before hiring atleast 1-2 stakeholders in ML<ul> <li>Why? This is because it's cheaper (and often faster) to change ML modeling approach than to make changes in data engineering pipelines</li> </ul> </li> <li>Don't start by hiring an intern to implement papers or take things to production before you've done them</li> <li>Don't expect data science to deliver or ship at the same \"user value\" pace as Product Engineering<ul> <li>Why? Data Science suffers from the twin problems of being new and experiment-driven</li> </ul> </li> <li>Don't assume that you've so much data, and since all of it is queryable, it's all usable</li> <li>Hire ultra-specialists e.g. post-docs and PhDs too early, barring products which requires invention and not application</li> </ul>"},{"location":"writing/mlops-for-startups/","title":"MLOps for Startups","text":"<p>Start your development by writing the overall impact and feature overview in the Press Release doc and README</p> <p>If your time to ship is more than 2 weeks, write a functional spec</p> <p>In case of bug fixes, add bug details or link to Asana/Github Issues</p> <p>Always. Do trunk-based development. Don\u2019t restrict a deployment trigger to specific people. As soon as you are done, go ahead, deploy and let others deploy.</p>"},{"location":"writing/mlops-for-startups/#service-details","title":"SERVICE DETAILS","text":""},{"location":"writing/mlops-for-startups/#docs","title":"DOCS","text":"<ol> <li>Please provide API documentation for your service (e.g. via API definition)</li> <li>Add auto-generated engineering docs in HTML/Markdown</li> <li>Who is the maintaining team/person at this moment for the service? All maintainers and committers should be listed Repo README should include instructions to set up this repo for development</li> </ol>"},{"location":"writing/mlops-for-startups/#service-component-database","title":"Service Component \u2014 DATABASE","text":"<ol> <li>List down all the database changes, if you added any columns, removed any columns, added or removed any tables.</li> <li>What kind of indexes do you have? If you added a new column does it require an index? If yes, why? If no, why not?</li> <li>Do you make changes to records? Do you do frequent deletes or updates?</li> </ol>"},{"location":"writing/mlops-for-startups/#service-component-monitoring","title":"Service Component \u2014 MONITORING","text":"<ol> <li>List all the services you own and list down each server monitoring parameters</li> <li>Alerts for service uptime, service performance degradation e.g. latency, throughput</li> <li>Alerts for service machine disk/CPU/memory \u2014 what\u2019s the threshold and how are they triggered</li> </ol> <p>Please include today\u2019s screenshots for each of them e.g. StackDriver. We need to make sure that you have proper monitoring in place.</p>"},{"location":"writing/mlops-for-startups/#service-component-deployment","title":"Service Component \u2014 DEPLOYMENT","text":""},{"location":"writing/mlops-for-startups/#todo-add-a-step-by-step-documentation-on-how-to-write-your-first-service-and-deploy-it-in-the-devstageproduction-environment-for-your-org","title":"TODO Add a step by step documentation on how to write your first service and deploy it in the dev/stage/production environment for your org","text":""},{"location":"writing/mlops-for-startups/#code-styling","title":"CODE STYLING","text":""},{"location":"writing/mlops-for-startups/#python","title":"PYTHON","text":"<ol> <li> <p>Add a pre-commit with black, sort and flake8 to your code. Follow the happy path convention. Add type hints</p> </li> <li> <p>Start writing code by writing APIs, then tests and then implement code. We use tests as API docs</p> </li> <li> <p>Add proper liveness and/or readiness check for your Kubernetes deployment:</p> </li> </ol> <p>Liveliness check is for Kubernetes to know whether your application is running.</p> <p>Readiness check is for you to specify when traffic should be sent to the pod. For example, if your pod needs some operations to be done before it can take traffic (such as downloading a dataset), your readiness check should send back 200 only once it's completely ready for taking traffic.</p> <p>Docs:</p> <ol> <li>Kubernetes Manual</li> <li>Blog on Best Practices</li> </ol>"},{"location":"writing/mlops-for-startups/#data-best-practices","title":"DATA BEST PRACTICES","text":""},{"location":"writing/mlops-for-startups/#version-control","title":"VERSION CONTROL","text":"<p>Use dvc.org and version all your datasets to Cloud Storage Buckets and include your DVC file in your Github repo</p>"},{"location":"writing/mlops-for-startups/#experiments","title":"EXPERIMENTS","text":"<ol> <li> <p>Aim for reproducible experiments by re-using actively maintained APIs</p> </li> <li> <p>There is no specific tooling to do reproducible experiments, but consider using something simple like Sacred or Hydra</p> </li> </ol>"},{"location":"writing/model-monitoring/","title":"ML Model Monitoring","text":"<p>Mayank asked on Twitter: </p> <p>Some ideas/papers/tools on  monitoring models in production. A use case would be say a classification task over large inputs. I want to visualise how are the predicted values or even confidence scores vary over time? (paraphrased)</p>"},{"location":"writing/model-monitoring/#quick-hacks","title":"Quick Hacks","text":""},{"location":"writing/model-monitoring/#pandas-profiling","title":"pandas-profiling","text":"<p>If you are logging confidence scores, you can begin there. The quickest hack is to visualize with pandas-profiling: https://github.com/pandas-profiling/pandas-profiling/</p>"},{"location":"writing/model-monitoring/#rolling-means","title":"Rolling means","text":"<p>Calculate rolling aggregates (e.g. mean, variance) of your confidence scores. pandas inbuilt. Quite quick. Add them to your set of monitoring and alerting product metrics. </p> <p>A better version of this would be to do it on cohort level. Actually, doing all the following analysis on cohort level makes sense.</p>"},{"location":"writing/model-monitoring/#confidence-scores-and-thresholds","title":"Confidence Scores and Thresholds","text":"<p>One of the most common mistakes is to use static threshold(s) on a confidence score(s). </p> <p>If you hear someone saying that they do not use thresholds for a classification problem. Stop and think. They are using a threshold, usually 0.5 from within the ML library that you are using. </p> <p>This is sub-optimal. The better option would be to use a holdout validation set and determine the threshold from that. </p>"},{"location":"writing/model-monitoring/#tagging-data","title":"Tagging Data","text":"<p>It is obvious that you will tag the predictions for which the model is least confident -- so that the model can learn. </p> <p>What you should also do is this: </p> <ul> <li> <p>Find out samples which have high confidence and tag them first, this is a form of negative sample mining</p> </li> <li> <p>For multi-class classification: Figure out samples which did not clear your threshold, and the prediction is correct. Add these back to your new training+validation set</p> </li> <li> <p>Tag samples which are too close to the threshold. This will help you understand your model and dataset's margin of separation better</p> </li> </ul>"},{"location":"writing/model-monitoring/#training-serving","title":"Training-Serving","text":"<p>The most common causes of trouble in production ML models is training-serving skews or differences.</p> <p>The differences can be on 3 levels: Data, Features, Predictions</p>"},{"location":"writing/model-monitoring/#data-differences","title":"Data Differences","text":"<p>Data differences can be of several types, the most frequest are these: Schema change - someone dropped a column!,  Class Distribution Change - When did this 10% training class have 20% predictions, or  Data Input Drift - users have started typing instead of copy-pasting!</p>"},{"location":"writing/model-monitoring/#schema-skew-from-googles-ml-guide","title":"Schema skew (from Google's ML Guide)","text":"<p>Training and serving input data do not conform to the same schema.  The format of the serving data changes while your model continues to train on old data.     </p> <p>Solution? Use the same schema to validate training and serving data. Ensure you separately check for statistics not checked by your schema, such as the fraction of missing values </p>"},{"location":"writing/model-monitoring/#class-distribution-check-with-great-expectations","title":"Class Distribution check with Great Expectations","text":"<p>Training and serving input data should conform to the same class frequency distribution.  Confirm this. If not, update the model by training with updated class frequency distribution. </p> <p>For monitoring these first two, check out: https://github.com/great-expectations/great_expectations</p> <p>For understanding data drift, you need to visualize data itself. This is too data-domain specific (e.g. text, audio, image). And more often than not, it is just as better to visualize features or vectors.</p>"},{"location":"writing/model-monitoring/#feature-viz-for-monitoring","title":"Feature Viz for Monitoring","text":"<p>Almost all models for high dimensional data (images or text) vectorize data. I am using features and vectorized embedding as loosely synonymous here.</p> <p>Let's take text as an example:</p>"},{"location":"writing/model-monitoring/#class-level-with-umap","title":"Class Level with umap","text":"<p>Use any dimensionality reduction like PCA or umap (https://github.com/lmcinnes/umap) for your feature space. Notice that these are on class level. </p> <p></p> <p>Plot similar plots for both training and test, and see if they have similar distributions. </p>"},{"location":"writing/model-monitoring/#prediction-viz-for-monitoring","title":"Prediction Viz for Monitoring","text":"<p>Here you can get lazy, but I'd still recommend that you build data-domain specific explainers</p>"},{"location":"writing/model-monitoring/#sample-level-with-lime","title":"Sample Level with LIME","text":"<p>Consider this for text:</p> <p></p> <p>Check out other black box ML explainers: https://lilianweng.github.io/lil-log/2017/08/01/how-to-explain-the-prediction-of-a-machine-learning-model.html by the amazing @lilianweng</p>"},{"location":"writing/model-monitoring/#class-level","title":"Class Level","text":"<p>You can aggregate your predictions across multiple samples on a class level:</p> <p></p>"},{"location":"writing/model-monitoring/#training-data-checks","title":"Training Data Checks","text":"<p>Expanding on @aerinykim's tweet</p>"},{"location":"writing/model-monitoring/#robustness","title":"Robustness","text":"<p>Adding in-domain noise or perturbations should not change the model training and inference both.</p>"},{"location":"writing/model-monitoring/#citations-and-resources","title":"Citations and Resources","text":"<p>[1] Machine Learning Testing in Production: https://developers.google.com/machine-learning/testing-debugging/pipeline/production</p> <p>[2] Recommended by DJ Patil as \"Spot On, Excellent\": http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html</p> <p>[3] Practical NLP by Ameisen: https://bit.ly/nlp-insight. The images for umap, LIME, and aggregated LIME are all from nlp-insight</p> <p>[4] Machine Learning:The High-Interest Credit Card of Technical Debt: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43146.pdf</p>"},{"location":"writing/peakxv-hack/","title":"PeakXV Summit Hackathon!","text":"<p>Last weekend, I was a mentor at the PeakXV Summit Hackathon. This year's theme was Consumer AI. </p> <p>It was a fantastic hackathon with exceptional organization by the PeakXV team. The space which they selected and the overall timeline was quite good, the organization ranging from food to Internet and credits was quite neat and well done. </p> <p>It was safe to say that the 5-7 ideas that I interacted with ranged from the extremely mundane to boring [1]. The most interesting attempt was the game world generation from Rajeev Singh Naruka. I'm glad to hear that he did win the overall hackathon and as a long time friend I'm very very happy for him. </p> <p>The media winners were Sheetal Chauhan (Microsoft, Sarvam) and Vijit. </p> <p>The AI Journaling idea was quite interesting though that's closer to a prosumer idea from an indiehackers perspective than a consumer one. </p> <p>[1] Ideas: 3x AI Assistant for the CEO to book cab, order groceries using voice + computer use, AI Journaling, Superhuman for WhatsApp. </p>"},{"location":"writing/pgvector-vs-qdrant/","title":"pgvector vs Qdrant- Results from the 1M OpenAI Benchmark","text":"<p>You may have considered using PostgreSQL's <code>pgvector</code> extension for vector similarity search. There are good reasons why this option is strictly inferior to dedicated vector search engines, such as Qdrant.</p> <p>We ran both benchmarks using the ann-benchmarks solely dedicated to processing vector data. The difference in performance is quite staggering.</p>"},{"location":"writing/pgvector-vs-qdrant/#query-speed","title":"Query Speed","text":"<p>Final results show that <code>pgvector</code> lags behind Qdrant by a factor of 15 when it comes to throughput.  </p> <p>That is a 1500% deficit in speed. However, we shouldn't only consider speed as the main metric when evaluating a database. In terms of accuracy, <code>pgvector</code> delivers way fewer relevant results than Qdrant.</p>"},{"location":"writing/pgvector-vs-qdrant/#workload","title":"Workload","text":"<p>Interestingly, these disparities start to surface with as few as 100,000 chunked documents. </p> <p></p> <p>As an ardent supporter of PostgreSQL, it is disheartening to witness that <code>pgvector</code> doesn't just commence at under half the QPS at 100,000 vectors, when compared to Qdrant - it plunges precipitously beyond that. </p> <p></p>"},{"location":"writing/pgvector-vs-qdrant/#correctness","title":"Correctness","text":"<p>One might try to rationalize this by assuming that Postgres is slower, but more accurate? Data reveals that <code>pgvector</code> is not just slower, but also ~18% less accurate!</p> <p></p> <p>We measure this using the same methodology as the ann-benchmarks codebase: k-NN bruteforce as ground truth.</p>"},{"location":"writing/pgvector-vs-qdrant/#latency","title":"Latency","text":"<p>Here, Qdrant holds its own. The worst p95 latency for Qdrant is 2.85s, a stark contrast to pgvector, whose best p95 latency is a full 4.02s. Even more astonishing, pgvector's worst p95 latency skyrockets to an unbelievable 45.46s.</p>"},{"location":"writing/pgvector-vs-qdrant/#benchmark-specs","title":"Benchmark Specs","text":"<p>The machine we used to run the benchmark: <code>t3.2xlarge, 8 vCPU, 32GB RAM</code></p> <p>For data enthusiasts among us, this Google Sheet details all the numbers for a more in-depth analysis: Google Sheet</p>"},{"location":"writing/pgvector-vs-qdrant/#configuration","title":"Configuration","text":"<p>We use the default configuration for Qdrant and much better parameters for pgvector:</p> <pre><code>Qdrant(quantization=False, m=16, ef_construct=128, grpc=True, hnsw_ef=None, rescore=True)\n</code></pre> <pre><code>PGVector(lists=200, probes=2)\n</code></pre> <p>The <code>pgvector</code> recommendation which'd be possibly worse performance-wise: <pre><code>PGVector(lists=1000, probes=1)\n</code></pre></p> <p>There is much more to be tested. We will continue to explore the configuration space for both platforms and update this. </p>"},{"location":"writing/pgvector-vs-qdrant/#conversations-with-the-community","title":"Conversations with the Community","text":"<p>Paul Copplestone (CEO, Supabase) has also shared his thoughts on the matter:</p> <p>Yup: 1. Wait 6 months, a lot of development is happening on pgvector 2. Use hybrid search 3. Use filters on other indexed columns 4. Use partitions</p> <p>And as always, take benchmarks with a grain of salt, they are never as clear-cut as they seem. We\u2019ll publish benchmarks soon too using the latest version of pgvector</p> <p>Adding my notes here:</p> <p><code>pgvector</code> uses full-scan when there are filters or hybrid search. This is a very slow algorithm when using 1536 embeddings. It's <code>O(n)</code> where <code>n</code> -&gt; number of vectors matching the filter. </p> <p>When there are no filters, <code>pgvector</code> uses IVF. This is a slower algorithm when using 1536 embeddings, and it\u2019s less accurate than Qdrant's HNSW. </p> <p>Aside: Feel free to check out my Twitter Intro to IVFPQ.</p> <p>@jobergum, creator of Vespa.ai (a vector search engine) also shared his thoughts:</p> <p><code>pgvector</code> is an extension which default will just search the closest cluster to the query vector which for most high dimensional embedding models will return just 2-3 out of 10 real neighbors.</p> <p>This is a very important point. <code>pgvector</code> is not a vector search engine. It's a vector extension for PostgreSQL, and that involves some tradeoffs which are sometimes not obvious.</p> <p>There is a US$2000 bounty for anyone who can raise a PR to make the <code>pgvector</code> extension use HNSW instead of IVF.</p>"},{"location":"writing/pgvector-vs-qdrant/#acknowledgements","title":"Acknowledgements","text":"<p>The engineering and dataset were both done by Kumar Shivendu. Most of my contribution was in the form of spotting the bottlenecks, feedback and sponsorship.</p> <p>These surprising revelations are courtesy of Erik Bernhardsson's ann-benchmarks code.</p>"},{"location":"writing/picking-startup-to-work-with/","title":"Why we need to do a reference check on startup teams we work with?","text":"<p>Great Startups always ask someone who is joining for a reference check.  They speak to your previous managers and colleagues. There is no reason why candidates should not do the same. Many startup employees have more opportunity costs than the founders themselves.</p> <p>I'm surprised by the lack of due diligence which candidates do when interviewing with us. I assume that at least a part of this is that they don't know that they can do this. And second: how to check which startups are worth working with?</p> <p>Well, first off: </p> <p>I, Nirant, with the power vested in me by decent human beings who care about their lives, hereby grant you the license to do reference checks at places where you will work. </p> <p>That out of our way, I'll invest the rest of this post how to evaluate startup teams. I write this with an engineer's context, with early career folk in mind. The mindset should transfer to other functions, but the specific examples will not. </p>"},{"location":"writing/picking-startup-to-work-with/#how-do-you-pick-a-team-you-want-to-work-with","title":"How do you pick a team you want to work with?","text":"<p>Given that you're reading this, I am assuming you're are a proactive person. </p> <ul> <li>Pick teams which grow on your strengths</li> <li>Pick teams which mentor proactive people</li> <li>Pick teams which take pride in promoting people who are young or from similar backgrounds as you</li> </ul> <p>Go where you're a profit center, not a cost center:</p> <p>For instance, if your strengths are in Natural Language Processing, you might not want to go work for a neobank. Why? Because their moats and growth come from consumer trust and regulation. Your innovation is a cog in the wheel. </p>"},{"location":"writing/picking-startup-to-work-with/#when-you-are-interviewing","title":"When you are Interviewing","text":"<p>When you are interviewing with a team, ask your interviewer</p>"},{"location":"writing/picking-startup-to-work-with/#on-mentoring","title":"On Mentoring","text":"<p>Have they mentored someone recently? Are they mentoring someone for the last few months? </p> <ul> <li>What are the key accomplishments of their mentees?</li> <li>What incentives (direct or indirect) do they have for mentoring others?</li> <li>Is the mentee growing enough to inspire/challenge the mentor?</li> </ul>"},{"location":"writing/picking-startup-to-work-with/#org-scaling","title":"Org Scaling","text":"<p>Do they scale by hiring more people or up-skilling employees? What is the balance between these extremes?</p> <ul> <li>Do they promote internally? If not, ask/learn why?</li> <li>Would they back you if you want to learn something adjacaent to your skill?</li> <li>What is the state of tooling which acts as safeguards e.g. central platform, data teams, and accelerators in the ecosystem e.g. standardized API configs and deployment across all devs?</li> </ul> <p>(Optional) How many engineers have quit in the last 6 months as a percentage of team size?</p>"},{"location":"writing/picking-startup-to-work-with/#on-decision-making","title":"On Decision Making","text":"<p>How do they design systems? E.g. who makes the final decision? You might not want to work for hero driven or extremely consensus driven engineering teams.</p> <ul> <li>What does their release cycle look like? </li> <li>How do they handle technical debt?</li> <li>What are the safeguards (e.g. central platform, data teams) and accelerators in the ecosystem (standardized API configs and deployment across all devs)?</li> </ul> <p>Asking questions along these lines will give you a pretty fair sense of how they think about investing and growing people, as well as how they make decisions. Those are probably the Top 3 on what determines what your experience will be like with that team. </p>"},{"location":"writing/picking-startup-to-work-with/#things-to-avoid","title":"Things to Avoid","text":"<p>Teams which have communication failures e.g. when you talk to interviewers the company messaging is inconsistent. One person might pitch college pedigree of existing employees. While another person is pitching how meritocratic they are. </p> <p>Communication failures are easiest to spot. It\u2019s symptomatic of larger problems within the company. When you have a high temperature, you know you are unwell, but you can\u2019t guess why. That\u2019s fine.</p> <p>If the team does not value hiring, that means the people you work with will neither be effective, nor motivated.</p> <p>Some of the most common reasons behind inconsistent communication are these:</p> <ul> <li>leaders are poor communicators of their vision/roadmap</li> <li>mid-level managers are poor at executing the vision/roadmap</li> <li>they don\u2019t have a sense of what \u201cquality\u201d or skills are needed to accomplish desired goals</li> </ul> <p>There is a motley mix of reasons why communication can be broken. Either way, you don\u2019t want to work with such a team.</p> <p>And ohh, on startups with condescending recruiters:  If someone who is supposed to invite you within the team is not 5-start hospital courteous on call - the odds that they'll be polite &amp; professional when you join are low.</p>"},{"location":"writing/picking-startup-to-work-with/#who-to-talk-to","title":"Who to Talk To","text":""},{"location":"writing/picking-startup-to-work-with/#previous-employees","title":"Previous Employees","text":"<p>Common Myth: Need to talk to someone from exactly the same role as you. </p> <p>Better: Anyone who has worked with that team, or close to that team. For instance, do not hesitate to talk about a backend engineering team's skill level from a Data Scientist or Product Manager. </p> <p>Small teams enable everyone to work closely with each other and have a lot more signal about each other's competence. That means, cross-skill/cross-function bar and ability of competence is quite well understood. </p> <p>Exception: Someone whose first job was the company you're doing a reference check. Most people seem to have extreme opinions about their first job, atleast till their 3<sup>rd</sup> or 4<sup>th</sup> job.</p> <p>Chai Sutta Story</p> <p>True Story: I was about to accept an offer from a company but on a whim, I decided to visit their office. Because I was waiting for my hiring manager to come to work, I hung around at the nearby chai sutta place.</p> <p>I learnt that the CTO had quit, the Engineering Managers were on notice period and other small array of red flags which I didn't see earlier. Worse, they had no one who could mentor me on data science and my hiring manager's strengths were different from ML. I met the hiring manager, and said No. That was a close save.</p> <p>The best place to learn about a company's culture is the chai-sutta place. The geekier and more engaged they're about work itself, and not bitching about work -- the better off that team is. Nothing sparks joy than joy of creation in a good team.</p> <p>If not any of these, I guess it'd be a good idea to do a little bit of Linkedin, Github and Twitter stalking. If none of the devs have never done anything impressive on any of these places, the team might just not be that high caliber. I'd wager that these websites capture something like atleast \u2154rd of the top 2% talent.</p>"},{"location":"writing/picking-startup-to-work-with/#present-employees","title":"Present Employees","text":"<p>Present employees in India typically avoid giving anti-referrals. I get why this happens: we like to gossip. That said, there are ways to get enough signal by asking a lot of specific questions. These work best when you've worked for some time (sorry to folks graduating this year):</p> <ul> <li>What is your typical release cycle? What are the steps? What does your developer tooling look like?</li> <li>How do you measure/assess the quality of your software? What metrics define these? What led you to pick these metrics?</li> <li>Of the people promoted in last 12 months, why do you think they were promoted? </li> <li>I saw on Linkedin that X recently quit to join company Y - do you have a guess on why that happened? </li> </ul> <p>These are all very  direct versions, they work for me because I am a little more direct than usual. My friends and juniors, both have used more indirect versions to great effect. The key idea is to get that information, it's less important whether you were tactful in getting it.</p>"},{"location":"writing/picking-startup-to-work-with/#how-to-reach-out-to-people","title":"How to reach out to people?","text":"<p>Well, there are no right answers to this. For some people, specially those from certain developer communities or good engineering colleges have a wide enough network. They've friends/seniors/juniors in almost every good company worth working with. </p> <p>If you don't, fret not. Internet is your friend. Tweet to people, find their email id from Github commit messages. For instance, I've had great luck following advice from CTO of Shaadi.com, whom I met over Twitter. And maybe it's finally time to give that 1 free month of Linkedin Premium a try for their inMail.</p> <p>I've personally had a decent modicum of success with emailing people, simply because developers don't get cold emails from other developers often enough. It's so rare that as long as I don't bungle it up, people do get back to me. Even if it's just to say \"I'm busy, but let's talk next week/month\". </p> <p>-- That's it. I hope this quick intro helped you get started on how to evaluate and pick teams worth working with. If you have questions, please feel free to reach out to me on Twitter (or email).</p> <p>Till we meet again, </p> <p>Natkhat Nirant</p> <p>Revised: September 2020. Original 2017 Edition lives here</p> <p>Get this directly in your inbox by leaving your email id here: </p>"},{"location":"writing/politics-at-work/","title":"Politics at Work","text":"<p>This quick entry is partially inspired by a similar entry from Manas Saloi. Ofc, other folks have offered their opinions on Basecamp and earlier Coinbase.</p> <p>The woke mob is shitting on Jason and DHH, but I strongly agree with Basecamp\u2019s new policies. IMHO a company is supposed to create an environment where people come and do their best work without having to align to one side of the political spectrum.</p> <p>Want to be activists and offer your 2c on every political issue? Sure, do that in your free time. The company slack channel is not the place for it. I am surprised that it is even a controversial opinion.</p> <p>Even more extreme position that I hold: Society (+most) Whatsapp groups should not have political discussions. I am there to know why the water pump did not run as per schedule. I don\u2019t go there to learn about Modi\u2019s covid performance report.</p> <p>(Just my thoughts. Does not reflect my employer yada yada.)</p>"},{"location":"writing/pythonfstrings/","title":"Best of Python 3 f-strings","text":"<p>This piece is primarily meant for those new to Python. These include mathematicians, economists, and so on who want to use Python within a Jupyter environment. Here is a quick guide on how to make Best of Jupyter.</p>"},{"location":"writing/pythonfstrings/#quick-primer","title":"Quick Primer","text":"<p>If you are familiar with earlier Python versions, here are my top picks on how to move from .format () to this new one:</p> <p>{{&lt; highlight python &gt;}}</p> <p>_fstring = f'Total: {one + two}'  # Go f-string! _format = 'Total: {}'.format(one + two) _percent = 'Total: %s' % (one + two) _concatenation = 'Total: ' + str(one + two) assert _fstring == _format == _percent == _concatenation {{&lt; /highlight &gt;}}</p>"},{"location":"writing/pythonfstrings/#f-string-magic","title":"f-string Magic","text":"<p>f-strings are how you should use print statements in Python. It is fairly reminiscent of LaTeX in it\u2019s inline notation: {{&lt; highlight python &gt;}}</p>"},{"location":"writing/pythonfstrings/#inline-variables-similar-to-latex","title":"inline variables, similar to LaTeX","text":"<p>name = \"Fred\" print(f\"He said his name is {name}.\")</p>"},{"location":"writing/pythonfstrings/#he-said-his-name-is-fred","title":"'He said his name is Fred.'","text":"<p>{{&lt; /highlight &gt;}}</p> <p>Notice how the variable name can now be used inline. This is a simple and easy to use syntax: just include the variable in surrounding {} while marking the string type as f-string using the \u2018f\u2019 in the beginning.</p> <p>Note to the advanced programmer: </p> <p>\u2018f\u2019 may be combined with \u2018r\u2019 to produce raw f-string which can be used inside regex or similar functions. \u2018f\u2019 may not be combined with \u2018u\u2019, this is because all Python3.6+ strings are Unicode by default now. This means, you can write fstrings in Hindi, Chinese, French, Korean and atleast 10 other languages.</p> <p>You can write fstrings in Hindi, Chinese, French, Korean and any language covered by Unicode.</p> <p>But why are these called formatted-strings in the first place? Because you can use with some cool formatting hacks.</p>"},{"location":"writing/pythonfstrings/#simplified-alignment-and-spacing","title":"Simplified Alignment and Spacing","text":"<p>Have you ever tried creating a table such as that for logging or visualization? Arranging the elements becomes a nightmare with several <code>\\t</code> tab characters flying around.</p> <p>This is much easier with Python f-strings using the colon \u2018:\u2019 operator, followed by a an alignment operator and field width value.</p> <p>There are atleast 3 alignment operator: &lt; for left aligned, &gt; for right aligned, and ^ for center aligned. Refer the code example:</p> <p>{{&lt; highlight python &gt;}} correct = 'correct' phonetic_correct = 'phonetic_correct' typo = 'typo' phonetic_typo = 'phonetic_typo' phonetic_distance = 'phonetic_distance'</p> <p>{{&lt; /highlight &gt;}} {{&lt; highlight python &gt;}}</p> <p>print(f'No Spacing:') print(f'{correct}|{phonetic_correct}|{typo}|{phonetic_typo}|{phonetic_distance}|\\n')</p>"},{"location":"writing/pythonfstrings/#no-spacing","title":"No Spacing:","text":""},{"location":"writing/pythonfstrings/#correctphonetic_correcttypophonetic_typophonetic_distance","title":"correct|phonetic_correct|typo|phonetic_typo|phonetic_distance|","text":"<p>{{&lt; /highlight &gt;}} {{&lt; highlight python &gt;}}</p> <p>print(f'Right Aligned:') print(f'{correct:&gt;10}|{phonetic_correct:&gt;20}|{typo:&gt;10}|{phonetic_typo:&gt;20}|{phonetic_distance:&gt;20}|\\n')</p>"},{"location":"writing/pythonfstrings/#right-aligned","title":"Right Aligned:","text":""},{"location":"writing/pythonfstrings/#correct-phonetic_correct-typo-phonetic_typo-phonetic_distance","title":"correct|    phonetic_correct|      typo|       phonetic_typo|   phonetic_distance|","text":"<p>{{&lt; /highlight &gt;}} {{&lt; highlight python &gt;}}</p> <p>print(f'Left Aligned:') print(f'{correct:&lt;10}|{phonetic_correct:&lt;20}|{typo:&lt;10}|{phonetic_typo:&lt;20}|{phonetic_distance:&lt;20}|\\n') </p>"},{"location":"writing/pythonfstrings/#left-aligned","title":"Left Aligned:","text":""},{"location":"writing/pythonfstrings/#correct-phonetic_correct-typo-phonetic_typo-phonetic_distance_1","title":"correct   |phonetic_correct    |typo      |phonetic_typo       |phonetic_distance   |","text":"<p>{{&lt; /highlight &gt;}} {{&lt; highlight python &gt;}}</p> <p>print(f'Centre Aligned:') print(f'{correct:<sup>10}|{phonetic_correct:</sup>20}|{typo:<sup>10}|{phonetic_typo:</sup>20}|{phonetic_distance:^20}|') </p>"},{"location":"writing/pythonfstrings/#centre-aligned","title":"Centre Aligned:","text":""},{"location":"writing/pythonfstrings/#correct-phonetic_correct-typo-phonetic_typo-phonetic_distance_2","title":"correct  |  phonetic_correct  |   typo   |   phonetic_typo    | phonetic_distance  |","text":"<p>{{&lt; /highlight &gt;}}</p> <p>You also have support for decimal truncation and similar standard formatting utilities: {{&lt; highlight python &gt;}}</p>"},{"location":"writing/pythonfstrings/#auto-resolve-variable-scope-when-nested","title":"auto-resolve variable scope when nested","text":"<p>width = 10 precision = 4 value = decimal.Decimal(\"12.34567\") print(f\"result: {value:{width}.{precision}}\")  # nested fields</p>"},{"location":"writing/pythonfstrings/#result-1235","title":"'result:      12.35'","text":"<p>{{&lt; /highlight &gt;}}</p> <p>You might notice something interesting here: width and precision are automatically picked up from the scope. This means you can calculate width and precision using screen width or other inputs from system and use that.</p>"},{"location":"writing/pythonfstrings/#full-python-expressions-support","title":"Full Python Expressions Support","text":"<p>The above is only possible because the expression inside {} is actually being evaluated, or in programming terms: being executed.</p> <p>This implies, that you can make any function call from within those {}.</p> <p>Though, you should avoid doing this in practice very often because it might make your debugging very difficult. Instead, store the returned value from function in a variable and then add the variable in a fstring print statement.</p> <p>Those coming from functional programming might miss their lambda functions. Don\u2019t worry, Python has you covered:</p>"},{"location":"writing/pythonfstrings/#lambda-functions-in-f-strings","title":"Lambda Functions in f-strings","text":"<p>{{&lt; highlight python &gt;}}</p>"},{"location":"writing/pythonfstrings/#if-you-feel-you-must-use-lambdas-they-may-be-used-inside-of-parentheses","title":"If you feel you must use lambdas, they may be used inside of parentheses:","text":"<p>print(f'{(lambda x: x*3)(3)}')</p>"},{"location":"writing/pythonfstrings/#9","title":"'9'","text":""},{"location":"writing/pythonfstrings/#note-that-this-returned-a-and-not","title":"note that this returned a  and not  <p>{{&lt; /highlight &gt;}}</p>","text":""},{"location":"writing/pythonfstrings/#summary","title":"Summary <ul> <li>f strings mean you can include variables and function calls inside your print statements</li> <li>Inline variables: these are easier to read and debug for the developer</li> <li>Use f-strings when you can!</li> </ul>","text":""},{"location":"writing/r1-for-non-technical-audience/","title":"The Evolution of R1: Building on Prior AI Research","text":"<p>The recent R1 papers represent an evolution of work from DeepSeek v3 and other prior open source research. Here are the 3 most significant technological implications:</p> <ol> <li> <p>Open Source Innovation in Reasoning: The R1 model stands as the first open source system to show reasoning chains.  The reasoning chains being open source means that almost everyone can make their own reasoning model now without being blocked on OpenAI or Anthropic to expose those. This mirrors Claude and Llama 3's impact from a year ago, which showed the possibility of building competitive models, but R1 takes this further by achieving state of the art performance without requiring original training data. </p> </li> <li> <p>Synthetic Data Revolution: The model's training approach relies on self-generated data, requiring minimal human input and eliminating the need for manual data tagging. While DeepSeek V3 used human-tagged data as a foundation, R1's reinforcement learning phase operates entirely on synthetic data. This represents the first publicly documented instance of achieving state-of-the-art performance using primarily synthetic data, breaking through the traditional bottleneck of human data labeling. This basically means that LLMs are now truly unblocked on human intelligence to get smarter. This is the opening of Pandora's box, which people have been talking about since 2012. </p> </li> <li> <p>Better Reinforcement Learning: The reinforcement learning methodology is a shift from conventional approaches. While the major American AI labs, including OpenAI, use complex, potentially unstable methods requiring extensive experimentation, DeepSeek's approach employs a more straightforward ranking-based methodology. This simplification, while still maintaining high performance, challenges existing assumptions about necessary complexity in AI development.</p> </li> </ol>"},{"location":"writing/r1-for-non-technical-audience/#example-grpo-with-multiple-chains-of-thought","title":"Example: GRPO with Multiple Chains of Thought","text":"<p>GRPO stands for Group Reward Policy Optimization. R1 is the first major model to do reinforcement learning on a large model backbone. While GRPO is not a new idea, it is the first time it has been applied to a large model. </p>"},{"location":"writing/r1-for-non-technical-audience/#1-generate-multiple-responses","title":"1. Generate Multiple Responses","text":"<p>For a math question (e.g., \"Solve x\u00b2\u22125x+6=0\"), the model generates 3 Chain of Thought responses:</p> <ul> <li>Response A: Uses factoring (\\((x-2)(x-3)=0\\))</li> <li>Response B: Uses the quadratic formula (\\(x = \\frac{-5\\pm\\sqrt{25-24}}{2}\\))</li> <li>Response C: Contains an arithmetic error (e.g., \\(x = -5\\pm3\\))</li> </ul>"},{"location":"writing/r1-for-non-technical-audience/#2-score-each-response","title":"2. Score Each Response","text":"<p>Assign rewards based on accuracy or adherence to format:</p> <ul> <li>Response A: Reward = 1.0 (correct and concise)</li> <li>Response B: Reward = 0.8 (correct but verbose) </li> <li>Response C: Reward = 0.2 (incorrect)</li> </ul>"},{"location":"writing/r1-for-non-technical-audience/#3-calculate-group-average-reward","title":"3. Calculate Group Average Reward","text":"<p>Compute the mean reward: Group Average = \\(\\frac{1.0 + 0.8 + 0.2}{3} = 0.67\\)</p>"},{"location":"writing/r1-for-non-technical-audience/#4-compute-relative-advantages","title":"4. Compute Relative Advantages","text":"<p>Subtract the group average from each response's reward:</p> <ul> <li>Response A: \\(1.0 - 0.67 = +0.33\\) (positive advantage)</li> <li>Response B: \\(0.8 - 0.67 = +0.13\\) (positive advantage)</li> <li>Response C: \\(0.2 - 0.67 = -0.47\\) (negative advantage)</li> </ul>"},{"location":"writing/r1-for-non-technical-audience/#5-update-the-policy","title":"5. Update the Policy","text":"<p>The model's parameters are updated to:</p> <ul> <li>Increase the probability of generating Response A (highest advantage)</li> <li>Slightly increase Response B (moderate advantage)</li> <li>Decrease Response C (negative advantage)</li> </ul> <p>The update is regularized using KL divergence to prevent abrupt policy changes.</p>"},{"location":"writing/r1-for-non-technical-audience/#broader-implications","title":"Broader Implications","text":"<p>This development demonstrates that innovative AI advancement isn't confined to American companies, highlighting the viability of multiple approaches in achieving comparable results. It suggests a democratization of AI development that could accelerate global progress in the field.</p>"},{"location":"writing/rag-best-practices/","title":"Retrieval Augmented Generation Best Practices","text":""},{"location":"writing/rag-best-practices/#retrieval-and-ranking-matter","title":"Retrieval and Ranking Matter!","text":""},{"location":"writing/rag-best-practices/#chunking","title":"Chunking","text":"<ol> <li>Including section title in your chunks improves that, so does keywords from the documents</li> <li>Different token-efficient separators in your chunks e.g. ### is a single token in GPT</li> </ol>"},{"location":"writing/rag-best-practices/#examples","title":"Examples","text":"<ol> <li>Few examples are better than no examples</li> <li>Examples at the start and end have the highest weight, the middle ones are kinda forgotten by the LLM</li> </ol>"},{"location":"writing/rag-best-practices/#re-rankers","title":"Re Rankers","text":"<p>Latency permitting \u2014 use a ReRanker \u2014 Cohere, Sentence Transformers and BGE have decent ones out of the box</p>"},{"location":"writing/rag-best-practices/#embedding","title":"Embedding","text":"<p>Use the right embedding for the right problem: </p> <p>GTE, BGE are best for most support, sales, and FAQ kind of applications. </p> <p>OpenAI is the easiest for Code Embedding to use. </p> <p>e5 family does outside English and Chinese</p> <p>If you can, finetune the embedding to your domain \u2014 takes about 20 minutes on a modern laptop or Colab notebook, improves recall by upto 30-50%</p>"},{"location":"writing/rag-best-practices/#evaluation","title":"Evaluation","text":"<p>Evaluation Driven Development makes your entire \"dev\" iteration much faster. </p> <p>Think of these as the \"running the code to see if it works\"</p> <p>Strongly recommend using Ragas for something like this. They've Langchain and Llama Index integrations too which are great for real world scenarios.</p>"},{"location":"writing/rag-best-practices/#scaling","title":"Scaling","text":""},{"location":"writing/rag-best-practices/#llm-reliability","title":"LLM Reliability","text":"<p>Have a failover LLM for when your primary LLM is down, slow or just not working well. Can you switch to a different LLM in 1 minute or less automatically?</p>"},{"location":"writing/rag-best-practices/#vector-store","title":"Vector Store","text":"<p>When you're hitting latency and throughput limits on the Vector Store, consider using scalar quantization with a dedicated vector store like Qdrant or Weaviate</p> <p>Qdrant also has Binary Quantization which allows you to scale 30-40x with OpenAI Embeddings.</p>"},{"location":"writing/rag-best-practices/#finetuning","title":"Finetuning","text":"<p>LLM: OpenAI GPT3.5 will often be as good as GPT4 with finetuning.</p> <p>Needs about 100 records and you get the 30% latency improvements for free</p> <p>So quite often worth the effort!</p> <p>This extends to OSS LLM models. Can't hurt to \"pretrain\" finetune your Mistral or Zephyr7B for $5</p>"},{"location":"writing/rag-metrics-for-technical-leaders/","title":"RAG Metrics for Technical Leaders: Beyond Recall","text":"<p>Title: MRR, nDCG, Hit Rate, and Recall: Know Your Retrieval Metrics</p> <p>If you're working on RAG, search, or anything that touches vector databases, you've probably run into a mess of evaluation metrics: MRR, nDCG, hit rate, recall. Everyone throws these terms around. Few explain them well. </p> <p>This post is for practitioners who want to go from vague intuition to confident decisions.</p> <p>If you're just starting out and debugging a hallucinating LLM, use Hit Rate. If you're ready to get serious, use MRR + Recall during retriever tuning. If you're ready to get serious, use nDCG + Hit Rate when tuning reranker or doing system evals.</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#tldr-when-to-use-what","title":"TL;DR: When to Use What","text":"Use Case / Need Metric to Use Why You just want to check if any correct result was retrieved Hit Rate Binary success metric, useful for RAG \"was it in the top-k?\" You want to know how many of the correct results were found Recall Focused on completeness \u2014 how much signal did you recover You want to know how early the 1<sup>st</sup> correct result appears MRR Good for single-answer QA and fast-hit UIs You care about ranking quality across all relevant results nDCG Ideal for multi-relevance tasks like document or product search"},{"location":"writing/rag-metrics-for-technical-leaders/#understanding-each-metric-in-detail","title":"Understanding Each Metric in Detail","text":""},{"location":"writing/rag-metrics-for-technical-leaders/#hit-rate","title":"\u2705 Hit Rate","text":"<ul> <li>Binary metric: did any relevant doc show up in top-k?</li> <li>Doesn\u2019t care if it\u2019s Rank 1 or Rank 5, just needs a hit.</li> </ul> <p>Use Hit Rate when: You're debugging RAG. Great for checking if the chunk with the answer even made it through.</p> <p>Think: \"Did we even get one hit in the top-k?\"</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#recall","title":"\u2191 Recall","text":"<ul> <li>Measures what fraction of all relevant documents were retrieved in top-k.</li> <li>Penalizes for missing multiple relevant items.</li> </ul> <p>Use Recall when: You want completeness. Think medical retrieval, financial documents, safety-critical systems.</p> <p>Think: \"Did we find enough of what we needed?\"</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#mrr-mean-reciprocal-rank","title":"\ud83d\udd2e MRR (Mean Reciprocal Rank)","text":"<ul> <li>Tells you how early the first relevant document appears.</li> <li>If the first correct answer is at Rank 1 \u2192 score = 1.0</li> <li>Rank 2 \u2192 score = 0.5; Rank 5 \u2192 score = 0.2</li> </ul> <p>Use MRR when: Only one answer matters (QA, intent classification, slot filling). You care if your system gets it fast.</p> <p>Think: \"Do we hit gold in the first result?\"</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#ndcg-normalized-discounted-cumulative-gain","title":"\ud83d\udd20 nDCG (Normalized Discounted Cumulative Gain)","text":"<ul> <li>Looks at all relevant docs, not just the first.</li> <li>Discounts docs by rank: higher = better.</li> <li>Supports graded relevance (\"highly relevant\" vs \"somewhat relevant\").</li> </ul> <p>Use nDCG when: Ranking quality matters. Ideal for search, recsys, anything with many possible good results.</p> <p>Think: \"Did we rank the good stuff higher overall?\"</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#how-they-differ","title":"How They Differ","text":"Metric Binary or Graded 1<sup>st</sup> Hit Only? Sensitive to Rank? Use For... Hit Rate Binary \u274c No \u274c No (thresholded) RAG debugging, presence check Recall Binary or Graded \u274c No \u274c No Completeness, coverage MRR Binary \u2705 Yes \u2705 Yes Fast hits, QA nDCG Graded \u274c No \u2705 Yes Ranking quality, search"},{"location":"writing/rag-metrics-for-technical-leaders/#retrieval-is-not-one-metric","title":"Retrieval Is Not One Metric","text":"<p>People default to one number because it's convenient. But retrieval is multi-objective:</p> <ul> <li>You want early relevant hits (MRR)</li> <li>You want most relevant hits (Recall)</li> <li>You want them ranked well (nDCG)</li> <li>You want to know if you're even in the game (Hit Rate)</li> </ul> <p>Choose the metric that matches your product surface.</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#pro-tips","title":"Pro Tips","text":"<ul> <li>Use Hit Rate when you're just starting out and debugging a hallucinating LLM</li> </ul> <p>And then use the right metric for the right job: - Use MRR + Recall during retriever tuning - Use nDCG + Hit Rate when tuning reranker or doing system evals</p>"},{"location":"writing/rag-metrics-for-technical-leaders/#final-word","title":"Final Word","text":"<p>MRR isn\u2019t better than nDCG. Recall isn\u2019t cooler than Hit Rate. They just answer different questions.</p> <p>So the next time someone asks, \"What's your retrieval performance?\" You can say: \"Depends. What do you care about?\"</p>"},{"location":"writing/rag-query-types/","title":"5 RAG Query Patterns Every Engineering Leader Should Know","text":"<p>Ever tried building a RAG system that actually works for the all the different ways humans ask questions? After years of building and breaking retrieval systems at scale, I've found that most RAG failures happen at the query understanding level.</p> <p>Here's the thing: not all queries are created equal. The reason your system hallucinates or gives garbage answers often has more to do with the question type than your vector DB settings or chunking strategy.</p> <p>I've distilled RAG queries into 5 distinct patterns, each requiring different handling strategies. Understanding these will save your team months of confusion and help you diagnose issues before they become production nightmares. These are the most common patterns I've seen in RAG systems, but I don't claim they are the only ones.</p>"},{"location":"writing/rag-query-types/#tldr","title":"tl;dr","text":"<ul> <li>Synthesis queries: Straightforward factoid retrieval with light transformation</li> <li>Lookup queries: Require specific information retrieval, often with time/comparative elements</li> <li>Multi-hop queries: Need decomposition into sub-questions for complete answers</li> <li>Insufficient context queries: Questions your system should admit it can't answer</li> <li>Creative/generative queries: Where LLM hallucination is actually desired</li> </ul>"},{"location":"writing/rag-query-types/#1-synthesis-queries-the-rag-sweet-spot","title":"1. Synthesis Queries: The RAG Sweet Spot","text":"<p>Synthesis queries are the bread and butter of RAG systems - straightforward questions requiring basic factual retrieval and minimal transformation.</p> <p>Examples:</p> <ul> <li>\"What were our Q2 earnings?\"</li> <li>\"What's the maximum dosage for Drug X?\"</li> <li>\"When was our healthcare policy updated?\"</li> </ul> <p>\ud83d\udca1 Key insight: Synthesis queries typically map directly to content in your knowledge base, requiring minimal inferencing from the LLM. These are where RAG truly shines.</p> <p>These queries typically follow a predictable pattern:</p> <ul> <li>A clear, singular subject</li> <li>A specific attribute being requested</li> <li>No complex temporal or conditional elements</li> </ul> <p>Engineering implication: For synthesis queries, retrieval precision matters more than recall. Your system needs to find the exact relevant information rather than gathering broadly related context.</p> <p>I built a healthcare RAG system where we optimized specifically for synthesis queries by implementing a document-first chunking strategy. This increased our accuracy by 17% for straightforward factual queries while sacrificing performance on more complex questions - a tradeoff we explicitly made based on user behavior analysis.</p>"},{"location":"writing/rag-query-types/#2-lookup-queries-beyond-simple-facts","title":"2. Lookup Queries: Beyond Simple Facts","text":"<p>Lookup queries introduce additional complexity through comparative elements, time components, or the need to process patterns. These often rely on aggregation over some attributes e.g. time, location and I recommend setting up a metadata index to support these queries.</p> <p>Examples:</p> <ul> <li>\"How did our healthcare costs compare between 2022 and 2023?\"</li> <li>\"What's the trend in side effect reporting for Drug X over the past 5 years?\"</li> <li>\"Show me all dividend-paying stocks that increased yield for 3 consecutive quarters\"</li> </ul> <p>Look for these patterns in lookup queries:</p> <ul> <li>Time-bound components (\"during 2023,\" \"over the past five years\")</li> <li>Comparative elements (\"compared to,\" \"versus\")</li> <li>Trend analysis requirements (\"pattern,\" \"trend,\" \"over time\")</li> </ul> <p>Engineering implication: Lookup queries often require merging information from multiple documents or sources. Your RAG system needs strong reranking capabilities and potentially dedicated retrieval strategies e.g. text2sql and preprocessing the corpus to include tables which can be queried (h/t Dhruv Anand)</p> <p>One approach I've found effective is implementing a two-phase retrieval:</p> <ol> <li>Fetch the core entities and facts</li> <li>Run a separate retrieval for the comparison elements</li> <li>Let the LLM synthesize both retrieved contexts</li> </ol>"},{"location":"writing/rag-query-types/#3-multi-hop-queries-the-reasoning-challenge","title":"3. Multi-hop Queries: The Reasoning Challenge","text":"<p>These are the questions that require breaking down into sub-questions, with each answer feeding into the next retrieval step.</p> <p>Examples:</p> <ul> <li>\"Which of our healthcare plans has the best coverage for the conditions most common among our engineering team?\"</li> <li>\"What investment strategy would have performed best in the sectors where we saw the highest growth last quarter?\"</li> </ul> <p>\ud83d\udca1 Key insight: Multi-hop queries can't be solved with a single retrieval operation. They require decomposition, planning, and sequential execution.</p> <p>Engineering implication: Your system architecture needs to support query planning and multiple retrieval steps. This often means implementing:</p> <ol> <li>A query decomposition module to break complex questions into simpler ones</li> <li>A retrieval orchestrator to manage multiple search operations</li> <li>A synthesis component to integrate findings from multiple retrievals</li> </ol> <p>I remember debugging a financial RAG system that kept hallucinating on multi-hop queries. The root cause wasn't the retrieval system - it was the lack of a decomposition step. We implemented a simple query planning stage that improved accuracy by 32% for complex queries.</p>"},{"location":"writing/rag-query-types/#4-insufficient-context-queries-learning-to-say-i-dont-know","title":"4. Insufficient Context Queries: Learning to Say \"I Don't Know\"","text":"<p>Some questions simply cannot be answered with the information available. The hallmark of a mature RAG system is recognizing these cases.</p> <p>Examples:</p> <ul> <li>\"What will our stock price be next quarter?\"</li> <li>\"Which unreleased drug in our pipeline will have the fewest side effects?\"</li> <li>\"How will changes to healthcare policy affect our costs in 2026?\"</li> </ul> <p>Engineering implication: You need to implement robust confidence scoring and thresholds for when your system should refuse to answer. This requires:</p> <ol> <li>Evaluating retrieval quality (not just semantic similarity)</li> <li>Assessing whether retrieved content actually addresses the query</li> <li>Implementing explicit \"insufficient information\" detection</li> </ol> <p>One technique I've found effective is implementing a self-evaluation prompt after the RAG pipeline generates an answer:</p> <pre><code>Given the original query \"{query}\" and the retrieved context \"{context}\", \nevaluate whether the generated answer \"{answer}\" is:\n1. Fully supported by the retrieved context\n2. Partially supported with some unsupported claims\n3. Largely unsupported by the context\n</code></pre> <p>If the evaluation returns categories 2 or 3, we either refuse to answer or clearly indicate what parts of the response are speculative.</p>"},{"location":"writing/rag-query-types/#5-creativegenerative-queries-when-hallucination-is-a-feature","title":"5. Creative/Generative Queries: When Hallucination is a Feature","text":"<p>Some queries explicitly request creative generation where strict factuality isn't the primary goal.</p> <p>Examples:</p> <ul> <li>\"Draft a blog post about our healthcare benefits program\"</li> <li>\"Generate a sample investor pitch based on our financial performance\"</li> <li>\"Write a description of what our ideal drug delivery mechanism might look like\"</li> </ul> <p>\ud83d\udca1 Key insight: For creative queries, LLM capabilities should be emphasized over retrieval, using the knowledge base as inspiration rather than constraint.</p> <p>Engineering implication: Your system needs to:</p> <ol> <li>Identify when a query is creative rather than factual</li> <li>Adjust the retrieval-generation balance to favor generation</li> <li>Use broader, more diverse retrieval to spark creativity</li> <li>Preferably, implement different evaluation metrics for these queries</li> </ol>"},{"location":"writing/rag-query-types/#practical-implementation-query-type-detection-evals","title":"Practical Implementation: Query Type Detection (Evals)","text":"<p>Don't expect users to tell you what type of query they're asking. Your system needs to detect this automatically. I've implemented a simple but effective query classifier that looks something like this:</p> <pre><code>def classify_rag_query(query: str) -&gt; str:\n    \"\"\"\n    Classifies a query into one of the five RAG query types using Instructor for function calling.\n    \"\"\"\n    from instructor import patch\n    from pydantic import BaseModel, Field\n\n    class QueryClassification(BaseModel):\n        category: str = Field(\n            description=\"The query category\",\n            enum=[\n                \"synthesis\",\n                \"lookup\",\n                \"multi-hop\", \n                \"insufficient_context\",\n                \"creative\"\n            ]\n        )\n        confidence: float = Field(\n            description=\"Confidence score for the classification\",\n            ge=0.0,\n            le=1.0\n        )\n\n    # Patch the LLM to enable structured outputs\n    patched_llm = patch(llm)\n\n    result = patched_llm.chat.predict_model(\n        model=QueryClassification,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"Classify this query: {query}\"\n        }]\n    )\n\n    return result.category\n</code></pre>"},{"location":"writing/rag-query-types/#testing-matrix-for-different-query-types","title":"Testing Matrix for Different Query Types","text":"<p>For effective RAG system evaluation, you need a test suite that covers all five query types:</p> Query Type Evaluation Metrics Synthesis Precision, Answer correctness Lookup F1 score, Completeness Multi-hop Reasoning correctness, Factuality Insufficient context Refusal rate, Hallucination detection Creative Relevance, Creativity metrics"},{"location":"writing/rag-query-types/#think-about-this","title":"Think About This","text":"<p>How often does your team debug RAG issues without first identifying the query type? Most teams I see spend weeks optimizing retrieval parameters when the real problem is a mismatch between query type and system design. </p> <p>Next time your RAG system fails, ask: \"What type of query is this, and is our system designed to handle this specific type?\"</p> <p>Originally published by Nirant Kasliwal, who builds RAG systems that don't embarrass your brand.</p> <p>Thanks to Dhruv Anand and Rajaswa Patil for reading drafts of this.</p>"},{"location":"writing/read-deep-learning-paper/","title":"Read deep learning paper","text":""},{"location":"writing/read-deep-learning-paper/#who-is-this-for","title":"Who is this for?","text":"<p>Practitioners who are looking to level up their game in Deep Learning</p>"},{"location":"writing/read-deep-learning-paper/#why-do-we-need-instructions-on-how-to-read-a-deep-learning-paper","title":"Why Do We Need Instructions on How to Read a Deep Learning Paper?","text":"<p>Quantity: There are more papers than we can humanly read even within our own niche. For instance, consider EMNLP - which is arguably the most popular Natural Language Processing conference selects more than 2K papers across a variety of topics. And NLP is just one area!</p> <p>Some people read academic papers like they read novels: Open link. Read the text. Scroll Down. Finish. Close tab. Some people read like a math book with problems, obsessing over every detail. Their Zotero or hypothes.is accounts are filled with annotations which they are probably never going to revisit in their lifetimes. Others skim but without a coherent structure. All of these are valid ways to read a paper. </p> <p>Here, I am trying to distill and form a better structure for myself to improve the return on my very limited energy.</p>"},{"location":"writing/read-deep-learning-paper/#four-types-of-reading","title":"Four Types of Reading","text":"<ul> <li>In his cult classic book, \"How to Read a Book\", Mortimer J Adler explains his Four Types of Reading, mainly keeping a non-fiction book in mind. I am adapting these to the context of Deep Learning paper for us.</li> </ul>"},{"location":"writing/read-deep-learning-paper/#elementary","title":"Elementary","text":"<p>This is the point where you're when you finish a 101 course in Machine Learning. You know the key terms and vocabulary e.g. convergence, loss functions and optimizers. You can understand what the words in the paper mean and read them, maybe follow the narrative, but not much more. Since you're reading this blog, I assume you are already reading at a level above this.</p>"},{"location":"writing/read-deep-learning-paper/#inspectional","title":"Inspectional","text":"<p>This is basically skimming. You look at the headings, read the beginning and end of some sections, some of the statements in bold. The intent is to get a fast &amp; superficial sense of what the author is trying to say.</p>"},{"location":"writing/read-deep-learning-paper/#intelligently-skimming","title":"Intelligently Skimming","text":"<ul> <li> <p>The first type of inspectional reading is systematic skimming, which you can easily put into practice today. This is most useful when you're reading within a topic you've some familiarity with. For instance, within most topics around intent classification in dialogue systems -- this is how I would read. Here\u2019s how you start:</p> </li> <li> <p>Read the title and abstract.\u00a0This might seem obvious, but authors do put in the effort to compress their key ideas, findings or contributions in these places. This effect is even more amplified since these are the most important fields on arXiv. If you spend a minute of full attention, you should get a feel for the intent and scope of the work. This will not only prime you for what you might be reading next, but also mentally map this work to ideas which you might be already familiar with.</p> </li> <li> <p>Skim the Section Headings, which will give you a feel for organization of the paper. Is the paper emphasizing datasets? A new architecture? Or an empirical work which is basically throwing compute and sharing that \"See, It works!\". I am always a little annoyed when I am discussing a paper when it turns out that the reader has not even got the intent of why the paper sections are organized in a specific way. Obviously, many conferences have specific templates which make it even easier to discover the structure which the authors actually wanted you to pay attention to.  </p> </li> <li> <p>Get a sense of the context\u00a0This means skimming the Related Work section. The intent isn't to read every paper or idea mentioned in this section, but only the topics mentioned here. This will help you get a sense of the jargon used, the variety of topics and what the authors consider adjacent problems/areas. </p> </li> <li> <p>Read the Conclusion.\u00a0Authors generally do a good job summarizing their work in the last few pages. This where they sum up what they think is most important about their work. Just jump to this first.</p> </li> </ul> <p>\ud83d\udca1Pro Tip\ud83d\udca1: Check out their interview, podcast, oral presentation, or Twitter thread or poster.\u00a0While this has nothing to do with the actual paper, these can be a great way to get the gist of a paper in 30 minutes or so. Authors do so much promotion now that it's relatively easy to find interviews. Many selected papers have oral presentations. And of course, they use the best examples from the book in these interviews.</p>"},{"location":"writing/read-deep-learning-paper/#superficial-reading","title":"Superficial Reading","text":"<p>This is most useful when you\u2019re reading outside your usual comfort zone.  Here is the key idea: Read without stopping</p> <p>If you read a lot of papers, you will find that there are some things that you don\u2019t understand. If you stop and try to figure out what it means, it will take a long time to finish the paper. But if you keep on reading, the next thing that happens will help explain what the first thing meant. And so on.</p> <p>You might get very little of what is being said in the first pass and that's fine. You now know the lay of the land, and when you make a second pass -- you can connect the dots much better and faster.</p>"},{"location":"writing/read-deep-learning-paper/#analytical-reading","title":"Analytical Reading","text":"<p>This is where you really dive into a text. You read slowly and closely, you take notes, you look up words or references you don\u2019t understand, and you try to get into the author\u2019s head in order to be able to really get what\u2019s being said.</p> <p>Don't Google Too Early. If there is a math formula, concept, or word you don\u2019t know, first look at the context to try to discern its meaning. See if the author explains what happens when or why they used it. Warm up and use your brain to get started. If it\u2019s something you simply can\u2019t get past, or the word is clearly too important for you to glance over, then check the citations. If even that isn't enough, then finally Google it. The main point is that you can use the tools around you, but don\u2019t lean on them. Let your brain work a little bit before letting Google work for you.</p> <p>Get a sense of the author's background. Look at what institutions do they mention. Are they from academia? An applied AI lab like Apple or GoogleAI? Or an academic lab, sponsored by industry like DeepMind/FAIR? Two examples of how it can inform your reading:</p> <pre><code>    1. There are some companies/labs where a person has to write a certain number of papers every year in order to get promoted (or even retain their jobs) -- they typically have narrow ideas which solve a specific problem incredibly well, but are mostly not adaptable to another domain or context.\n    2. Teams and labs have distinct flavors and sometimes work on specific themes. This can help you quickly get a sense of whether the paper is part of a longer series and see the papers before and after the one you're reading.\n</code></pre>"},{"location":"writing/read-deep-learning-paper/#answer-the-4-key-questions","title":"Answer the 4 Key Questions","text":"<ul> <li>This, Adler says, is actually the key to analytical reading. To be able to answer these questions shows that you have at least some understanding of the paper and what you've read. If you can\u2019t answer them, you probably haven\u2019t quite paid attention well enough. I also find it personally helpful that you should actually write (or type) these answers out. Consider it to be like a book journal. It\u2019ll stay with you and become much more ingrained than if you just answer them in your head. </li> </ul> <p>What is the paper about, as a whole? This is essentially the abstract or conclusion. You could cheat, but that's not going to be very helpful. Instead use your own words and write a the highlights of what you can recall about the paper. See if you can connect it to the wider knowledge base which you've read in the past.</p> <p>What is being said in detail, and how?\u00a0This is where you start to dig a little deeper. Briefly go back and skim through the paper, jogging your memory of the key points, formulae, section headings, graphs and tables with results. With most papers, outlining is pretty straightforward since the section headings do bulk of the job for you. For short papers, this could be as short as 5-10 lines. Pay special attention to what datasets, experiment configurations and ablation results if they're mentioned.</p> <p>Is the paper true - in whole or in part? If you're reading within your own comfort zone, you'll begin to see by now the scenarios/tasks/areas where the paper falls short. For instance, if you're reading up on Long Range Transformers -- based on your knowledge of pre-trained Transformer models like BERT, RoBERTa or T5, you should expect them do better at summarization and Q&amp;A tasks than these. If the paper falls short, you can quickly jot that down as a question to ask, ponder upon or experiment yourself. This is true for both peer-reviewed and pre-print papers - they can often have glaring errors and mistakes which you might notice.  </p> <p>So what? What\u2019s the significance? Most papers are incremental in their contribution to the world. This is not necessarily a bad thing. As long as the paper made you see the field or area in a new light, or even a new nugget of knowledge - it was helpful. We should aim for reading papers which at-least give us something valuable in either perspective, knowledge (e.g. empirical facts) or methods. This is by far the most useful question to ask since it helps contextualize the contributions of the author against your own personal context, understanding and knowledge.</p> <p>The core idea here is that reading is not a passive process. You have to actively engage with the text and think about what you read. It's natural to just scan over the text without actually retaining anything. To counteract this, you need to make a conscious effort to stop and think about what the author is saying. A good way of doing this is to write down a list of questions about what the author says. This will force you to stop and think about the content. When answering these questions, you need to write them in your own words. This means that you can't just parrot the authors words back at them. Instead, you need to rephrase the question in your own words. This will help you engage with the ideas in a more meaningful way.</p> <p>\ud83d\udca1Pro Tip\ud83d\udca1: Generate questions about the content of the reading. For example, convert headings and sub-headings into questions, and then look for answers in the content of the text. Other more general questions may also be formulated:    - What is this paper about?    - What question is this section trying to answer?    - How does this information help me?</p> <p>Optional, critique and share your thoughts with others.\u00a0This step is dead last. Only after having read the entire paper, or sets of papers, and thoughtfully answering the 4 Key Questions can you critique or have meaningful discussions about the paper.     - For starters, a reasonable critique asks more questions (\"Did they assume X?\", \"What would happen if I replaced method M1 with M3?\") than it makes verdicts (\"The paper is amazing\", \"This is stupid\").     - The second point is to fight the temptation to write a paper summary instead of a critique. That's the fad these days but isn't quite meaningful enough. You writing a measly, annoying Medium blog might feel like an achievement to yourself (thanks to cheap applause) but doesn't improve your understanding as much as writing a critique will.             - Also, you don't have to take everything that the authors claim as God's Gospel. Having a disagreement is completely fine and valid. But give them the benefit of doubt and ask questions instead of making assumptions. It goes without saying, that you don't have to disagree or agree with every part of the paper. You can freely completely love one part and ignore the rest. There is no need (or advantage) to have an opinion about everything.</p> <p>\ud83d\udca1Pro Tip\ud83d\udca1: You can use the Question Generation idea even during \"Intelligently Skimming\", especially for topics where you're comfortable. This will save you a lot of time and energy during Analytical Reading.</p> <p>Syntopical \u2014 This is mostly used by researchers and professors. It\u2019s where you read multiple papers on a single subject and form a thesis or original thought by comparing and contrasting various other authors\u2019 thoughts. This is time and research intensive, and it\u2019s not likely that you\u2019ll do this type of reading very much, unless your day job is paying you to read and write papers. I do not have the relevant expertise to help you with this. </p>"},{"location":"writing/read-deep-learning-paper/#to-quickly-recap","title":"To quickly recap:","text":"<ul> <li>Use Inspectional Reading when you're first reading a paper</li> <li>Use Analytics Reading and Answer the 4 questions when you're looking to get a deeper, better gist of the paper</li> </ul>"},{"location":"writing/read-deep-learning-paper/#four-questions-you-should-be-able-to-answer","title":"Four Questions You Should Be Able to Answer","text":"<ul> <li>What is this book about?</li> <li>What is being said in detail, and how?</li> <li>Is this paper true in whole or in part?</li> <li>So what?</li> </ul>"},{"location":"writing/read-deep-learning-paper/#resources","title":"Resources","text":"<ul> <li>Art of Manliness</li> <li>Farnam Street</li> </ul> <p>Thanks for Gokula Krishnan and Pratik Bhavasar for reviewing early versions of this. </p>"},{"location":"writing/recession-proofing/","title":"Recession Proofing My Career","text":"<p>I\u2019d written originally this in August 2019 with a bit of foresight of a pending recession.</p> <p>I wrote this primarily for myself. Multiple reviewers have mentioned that they found it immensely useful, so sharing it with you here!</p> <p>Unfortunately, I didn\u2019t follow some of the important notes here myself. Hope you do better than me!!!</p>"},{"location":"writing/recession-proofing/#hard-assumptions","title":"Hard Assumptions","text":"<ul> <li> <p>Recession will happen in the next 6-24 months</p> </li> <li> <p>Will hurt technology startup professionals which depend on VC money directly as pay cheques</p> </li> </ul>"},{"location":"writing/recession-proofing/#best-suited","title":"Best Suited","text":"<ul> <li> <p>You work in a tech career e.g. engineers, product managers, designers</p> </li> <li> <p>Are 2-7 years out of college, probably don\u2019t have kids</p> </li> <li> <p>Living in Indian Tier 1 or 2 city like Bangalore, Delhi, Bombay, Jaipur or Chennai</p> </li> </ul>"},{"location":"writing/recession-proofing/#prerequisite-mindset","title":"Prerequisite Mindset","text":"<ul> <li> <p>You can do this.  And you do not need to rely on some CA or a relative or your father</p> </li> <li> <p>Do your homework. Kill your gut instinct with mounds of calculations before deciding</p> </li> <li> <p>You are a fracking adult. Overcome your fear and learn a bit of Excel magic</p> </li> <li> <p>Don\u2019t expect early wins. Early win is you not being homeless </p> </li> </ul> <p>Three to Takeaway</p>"},{"location":"writing/recession-proofing/#30-40-50-rule","title":"30-40-50 rule","text":"<ul> <li> <p>Pay less than 30% of your income in rent and recurring expenses</p> </li> <li> <p>Spend less than 40% of your income in all expenses, other than rent and healthcare</p> </li> <li> <p>Save 50% or more of your post-tax income in long term investments</p> </li> </ul>"},{"location":"writing/recession-proofing/#spending","title":"Spending","text":"<ul> <li> <p>Get to Default Alive is good advice for people and startups</p> </li> <li> <p>Pay off any personal debt like credit card debt</p> </li> <li> <p>Spend on \u201cowned\u201d entertainment i.e. downloaded movies or music which you can see &gt;&gt; Netflix/Spotify</p> </li> <li> <p>If you have kids, pay off their school/tutoring fees upfront and negotiate a discount</p> </li> <li> <p>If you have high living expenses e.g. clothes, dining out, gauche groceries - please stop ordering free-delivery-but-expensive food from Swiggy/Zomato!</p> </li> <li> <p>Read blogs like FIRE focused blogs like Money Mustache. They\u2019ve been prepping for a recession since 2017</p> </li> </ul>"},{"location":"writing/recession-proofing/#money-box","title":"Money Box","text":""},{"location":"writing/recession-proofing/#health-family","title":"Health &amp; Family","text":"<ul> <li> <p>Buy health insurance, life insurance - term insurance. Never ULIPs</p> </li> <li> <p>Buy accident and personal insurance please.</p> </li> <li> <p>Get your health check up done regularly - prevention is cheaper than curing</p> </li> <li> <p>If you\u2019re delaying \u201cgoing to the gym\u201d or therapy - do it now. You might not be able to afford it later. Buy your Gym/Fitness class subscription long term</p> </li> </ul>"},{"location":"writing/recession-proofing/#invest","title":"Invest","text":"<ul> <li> <p>At least 50% of your Income for long term</p> </li> <li> <p>Keep 6 months of your expense rate, in a Liquid Fund. Not cash. </p> </li> <li> <p>Cash in your savings account is bad. FD/RDs are worse - you're losing money to inflation and taxes</p> </li> <li> <p>If you are in the 30% tax bracket, you need to invest in an FD offering at least 10% to get a positive inflation adjusted return. </p> <ul> <li> <p>No FD gives a 10% return, you just lose 2-4% every year.</p> </li> <li> <p>Don't try to time the dip - that is how at least 2 lakh people lost money in India</p> </li> <li> <p>Even God cannot beat DCA i.e. SIPs win over lump sum, always</p> </li> <li> <p>Automate and schedule everything you can - your savings via SIPs</p> </li> </ul> </li> </ul>"},{"location":"writing/recession-proofing/#mutual-funds","title":"Mutual Funds","text":"<ul> <li> <p>Beware of the fees - invest in direct MFs (NOT Regular) via a discount brokerage like Zerodha</p> </li> <li> <p>Opinion: Exchange Traded Funds &gt; Direct MFs</p> </li> <li> <p>Gold ETFs &gt; Physical Gold; Gold can be a golden investment in a recession</p> </li> <li> <p>Understand terms like debt, liquid, equity and duration like ultra short to long term</p> </li> </ul>"},{"location":"writing/recession-proofing/#money-box-bonuses","title":"Money Box: Bonuses","text":"<ul> <li> <p>Make sure your spouse, parents and grandparents don\u2019t prey fall to opportunistic vultures. Or are doing something egregious like investing in regular MFs, trying to time the DIP. Please get over yourself and talk money with them. </p> </li> <li> <p>If you need a safety net, they need too. Don\u2019t let them accidentally destroy their safety nets.</p> </li> </ul>"},{"location":"writing/recession-proofing/#career-safety-net","title":"Career Safety Net","text":"<ul> <li> <p>Read your employment contract for details e.g. </p> <ul> <li> <p>How much notice period do you have? </p> </li> <li> <p>If you are fired on the spot, can the company delay salary?</p> </li> <li> <p>Do you have a non-compete? </p> </li> <li> <p>How were the people who left recently treated?</p> </li> <li> <p>Reminder: Your ESOPs are not worth anything</p> </li> </ul> </li> </ul>"},{"location":"writing/recession-proofing/#build-some-network-capital","title":"Build some Network Capital","text":"<ul> <li> <p>Keep in touch, Deepen your connect, reach out. Start meeting folks outside your usual network</p> </li> <li> <p>Think of how you can act like a consultant. Side money and wider network</p> </li> </ul>"},{"location":"writing/recession-proofing/#build-some-career-capital","title":"Build some Career Capital","text":"<ul> <li> <p>Brand yourself as an authority figure in some areas. Compulsory Reading: Do not end the week with nothing</p> </li> <li> <p>Understand what happens if your job is automated en masse? </p> </li> </ul>"},{"location":"writing/recession-proofing/#in-a-recession-you-have-3-full-time-jobs","title":"In a recession you have 3 full time jobs:","text":"<ul> <li>Your day job, managing your finances unemotionally, figuring out where to go</li> </ul>"},{"location":"writing/recession-proofing/#want-more-paranoid","title":"Want more? Paranoid?","text":"<ul> <li> <p>Figure out your monthly expense, instead of 6x\u2019ing it, 12x it</p> </li> <li> <p>Pay off your house rent in advance, negotiate to get a discount that way</p> </li> <li> <p>Meditate. Lose your mind, lose your money. Maybe begin with Headspace or Calm.</p> </li> <li> <p>Keep this split across at least 2 banks in India - 1 private like ICICI, one PSU like SBI, keep about 2 month runway in cash. To avoid YesBank situations.</p> </li> <li> <p>Get a bank account without a debit card, and only cheque option to withdraw </p> </li> </ul>"},{"location":"writing/recession-proofing/#off-the-record","title":"Off the Record","text":"<p>Sometimes the best move is to not play the game at all.</p> <p>For example, getting out of the country, even within the same country: moving from Bengaluru to Varkala can be a huge income saver for most techies, </p> <p>You can also pause the \u201csociety\u201d game to win: go into monk mode instead of going after girls, delay your arrange wedding, if married - delay kids by a year or two, etc.</p> <p>Special thanks to Gokul Krishnan for suggesting an entire section, several insights and a very detailed review. Thanks Meghana Bhange for positive encouragement and feel good vibes!</p>"},{"location":"writing/reflectedglory/","title":"Don't Bask in Reflected Glory","text":"<p>Irshad Kamil</p> <p>\u092f\u0947 \u091c\u094b \u0932\u094b\u0917-\u092c\u093e\u0917 \u0939\u0948\u0902, \u091c\u0902\u0917\u0932 \u0915\u0940 \u0906\u0917 \u0939\u0948\u0902  Ye jo log baag hain, Jungle ki aag hain The people, are jungle fire</p> <p>\u0915\u094d\u092f\u0942\u0901 \u0906\u0917 \u092e\u0947\u0902 \u091c\u0932\u0942\u0901... Kyun aag mein jalun Why should I burn in the fire?</p> <p>\u092f\u0947 \u0928\u093e\u0915\u093e\u092e \u092a\u094d\u092f\u093e\u0930 \u092e\u0947\u0902, \u0916\u0941\u0936 \u0939\u0948\u0902 \u092f\u0947 \u0939\u093e\u0930 \u092e\u0947\u0902 Ye nakaam pyaar mein, Khush hai yeh haar mein Defeated in love, they're happy in defeat</p> <p>\u0907\u0928 \u091c\u0948\u0938\u093e \u0915\u094d\u092f\u0942\u0901 \u092c\u0928\u0942\u0901... Inn jaisa kyun banun Why should I be like them?</p>"},{"location":"writing/reflectedglory/#identity","title":"Identity","text":"<p>I am here to tell you how to keep your identity small.  I'll share what I think is the bare minimum context you need to have:</p>"},{"location":"writing/reflectedglory/#why-should-i-keep-my-identity-small","title":"Why should I keep my Identity small?","text":"<p>Paul Graham explains how keeping your identity small can be a competitive advantage:</p> <p>The most intriguing thing about this theory, if it's right, is that it explains not merely which kinds of discussions to avoid, but how to have better ideas. If people can't think clearly about anything that has become part of their identity, then all other things being equal, the best plan is to let as few things into your identity as possible.</p> <ol> <li> <p>If you know what is part of someone\u2019s identity, you can avoid inflammatory discussions. This can be as simple as avoiding discussions about which shoe brand is the best?</p> </li> <li> <p>The bigger advantage is this: Better ideas. The ability to think with lucidity, without biases is an unfair advantage. If you act on rational decisions without attacking someone's identity, it's an even bigger advantage which few can compete with.</p> </li> </ol>"},{"location":"writing/reflectedglory/#what-do-you-mean-by-identity","title":"What do you mean by Identity?","text":"<p>Identity, in the crudest sense are the words (and ideas) that you use to describe yourself - in your head.  For instance, while you might be someone who build rocket ships, your identity might be \"I am a <code>family man</code>\". Or \u201cI am a <code>craftsman engineer</code> making ships for space travel\u201d</p> <p>Here is a short mental exercise. Fill this blank with at least 3 small phrases:</p> <p>Your Identity</p> <p>I am ___</p> <p>Done?</p> <p>Awesome.  Let's carry on: </p> <p>Here is what I came up with a few years ago: </p> <p>My Identity</p> <p>I am {Indian, guy, techie}</p> <p>The labels in <code>{}</code> are (in the very crude sense) - my identity. Notice that these labels do not always have to be professional, personal or even Truth in the strictest sense. </p> <p>They don't have to align with reality in any meaningful way. They simply describe how you perceive yourself. </p> <p>Want another example? Here is what a friend filled: </p> <p>My Identity</p> <p>I am {smart, get things done, make no excuses}</p> <p>If you have not filled in your three, please pause - take a quick second and type it down somewhere. I'll wait. </p> <p>Cool, I hope you have your three written or typed somewhere accessible. Let's carry on. We'll come back to it, I promise. </p>"},{"location":"writing/reflectedglory/#dont-bask-in-reflected-glory","title":"Don't Bask in Reflected Glory","text":"<p>That's it. That is the only thing I want you to do and you will be on your way to learning how do I avoid these labels?</p> <p>Reflected Glory is the invisible demon which will lead you to the butcher, chop you up into pieces, then slow barbecue you in open air. All this while you and the Demon, both laugh and make merry. </p> <p>Reflected glory is how Satan wins over honest, smart and hard working people. It is insidious. Sinister. Subtle. </p>"},{"location":"writing/reflectedglory/#in-professional-circles","title":"In Professional Circles","text":"<p>Even in certain careers, labeling yourself can be a career limiting move:</p> <p>Don't Call Yourself a Programmer</p> <p>If you call yourself a programmer, someone is already working on a way to get you fired - from Don't Call Yourself a Programmer by @patio11</p> <p>And why are otherwise honest people, so deeply tempted to label themselves professionally?</p> <p>The best people in most professions are incredibly talented and (often enough) make good money. The professional label is aspirational. </p> <p>The worse off are your professional skills, the more you need to riff on the shared professional identity of amazing folk in your profession. </p> <p>Here is an important nuance which I constantly remind myself of: </p> <p>Stop calling yourself a programmer</p> <p>Don't bask in Reflected Glory</p> <p>Keep your identity small</p>"},{"location":"writing/reflectedglory/#in-politics","title":"In Politics","text":"<p>Politician's Trick</p> <p>\"People like us ... \" - wait, go deeper and think. Your mental dialog will begin: \"What does he mean by us? I am nobody. Why does a nobody matter? Is he using my emotions as a stepping stone to get where he wants to go?\"</p> <p>The politician is tricking you into filling in the gaps he left. What do you will fill this with? What makes best sense for you, from your own identity. </p> <p>He is hacking your identity to make most profit for him.  Don't waste your energy. When making rich people richer, get something in return. </p> <p>But why does this trope work in the first place? </p> <p>Well, because somewhere in your identity - you allowed words like Indian, American, Patriot or Nationalist to creep in. </p> <p>Why did you allow them in the first place? Because there are actually positive connotations of being a part of the world's largest and most powerful democracy respectively. </p> <p>Stop </p> <p>You walked into my trap.</p> <p>You basked in the reflected glory of your ancestors. </p> <p>Both America and India are countries born of blood, sweat and tears. Not enough of which is yours.  You haven't done anything meaningfully large to contribute to democracy. </p> <p>You are basking in the reflected glory of your dead ancestors</p> <p>Be grateful. Use them as your ideals. </p> <p>For the sake of good that is oft interred with bones, don't bask in glory of your dead ancestors </p> <p>Do the work that when you meet your ideals, they can be proud</p> <p>Do the work such that you can be proud</p> <p>Don't bask in reflected glory</p> <p>Keep your identity small</p>"},{"location":"writing/reflectedglory/#in-casual-conversation","title":"In Casual Conversation","text":"<p>Bragging</p> <p>\"Look at Michael, an Olympic athlete, I knew him in college\" - wait, and think.  You might end up going this way: \"Why is that relevant in this conversation? Is this an offer to make an intro to the Olympian, so that I improve my game? Or is this just banter-bragging? And why brag?\"</p> <p>Brag about achievements of someone you know. Or worked with. Or went to school with. </p> <p>Everyone around you looks at you for a brief moment of envy, or pride if they love you. </p> <p>They could have been proud of something you did. And instead they're proud that you know people who do things. You are basking in the reflected glory of others.</p> <p>The first time I internalized this perspective, I was deeply embarrassed. </p> <p>Given that you've read this piece so far, I hope something for you. </p> <p>I hope that when you go to sleep tonight - you will sleep with the resolution to not bask in reflected glory of your friends and family. </p> <p>No matter how much you love them. You will be proud of them. And you will not bask in their glory. Don't bask in their reflected glory. </p> <p>It's their spotlight and you will not even a light a candle to steal that</p>"},{"location":"writing/reflectedglory/#your-three","title":"Your Three","text":"<p>Remember the three blanks we filled? Those are the your borrowed labels. No matter how innocent, they cloud your judgment. </p> <p>Those are the ideas - and people whose reflected glory you are stealing unknowingly</p> <p>Even the <code>Get things done</code> is basking under the reflected glory of people who actually get things done. It clutters your thinking by making you swing for the extremes. By not letting you slow down.</p> <p>It might be quite some time before my friend sees that it's about the getting the right things done. </p> <p>That we are human beings, not human doings. </p> <p>The moon waxes and wanes because it basks in reflected glory - but even the darkest clouds cannot hide the glory of sun</p> <p>Borrowed Glory</p> <p>When your contributions are meaningful enough, you don't need borrowed glory. The sun shines it's own light, it's the moon that steals light</p>"},{"location":"writing/ruleof40/","title":"Thinking of Your Career as a Startup","text":"<p>Broadly speaking, it\u2019s useful to think of your career as a startup.</p> <p>Your profit+growth percentage should be a minimum 40%</p> <p>So say, you join a big company like Microsoft and your skills grow at 20% every year, while your pay only grows 10% - that's bad. </p> <p>You need to grow your skills and outcomes faster.</p> <p>The other extreme is also fine: You join a small startup which can make great use of entrepreneurial DNA plus specific skills.</p> <p>Your pay jump is 30%, and your skills grow only 10% - that is fine too</p> <p>Your pay is profit. Growth is demonstrable skills growth.</p> <p>Think of each career move as a Merger and Acquisition: Write a detailed document with the best reasons for making the switch.</p> <p>Make the best argument possible. And then, red team it. Shred it to pieces.</p> <p>I've done years where I grew by ~50% because of 15% pay jump and ~30% skills jumps - but doing it consistently is what matters. I\u2019ve never done a 2-year streak of 40%+ growth in my career.</p> <p>Most college graduates that I know - earn double of their starting pay at 30 years old. Which is effectively the growth rate of a fixed deposit. Fixed deposits are among the worst asset classes to invest in India.</p> <ul> <li>Their gains come from savings and investments and the magic of compounding, not higher income alone</li> <li>Your starting income should be high, or restart your career in some way e.g. moving to a new country</li> </ul> <p>Charge more. Grow more</p>"},{"location":"writing/ruleof40/#assigning-a-or-value-to-your-skills","title":"Assigning a % or $ value to your skills","text":"<p>The most common criticism of this approach is this: \u201cI don\u2019t know how to measure my skill growth - it\u2019s intangible\u201d</p> <p>Saying something is intangible is a way of saying it has large error margins, it still has a minimum, median and a maximum.</p> <p>Figure out all three, you don't need precision - you need worst case scenarios.</p> <p>Look at any company's balance sheet: They've a \"brand value\" intangible and they assign a $$ value to it.</p> <p>Millions of people pay millions of dollars for the Nike, Apple, Sony brand every year - it's a not a miracle.</p> <p>The $$ values don't have to be precise. People always get lost in details.</p> <p>I just ask them for the minimum, maximum and median - and people are like --- ohh, yeah, that makes sense - we can guess these with some work.</p> <p>Do the work. Charge more. Grow more.</p> <p>Mummy ka Raja beta, Nirant</p> <p>Originally appeared at my newsletter</p>"},{"location":"writing/sankar-wlb/","title":"The Case Against Work Life Balance","text":"<p>Note from Nirant: This is an archived blog post from here by Syam Sankar. I am not the author of this post. I'm archiving it here since the original source is no longer available.</p> <p>I've taken the liberty to copy paste the raw text:</p> <p>Given my journey, you can imagine my first reaction to questions of work-life balance is fairly unsympathetic. I want to protest that, by legitimizing such a false dichotomy, you\u2019re pre-empting a much more meaningful conversation. But I suspect that conversation is closer to the heart of this anxiety than most people realize. </p> <p>If you\u2019re worrying about work-life balance at the beginning of your career, and you\u2019re reading this, I\u2019m guessing you\u2019re not lazy. You\u2019re not looking for an easy life (even if this seems like an appealing concept right after midterms). I\u2019m willing to bet that what you\u2019re really worried about is someone else owning your most precious possession: your future. </p> <p>Staring into the abyss of companies that glorify triple-digit hours (never mind the substance of the work), this makes intuitive sense. But having surveyed the landscape of high-tech hiring, I\u2019m convinced you should be just as concerned about jobs that promise high stimulation and total comfort. When you let yourself be sold on easy hours, outrageous perks, and glib assurances about the project you\u2019ll join and the technologies you\u2019ll get to play with, you\u2019ve just agreed to let your future become someone else\u2019s.</p> <p>I hate the construct of work-life balance for the same reason I love engineering: the reality is dynamic and generative, not zero-sum. It\u2019s about transcending the constraints of simplistic calculations. Creating the life and the work you want are by no means easy challenges, but they are absolutely attainable. What\u2019s not realistic is thinking you can own your future and be comfortable at the same time. Grit, not virtuosity, will be the biggest determinant of your success, for reasons I\u2019ll explore in a bit. </p> <p>At the same time, grit and discipline aren\u2019t enough. You need purpose. And I can state categorically that the purpose you discover, with all the sacrifice that entails, will be more motivating and meaningful than the one handed to you in the form of some glamorous project that, realistically, will succeed or fail regardless of your involvement. </p> <p>The catch, of course, is that true purpose doesn\u2019t sit around waiting to be discovered. It requires constant pursuit. Here\u2019s what I\u2019ve learned from a decade and a half of sprinting.</p> <p>There\u2019s no time like now. As learning animals, we\u2019re subject to various ages of cognitive potency. As a young child, your aptitude for acquiring a language or learning an instrument is at its peak. Accordingly, as a professional, your early 20s are the most formative stage. It is absolutely critical to make the most of this time because the pace of learning grows slower and more incremental as you age, whether we care to admit it or not. Of course, you can always learn new things, but most often the wisdom of experience is largely the result of earlier realizations having the time to compound into something richer.</p> <p>The place of maximal learning is often at the point of significant pain. It\u2019s not just about having a more pliable mind - grit, and its close cousin, resilience, are essential for taking your intelligence further than it can get on its own. And while intelligence compounds, grit degrades in the vast majority of cases. Regardless, grit isn\u2019t something you can suddenly develop after a life of leisure. For these reasons, owning your future means choosing grit over the allure of a predictable pace.</p> <p>Of course, you still need to hold a pace. Studies show that marathoners/endurance runners do tons of self-talk to push past the pain. \u201cIt\u2019s a marathon, not a sprint\u201d is a well-worn clich\u00e9, but it\u2019s striking how often it\u2019s invoked to rationalize comfort as opposed to promoting sustained excellence. Don\u2019t think for a second that elite marathoners have trained to the point that a sub-six-minute mile pace is comfortable. It\u2019s incredibly painful. What separates the truly elite is having found a purpose that makes the sacrifice acceptable.</p> <p>At the same time, complete self-motivation is incredibly rare. It\u2019s probably not a realistic goal, and that\u2019s fine. Find the people who will sharpen your resolve as well as your ideas. Again, your first step matters. If you choose a job for work-life balance, chances are, so did everyone who came before. Talent is one thing when evaluating your future teammates, but ask yourself this: when you need models and inspiration to be more than you are, will you be able to find them?  Where will your gamma radiation come from?</p> <p>You can find your zen in stressful, chaotic times. In fact, I\u2019d argue this is the norm, even the ideal, for 20-somethings. Some adrenaline is good for your performance. Not having time to waste requires you to focus on the essentials and develop an innate sense of direction. That way, when you do eventually get to let your mind wander, it will be in rewarding directions. These days, I build in calendar blocks for \u201cbrain space\u201d. That wouldn\u2019t have made sense 10 or even 5 years ago \u2013 not because I have more free time now, but because, early in your career, you learn much more by doing than reflecting. And this can be the difference between creating your future and receiving it in a fancy envelope. </p> <p>At the limit, you probably should care about work-life balance \u2013 it\u2019s not going to remain a static thing your whole life. But at the margin, as a new grad, you should focus on the most important problem. Find the thing that motivates you, work your ass off, learn as much as you can, and trust that today\u2019s gains will compound well into the future \u2013 your future.</p> <p>Working your ass off isn\u2019t bleak \u2013 it\u2019s quite the opposite. Provided there\u2019s a purpose, sprinting at an unsustainable pace is an act of tremendous optimism. A mindset of premature retirement might sound rosy, but in truth it\u2019s deeply cynical and extraordinarily insidious \u2013 much more so than being overpaid or overpraised, and much harder to correct.</p> <p>But back to the concept of caring about work-life balance at the limit, how do you know where the limit is? Isn\u2019t life fundamentally uncertain? Here\u2019s what I\u2019ve come to realize: you can\u2019t pre-emptively retire without doing the work that makes you appreciate the chance to rest. Maybe you can, but assuming you have something to contribute, it\u2019s going to be an empty reward. Sacrificing your potential to comfort isn\u2019t a hedge against an early death \u2013 it IS an early death. As Emerson wrote in Self-Reliance, \"Life only avails, not the having lived. Power ceases in the instant of repose; it resides in the moment of transition from a past to a new state, in the shooting of the gulf, in the darting to an aim.\u201d </p> <p>We\u2019ve been told over and over to choose life over work in order to achieve balance. I\u2019m urging you, especially at the dawn of your career, to  instead choose life over balance, and make the work so meaningful that you wouldn\u2019t want it to exist as a distinct concept. This is how you ensure that your future remains yours.</p>","tags":["tech"]},{"location":"writing/shreyas/","title":"Shreyas Karanth: Moving from SW Engg to Product","text":"<p>Shreyas Karanth is a Product Manager at Soroco. Soroco is a Medium-sized Enterprise-focussed B2B Automation company. Before Product Management, Shreyas was the fastest growing engineer - going from a fresh grad to a Technical Lead within 3 years of graduation.</p> <p>This is also when I worked with him, during his engineering days. Here, I asked a bunch of questions why he moved to Product Management, what excites him and what he considers as Product Management work at Soroco.</p>"},{"location":"writing/shreyas/#background","title":"Background","text":"<p>Nirant: How many people work at Soroco now?</p> <p>Shreyas: Actually, closer to 250 I think, with around 170+ people in engineering</p> <p>Nirant: And how many people can you distribute work to?</p> <p>Shreyas: PM work? No one. There are 7-8 engineers in my team though.</p> <p>Nirant: What is PM work here?</p> <p>Shreyas: Customer demos, sales collateral, market research, sprint planning, end user facing documentation.</p> <p>Nirant: Demos and collaterals should be with the functional team, right? Also, what product do you PM - if I may ask?</p> <p>Shreyas: What do you mean by the functional team here? And I was PMing an internal product, where we had built a single interface to monitor and control automation systems and their dependencies.</p> <p>It was originally slated to be a companion to our automation systems, but we started selling it as a standalone product by itself, a few months ago.</p> <p>Nirant: Functional team - e.g. sales collateral comes from Sales team, docs from tech, market research from sales/business development.</p> <p>Shreyas: And before that, I worked for 3 months as a PM to build a demo version of one of our products. It was basically our enterprise offering with stripped down features, deployed as an Electron app, so that customers can download it on their machines and test out the tool quickly.</p> <p>So, Sales doesn't make most of their own collateral. In Soroco, Marketing + Product makes it for them. Rather, we create a library of slides/one-pagers. It's up to Sales to choose the right ones from this library for each customer. And to request for more when it's needed -- because Marketing owns the customer-facing message.</p> <p>Nirant: What are the advantages of this vs Sales owning it outright?</p> <p>Shreyas: Tight control over messaging by centralizing it. Also, easier to maintain quality of collateral.  Enterprise sales people don't really have the best aesthetic sense.</p> <p>Nirant: Centralized to who? Marketing?</p> <p>Shreyas: Yes.</p>"},{"location":"writing/shreyas/#what-do-you-love-the-most","title":"What do you love the most?","text":"<p>Nirant: What have you loved the most, in transitioning from tech to product?</p> <p>Shreyas: </p> <ol> <li> <p>90% of the articles I read when I was in tech, were \"product\" articles. What has been great is actually putting these things (that I'd read over the course of 2 years) into practice. Even small things, like coming up with a set of questions to ask customers, require me to go through articles and books that I'd bookmarked over the years, and then actually using those tips.</p> </li> <li> <p>Coming up with hypotheses around why certain features need to be built is also enjoyable to me. Putting together what I've learnt from the articles I've read, as well as what I have understood from sales pitches and customer interviews to come up with a view of what to do has been pretty fun.</p> </li> <li> <p>Here, PMs give a lot of customer demos, essentially doubling up as sales people on some calls. This is one thing, which I've unexpectedly come to enjoy (If you hadn't noticed already, I don't really talk much).</p> </li> <li> <p>Understanding all the other details that go into actually releasing a product in the market has been fun. Some of the things I've really enjoyed are: understanding how to price your product, figuring out whether you should go all-in on SaaS or keep an on-prem option, market research, competitor analysis and even fine-tuning sales pitches. Basically, I get to sample (almost) everything that goes into building a product.</p> </li> </ol>"},{"location":"writing/shreyas/#what-has-been-the-hardest","title":"What has been the hardest?","text":"<p>Shreyas: </p> <ol> <li> <p>Preventing myself from getting into the technical details has been pretty hard. I've come to look at it this way: If things break, I'm not going to be the one who is going to dive in and fix it; that'll be the engineers. So, I should stop getting too involved here. (credit to @Pentropy for this advice)</p> </li> <li> <p>Writing in a way that non-technical folks can understand you has been challenging. (However, I think you will have no problem here, based on what I've seen of your writing from when you were in Soroco)</p> </li> <li> <p>The amount of organizational skills required to be a PM is pretty crazy. I thought of myself as a fairly organized person, but I've found myself coming up short after I became a PM. You need to deal with engineering, sales, marketing and also leadership. And you have only so many hours in the day.</p> </li> <li> <p>An addendum to the above point is that you will have to inevitably deprioritize some things owing to lack of bandwidth. Some people are bound to be unhappy with the priorities that you've chosen and you have to handle that well, since you may be asking them to prioritize something you need the very next week.</p> </li> <li> <p>Accountability for things that you can't directly control. You become the scapegoat, in some way, for problems that are hard for you to control. If the tech team runs into an unexpected issue and that delays a release, that's your problem. If the sales team hasn't gotten a good enough response from customers for your product, that's your problem. If the marketing team has been too busy with other things to prepare collateral for you, that's still your problem.</p> </li> <li> <p>Your responsibilities are very fluid. As a tech guy, on every project/product I worked on, I knew my main set of responsibilities very clearly. I used to help out other people with their responsibilities, but I would make sure that I fulfilled my own first. As a PM, when there's something that needs to be done, and you can't clearly call it the engineering, marketing or sales team's responsibility, it just becomes your responsibility. Sometimes, even when it is another team's responsibility, but they are busy and you need to get it done, you need to just suck it up and do it (like creating marketing collateral).</p> </li> <li> <p>(Bonus) Oh, I forgot #7 for what's been harder - the feedback loop on my work has become much longer. Code gave me instant gratification, but I am not used to deriving satisfaction from making a market research deck or a specs doc yet. Also, I am not sure whether a lot of the decisions that I have taken were the right ones.</p> </li> </ol>"},{"location":"writing/shreyas/#what-does-your-typical-day-look-like","title":"What does your typical day look like?","text":"<p>Shreyas:</p> <ol> <li> <p>Standup in the morning</p> </li> <li> <p>Calls with engineering to clarify features that I've written specs for</p> </li> <li> <p>Calls with design to brainstorm UI/UX design for features</p> </li> <li> <p>Some ad-hoc work for client engagement, which includes, but is not limited to</p> <ul> <li> <p>Help with proposals</p> </li> <li> <p>Input and slides for client demos</p> </li> <li> <p>Understanding and prioritizing high priority bugs in the product</p> </li> <li> <p>Calls to understand asks from customers</p> </li> </ul> </li> <li> <p>Prep for client demos (if any)</p> </li> <li> <p>Prep for customer feedback sessions (if any)</p> </li> <li> <p>Write down specs and hypotheses for features in the next sprint</p> </li> <li> <p>Work on fine-tuning:</p> <ul> <li> <p>Customer-facing roadmap</p> </li> <li> <p>Slides for sales pitches</p> </li> </ul> </li> <li> <p>Read about competitors</p> </li> <li> <p>Read about the market, in general</p> </li> <li> <p>Client demos and customer feedback sessions (if any)</p> </li> </ol> <p>All of the above won't happen in one day. But you can consider this as a sum of what might happen over the course of a typical week.</p> <p>I may have missed out on a few things here, but I think I got the gist right. Discussions around pricing, product vision etc. happen much more rarely, so I can't really consider that as part of a typical week.</p> <p>Nirant: Aaah. That is a lot of variety. Thanks a ton! This is next-level detail. And thank you for taking the time to type it out! I'll be able to revisit this because of that!</p> <p>If you loved this writing, tweet to @shreyas94</p>"},{"location":"writing/silentriseofpytorch/","title":"The Silent Rise of PyTorch Ecosystem","text":"<p>While Tensorflow has made peace with Keras as it\u2019s high level API and mxNet now support Gluon \u2014 PyTorch is the bare matrix love.</p> <p>PyTorch has seen rapid adoption in academia and all the industrial labs that I have spoken to as well. One of the reasons people (specially engineers doing experiments) like PyTorch is the ease of debugging.</p> <p>What I don\u2019t like about PyTorch is it\u2019s incessant requirement of debugging because of inconsistent dimensions problems. In fact, one of the most recommended speed hacks for faster development: assert tensor shapes!</p> <p>This is something which Keras abstracts out really well. Additionally, PyTorch has no high level abstractions which picks good defaults for most common problems.</p> <p>This leads us to the observation that there are three niche problems unsolved in the PyTorch ecosystem:</p>"},{"location":"writing/silentriseofpytorch/#unsolved-problems","title":"Unsolved Problems","text":"<ul> <li>General Purpose Abstraction: Over PyTorch similar to Keras or tf.learn</li> <li>Adoption: Something to help traditional ML practitioners adopt PyTorch more easily</li> <li>Production Use Cases: Something which allows engineers to take Pytorch code as-is in production or port to Caffe2 with minimal effort. I like Gluon for this, it has no community support but is backed by MSFT and AWS both.</li> </ul> <p>Few specialized efforts like AllenAI\u2019s NLP though built for NLP, or PyTorch torchvision &amp; torchtext are domain specific instead of a generic abstraction similar to Keras. They deserve their own discussion space, separate from here.</p>"},{"location":"writing/silentriseofpytorch/#the-better-alternatives","title":"The Better Alternatives","text":""},{"location":"writing/silentriseofpytorch/#fastai","title":"fast.ai","text":"<ul> <li>github, docs</li> </ul> <p>fastai has outrageously good defaults for both vision and NLP. They have several amazing implementations for Cyclic Learning Rate, learning rate schedulers, data augmentation, decent API design, interesting dataloaders, and most important: extremely extensible!</p> <p>It as seen some rather great adoption among Kagglers and beginners alike for faster experimentation. It is also helped by their amazing MOOC course.</p>"},{"location":"writing/silentriseofpytorch/#ignite","title":"Ignite","text":"<ul> <li>github, docs</li> </ul> <p>Ignite helps you write compact but full-featured training loops in a few lines of code. It is fairly extensible, and results in a lot of compact code. There is no peeking under the hood. This is the best contender for Keras for PyTorch power users.</p> <p>I do not know of any power users of Ignite, despite their elegant design. Nor have I seen it\u2019s adoption in the wild.</p>"},{"location":"writing/silentriseofpytorch/#ptl-pytorch-lightning","title":"PTL: PyTorch-Lightning","text":"<ul> <li>github </li> </ul> <p>Built by folks over at NYU and FAIR, Lightning is gives you the skeleton to flesh our your experiments. The best contender to Keras for Researchers. The built in mixed precision support (via apex) and distributed training is definitely helpful.</p> <p>The biggest value add I guess will be explicit decision, all in one class\u2014 instead of the scattered pieces we see with PyTorch. Yay Reproducibility!</p> <p>The lib is still very new, and that shows up in it\u2019s lack of adoption but is getting a lot of star counts in first week of launch!</p> <p>Check out detailed comparison between Lightning and Ignite from the creator of Lightning</p>"},{"location":"writing/silentriseofpytorch/#skorch","title":"Skorch","text":"<ul> <li>github </li> </ul> <p>skorch is attacking the bringing ML people to Deep Learning problem above</p> <p>skorch is a scikit-learn style wrapper (with metrics and pipelines support!) for Pytorch by a commercial entity invested in it\u2019s adoption. It is being developed fairly actively (most recent master commit is less than 15 days old) and marching to v1.</p>"},{"location":"writing/silentriseofpytorch/#summary","title":"Summary","text":"<p>fast.ai: researchers, rapid iterators like Kagglers skorch: welcome people coming from more traditional Machine learning backgrounds PyTorch Lightning: custom built for DL experts looking for experimentation tooling</p> <p>Ending Note: What are using for deep learning experiments? Have you seen the light with PyTorch or still going with Tensorflow? Tell me @nirantk</p>"},{"location":"writing/standout/","title":"Standing Out at Work","text":"<p>In my short career, I've seen some amazing young people stand out at work. </p> <p>Here is what they did, organized by 5 themes: </p> <ol> <li> <p>Unlock Knowledge </p> </li> <li> <p>Automate Boring Stuff</p> </li> <li> <p>Write Widely</p> </li> <li> <p>Solve Open Problems</p> </li> <li> <p>Outside-Visible Work</p> </li> </ol>"},{"location":"writing/standout/#unlock-knowledge","title":"Unlock Knowledge","text":"<p>Expertise and know-how stay trapped in emails, docs and codebases. Unlock this for everyone else. </p> <p>Get your Star Salesman on an internal podcast to spill their secrets. Do email interviews with your tech leaders and PMs. Publish as internal docs on best practices of your teammates.</p> <p>Build a repo of this over the years.</p>"},{"location":"writing/standout/#automating-boring-stuff","title":"Automating Boring Stuff","text":"<p>You can get noticed quickly by writing Excel macros for the analysis your boss does manually every week. </p> <p>Or improve build-deploy speeds for devs. In design, make a theme for Sales Decks and Internal meetings. PMs can templatize repeated docs that they write, like updates or shipped emails.</p>"},{"location":"writing/standout/#write-widely","title":"Write Widely","text":"<p>Write team/company-wide docs. E.g. Setting up your machine, in-jokes, internal dictionaries etc. Make the codebase navigable by adding docs and README. </p> <p>Take notes in a meeting. Email them to everyone later. It helped Chris get into Google and become a VC.</p>"},{"location":"writing/standout/#outside-visible-work","title":"Outside-Visible work","text":"<p>In some teams, the culture will prevent you from taking initiative.</p> <p>In those cases, get visible outside your company first. Participate in hackathons, meetups and do open source work, or write blogs. </p> <p>This is what I've used the most and perfected to an art form. I got more projects for my Deep Learning skills at my work once Hindi2vec and Awesome-NLP went viral. My skills had not changed at all.</p>"},{"location":"writing/standout/#solve-open-problems","title":"Solve Open Problems","text":"<p>Most teams have 1 or 2 important problems which are parked for later. </p> <p>Research them</p> <p>You can do primary user interviews, Twitter polls and Google Form surveys on WhatsApp. Get macro-estimates from Google Searches. Compile into a crisp video/written report. Share widely. Add recommendations by asking experts.</p> <p>To re-iterate, all of this builds on the most fundamental principle: Doing your job well. </p> <p>When you promise something, get it done. Nothing stands out more than initiative and accountability.</p> <p>What have you seen amazing people around you do that you\u2019d want others to do as well? Do reply and share your experiences.</p> <p>That is all for tonight!</p> <p>Natkhat, Nirant</p>"},{"location":"writing/strong-beliefs-loosely-held/","title":"Strong Beliefs, Loosely Held","text":""},{"location":"writing/strong-beliefs-loosely-held/#on-death","title":"On Death","text":"<ul> <li> <p>Is usually better the sooner it happens. Just like it\u2019s best to retire from your cricket career at your peak for your own sake, it\u2019s best to die when you are at your peak. You just might become Father of the Nation.</p> </li> <li> <p>No one wishes that they\u2019d accomplished less on their death beds. You might hear that no one wishes that they\u2019d worked more. That\u2019s right too. And there in lies the great human stupidity. You can\u2019t hope to accomplish without working more. You might accomplish less despite working more.</p> </li> </ul> <p>This is also my answer for what I think is true, but almost no one agrees with me (from Peter Thiel\u2019s Zero to One)</p> <ul> <li>No one (with as many safety nets as me) would wish that they\u2019d take less risks in their 20s. Cool people (like cool companies) are polite, go against less empathetic institutions and take risks (h/t Jeff Bezos)</li> </ul>"},{"location":"writing/strong-beliefs-loosely-held/#on-purpose-of-life","title":"On Purpose of Life","text":"<ul> <li> <p>There is no higher purpose. Why would a microscopic life from a floating blue speck in the dark cold space have a higher purpose? It\u2019s arrogant to assume otherwise</p> </li> <li> <p>Serve the man next to you, then the one next to him and so on </p> </li> <li> <p>Follow the Paradoxical Commandments, not because they help, but follow them anyway. </p> </li> </ul>"},{"location":"writing/strong-beliefs-loosely-held/#on-philosophy-of-life","title":"On Philosophy of Life","text":"<ul> <li> <p>Options, choices are net bad. Decisions are good. Defaults are better. Burn your bridges.  All-you-can eat buffets are scams. Having good looks or being visibly rich is a dating disadvantage because it creates options.</p> </li> <li> <p>When in doubt, look for answers from the dead: Sufis, Stoics, Mystics, and Buddhists. </p> </li> <li> <p>Eastern philosophy is more tolerable than Western philosophy. Here is how they contrast</p> </li> <li> <p>Simple systems that fail at edges beat complicated systems that can handle edge cases. As in engineering, so in life. Regret minimization framework (h/t Jeff Bezos) seems like the best candidate and I am testing it as I write this uncomfortably honest piece</p> </li> </ul>"},{"location":"writing/strong-beliefs-loosely-held/#on-money","title":"On Money","text":"<ul> <li> <p>Time value of money is underrated</p> </li> <li> <p>Money is like sex. It\u2019s good to have a lot of it when you are young. That still won\u2019t fill the hole left in your heart by not having a philosophy</p> </li> <li> <p>If you\u2019ve money, invest in your learning by paying for coffees with smarter people. Books, blogs, and those Coursera courses are a slow learning curve</p> </li> </ul>"},{"location":"writing/strong-beliefs-loosely-held/#on-work-life-balance","title":"On Work-Life Balance","text":"<ul> <li>The Case Against Work Life Balance is stronger than ever. <ul> <li>Related: Keep looking for your Gamma Radiation</li> </ul> </li> </ul> <p>An older version of this lives on Medium</p>"},{"location":"writing/talentcooling/","title":"Talent Cooling","text":""},{"location":"writing/talentcooling/#talent-cooling-kills-startup-growth","title":"Talent Cooling Kills Startup Growth","text":"<p>Evaporative Cooling of talent occurs when the most high-value contributors to a community realize that the community is no longer serving their needs anymore and, therefore, leave.</p> <p>Then something remarkably interesting happens:</p> <p>When that happens, it drops the general quality of the community down such that the next most high-value contributors now find the community underwhelming. Each layer of disappearances slowly reduces the average quality of the group until such a point that you reach the people who are so unskilled and unaware of it that they\u2019re unable to tell that they\u2019re part of a mediocre group.</p> <p>This applies to startups trying to scale/grow as well. For small teams, as little as 1-2 resignations can open a dam of exits.</p>"},{"location":"writing/talentcooling/#best-engineers-are-a-guild","title":"Best Engineers are a Guild","text":"<p>A useful mental model to have is this: Engineering is a Guild.</p> <p>Makers, and particularly software engineers are loosely defined guilds. The guild-like behaviour emerges partially because the best engineers are ~within 3 degrees of each other at any particular given point in time.  The guild decides which companies and offices it wants to support.</p> <p>Most entrepreneurs and business folks e.g. product managers do not seem to mentally acknowledge the tacit existence of this guild early enough.</p>"},{"location":"writing/talentcooling/#recognizing-mediocrity","title":"Recognizing Mediocrity","text":"<p>Here is one version of how this story plays out: </p> <p>As you scale, you hire some amazing technology people. Build an engineering team. You build something useful together. 1-2 people quit.But you\u2019re okay. You\u2019ve promoted the people who trained under the expert. That is how you grow \u201cyour people\u201d. You\u2019re hiring some senior external talent too!</p> <p>After a while, your tech team is unable to ship fast enough and sometimes even your basic hygiene factors like uptime are suffering. \u201cRefactoring\u201d becomes a rallying cry instead of shipped.</p> <p>Product is blaming tech, and within tech, different functions e.g. front end vs backend work with varying degrees of distrust.</p> <p></p> <p>Here is where things get truly interesting. You\u2019re now actively fighting the Fiery Demon of Talent Cooling. </p> <p>The new engineering manager instils a bunch of processes, hires some amazing people and shows a lot of movement. But when you look back 1-2 quarters later, you\u2019re again at the same place: there is no fire in your shipping engine. </p> <p>Why? Because it\u2019s extremely hard for mediocre people to realize that they\u2019re in a mediocrity.</p> <p>Sources/Further Reading: 1. https://web.archive.org/web/20190328104722/http://blog.bumblebeelabs.com/?p=1207  2. https://medriscoll-com.cdn.ampproject.org/c/s/medriscoll.com/post/9117396231/the-guild-of-silicon-valley/amp 3. If you\u2019re interested in avoiding losing your best tech talent to discomfort and boredom, this is an excellent starting point, written by a practitioner: https://randsinrepose.com/archives/bored-people-quit/</p>"},{"location":"writing/tax-tips-for-consultants/","title":"Tax Tips for Technical Consultants in India","text":"<p>Managing taxation and financial compliance as a consultant in India involves several considerations: GST to the benefits of Section 44ADA and the complexities of foreign entity considerations, consultants must juggle multiple aspects to ensure compliance and tax optimization. I'm sharing what I know in the hopes it's helpful to others.</p>"},{"location":"writing/tax-tips-for-consultants/#1-compliance-gst-lut-and-firc","title":"1. Compliance: GST, LUT and FIRC","text":"<ul> <li> <p>Threshold: Acquiring a GST number becomes almost mandatory once your income surpasses some X=20 lakhs INR in a given financial year. This step legitimizes your income and ensures compliance with Indian taxation laws. The X limits keeps changing and I'd recommend you check the latest one with a CA and NOT rely on Google for this. Some online services like ClearTax offer GST registration services, but I'd recommend you get it done via a CA. It's a one-time process and you don't need to renew it every year.</p> </li> <li> <p>Sole Proprietorship: If you are an individual, you get a GST number as a sole proprietor. This is the simplest way to get a GST number and is recommended for most consultants. You can also get a GST number as a private limited company, but that might be overkill for most consultants. My CA also recommended that I get an Udyog Certificate, so I did get one but I'm not sure if it's necessary.</p> </li> <li> <p>Quarterly Filing: If you are a sole proprietor or operating as an individual consultant, quarterly GST filing can be more manageable than doing it monthly. This approach helps streamline administrative tasks and keeps compliance straightforward.</p> </li> <li> <p>FIRC: Every time that you receive a payment from a foreign entity, you will need to get a Foreign Inward Remittance Certificate (FIRC) from your bank. This document is a proof of payment and is required for filing taxes. It is advisable to get a FIRC for each payment, as it can be difficult to get one later. Salt is a great tool for automating this process.</p> </li> <li> <p>Accounting &amp; Compliance: Using a CA to file for managing both your GST and Letter of Undertaking (LUT) is advisable. It serves as an effective tool for official documentation and ensures a smoother filing process.</p> </li> <li> <p>Current Account: Having a current account separate from your personal finances helps in meticulous financial management. It allows you to distinctly separate your business income and expenses, making accounting more cleaner. E.g. you can use this current to pay for your AWS bills. This can help you save on GST, as you can claim GST input credit on these expenses.</p> </li> <li> <p>Letter of Undertaking (LUT): For consultants involved in international transactions, an LUT is crucial. This legal document clarifies that all relevant taxes will be paid exclusively in India, thereby simplifying compliance requirements for cross-border business. It is advisable to get an LUT even if you are not sure if you will be involved in international transactions. You need to mention the LUT in your invoices to foreign entities.</p> </li> <li> <p>GST on Foreign Transactions: GST is not applicable on foreign transactions. However, you will need to mention the LUT in your invoices to foreign entities. This is to ensure that the GST authorities are aware that you are not liable to pay GST on these transactions.</p> </li> </ul>"},{"location":"writing/tax-tips-for-consultants/#11-gst-filing","title":"1.1 GST Filing","text":"<ul> <li>GST Filing: You can file GST monthly or quarterly. I'd recommend you to file quarterly, as it's easier to manage. I also recommend hiring a CA to do this for you who can also help you with other compliances.</li> </ul>"},{"location":"writing/tax-tips-for-consultants/#12-address","title":"1.2 Address","text":"<ul> <li>I'd recommend you to get a separate address for your business. I use a Virtual Office address and that works. I'd recommend that you get this somewhere you can visit if needed by the bank or tax authorities.</li> <li>If you are a consultant, you can use your home address as your business address. This will need a NoC from the title owner of the property. This owner might be your landlord or your parents.</li> <li>GST Number is different for each state/billing address</li> </ul>"},{"location":"writing/tax-tips-for-consultants/#2-taxation","title":"2. Taxation","text":""},{"location":"writing/tax-tips-for-consultants/#21-personal-income-tax-section-44ada","title":"2.1 Personal Income Tax: Section 44ADA","text":"<ul> <li>50% Rule: Section 44ADA offers a benefit by allowing you to assume 50% of your income upto 75 lakhs as an expense for tax filing. This helps significantly in reducing your taxable income, which can be advantageous for many consultants. </li> </ul> <p>Example Math:</p> <p>If you make 72 lakhs INR in a year, you can assume 36 lakhs INR (half of 75 lakhs) as your expenses and pay taxes on the remaining 72-36=36 lakhs INR. At 30% income tax rate, you'd pay 10.8 lakhs INR in taxes. Effectively, you'd be paying 10.8/72 = 15% of your income in taxes.</p> <p>\ud83d\udca1 You do NOT need to maintain books of expenses for Section 44ADA. This is a huge benefit as it simplifies the process of filing taxes!</p> <p>There is another catch on 100 lakhs INR: 15% surcharge on income tax. Source: ClearTax -- I'm unclear if Section 44ADA applies to this surcharge. </p> <ul> <li>&gt;50% Expenses: If your actual expenses amount to more than half of your income, then Section 44ADA may not offer you the best tax advantage. This is because you can only claim 50% of your income as an expense, even if your actual expenses are higher. You will have to get audited by a CA and file your taxes later (usually in September) if you want to claim more than 50% of your income as expenses.</li> </ul>"},{"location":"writing/tax-tips-for-consultants/#22-foreign-llc-or-c-corp","title":"2.2 Foreign LLC or C-Corp","text":"<ul> <li>LLC Abroad: If you are working with a foreign entity, it is sometimes useful to have a Limited Liability Company (LLC) in the country of the entity. E.g. you can open a Delaware LLC via Stripe Atlas and use that to invoice your clients. You will then pay taxes via this entity in the source country. It's only for the amount that you move to India, that you will pay taxes in India. </li> </ul> <p>If I remember correctly, you will be paying 8.7% corporate tax in Delaware, and similarly 8-9% in Dubai. With all of these, you pay additional 30% personal income tax in India. So if you make 100 lakhs INR in a year, you will pay 8.7 lakhs INR in the US. And then you pay personal income tax in India on your living expenses: So say, you move 18 lakhs INR to India, you will pay 0.3*18 = 5.4 lakhs INR in taxes in India. Effectively, you'd be paying 14.1% of your income in taxes. If you are making more than 100 lakhs INR in a year, this might be a good option for you.</p> <p>This can help simplify the process of receiving payments. However, it is important to note that this option is not always the most tax-efficient. It is advisable to consult a CA to understand the implications of this option for your specific amount, transaction frequency and expected personal expenses.</p> <p>RemoteIndian has a similar Tax Guide that you might find useful. </p>"},{"location":"writing/tax-tips-for-consultants/#3-invoicing","title":"3. Invoicing","text":"<p>I use Refrens to invoice my clients. It's a great tool for invoicing and managing your invoices. Here are some of the fields which I'd recommend you to fill in: </p> <ol> <li>GSTIN: This is your GST number.</li> <li>Billed To: This is the name, address, phone number, PAN and GSTIN of the client.</li> <li>Invoice Date: This is the date of the invoice.</li> <li>Due Date: This is the date by which the client needs to pay the invoice.</li> <li>Invoice Number: This is the invoice number.</li> <li>Invoice Amount: This is the amount of the invoice.</li> <li>Invoice Currency: This is the currency of the invoice.</li> </ol> <p>I'm not a CA, so please consult a CA before making any decisions. I'm sharing what I know in the hopes it's helpful to others. If you have any questions, please feel free to reach out to me on Twitter.</p>"},{"location":"writing/techtalktips/","title":"Tech Talk Tips","text":"<p>Collection of the Best Advice on Internet that I know about on giving a tech talk. Based on responses from my question on Twitter. </p> <p>{{&lt; figure src=\"/images/meghanaTalk.jpg\" caption=\"Meghana gave a talk based on these tips at PyData Bengaluru\" &gt;}}</p> <p>You: Hey, I know something better! </p> <p>Me: Please tell me about it! Raise a PR. Or reply to the tweet above!</p>"},{"location":"writing/techtalktips/#the-mindset","title":"The Mindset","text":"<ul> <li>Presentations Skills Considered Harmful tl;dr: Do not block the message with your actions. Put in the work to get the message across. </li> </ul>"},{"location":"writing/techtalktips/#how-to-structure-and-style-the-talk","title":"How to structure and style the talk?","text":"<p>Everyone's experiences are different so not sure how well mine generalise. But: find someone whose style of presentation you like &amp; take some inspiration. There are soo many different ways of delivering a talk &amp; it's all about finding the one that works best for your personality.</p> <p>IMO, slides shouldn't contain everything that's being said in the talk. They should provide an overview of the main points and complement them visually \u2013 e.g. with diagrams, illustrations, small code examples. Also, people LOVE taking pics of slides.</p> <p>I typically practice my talks in logical units, then practice the transitions, then put it all together at the end. I find that much easier and more efficient than only ever doing full run-throughs. But then again, people have different preferences here.</p> <ul> <li>From Ines, the co-creator of spaCy</li> </ul>"},{"location":"writing/techtalktips/#on-how-to-start-the-talk","title":"On how to start the talk:","text":"<p>Give an outline to the audience about the topics that will be addressed. More importantly, gauge the existing level of understanding of the subject of the audience and modify the presentation accordingly. Best way is to ask questions before starting the presentation.</p>"},{"location":"writing/techtalktips/#on-what-is-important","title":"On what is important","text":"<p>Holding the attention of the audience is v important. You are easier to pay attention to when you are  * entertaining to the audience * telling a story * telling the truth</p> <p>For every point of truth you want to cover, wrap it with an entertaining story arc. One are per point.</p> <p>List all your points. Then list all the stories you could tell per point. Then sort the stories by entertainment value to the audience.  Assume one story arc is approx per 10 mins of speaking. A mental model of \"telling a fun story to someone you've met a few times\" helps. - From Sidu Ponappa (@ponappa), GoJek India MD</p>"},{"location":"writing/techtalktips/#dont-fear-live-coding-or-live-demos","title":"Don\u2019t fear live coding or live demos","text":"<p>For technical talks, I\u2019m a big fan of live coding or live demos, as opposed to presenting all the material on slides. Audiences like live coding better than slides, because they actually get to see how the system works, as opposed to looking at static snapshots of a perfect working system. Have all my live coding talks gone perfectly? No, but I\u2019ve found audiences to be very understanding when things go awry. Everyone knows that there is risk involved, and I\u2019ve even had people tell me they learned more when they watched the recovery from (or explanation of) an error than they would have by watching glossy slides fly by. Audience attention is definitely higher for live coding talks.</p> <ul> <li>Public Speaking Tips by Chris Shaver, includes a great list of broadly applicable advice</li> </ul>"},{"location":"writing/techtalktips/#more-good","title":"More good","text":"<ul> <li>Technical Speaking Archives</li> </ul>"},{"location":"writing/vector-search-at-scale/","title":"Vector Search at Scale: Balancing Cost, Quality, and Sanity","text":"<p>At scale, relevance isn't your only enemy. Cost is. Every millisecond of latency, every token passed to an LLM, and every unnecessary reranker call adds up\u2014fast. The iron triangle of retrieval and hence, RAG is relevance, cost, and latency. You can only pick two.</p> <p>Today, we'll focus on the cost and latency.</p> <p>Here's a list of things that teams do that can be improved:</p> <ul> <li>Run full-precision vector search for every query</li> <li>Skip lexical signals altogether</li> <li>Avoid reranking because \"it's too expensive\"</li> <li>Have no system to analyze why results are bad</li> </ul> <p>This post is a walkthrough of what a real retrieval stack looks like when it's designed not just for correctness, but also for operational efficiency and failure debugging.</p> <p> Figure: Retrieval stack architecture balancing cost, quality, and latency. Each layer maximizes relevance per dollar and enables debugging.</p>"},{"location":"writing/vector-search-at-scale/#the-architecture","title":"The Architecture","text":"<p>Forget monoliths. Retrieval is a pipeline. Here's the architecture I recommend. Each layer exists for a reason: to maximize relevance per dollar and to make debugging sane.</p>"},{"location":"writing/vector-search-at-scale/#1-query-router","title":"1. Query Router","text":"<p>This is your traffic cop. It decides how to fan out the query: to a lexical search engine (BM25), a fast vector index, or both. You can route based on query class, business priority, or budget.</p>"},{"location":"writing/vector-search-at-scale/#2-bm25-search","title":"2. BM25 Search","text":"<p>Not dead. In fact, BM25 still shines for acronym-heavy domains, product names, and anything with proper nouns. It's cheap, precise, and the ideal complement to lossy vector embeddings. Run it in parallel with your vector retrieval.</p>"},{"location":"writing/vector-search-at-scale/#3-binary-quantized-vector-search-ram","title":"3. Binary Quantized Vector Search (RAM)","text":"<p>This is your fast recall layer\u2014usually IVFPQ or scalar quantization in FAISS or ScaNN. Gets you top-K quickly, cheaply. Think of it as a rough shortlist generator. Latency under 5ms is normal.</p>"},{"location":"writing/vector-search-at-scale/#4-full-precision-vector-search-disk","title":"4. Full-Precision Vector Search (Disk)","text":"<p>From your shortlist, you can now hit the full-resolution vectors. Higher fidelity, slower access, stored on disk. You should only do this when needed\u2014ambiguous queries, high-value flows, or when the approximate search isn't enough.</p>"},{"location":"writing/vector-search-at-scale/#5-cross-encoder-reranker","title":"5. Cross-Encoder Reranker","text":"<p>This is the first component in the stack that actually understands relevance. Embeddings collapse meaning into vectors. Cross-encoders read both the query and the doc, and compute true semantic alignment. Expensive, yes. But reranking the top 20\u2013100 candidates is usually all you need.</p>"},{"location":"writing/vector-search-at-scale/#6-result-aggregation","title":"6. Result Aggregation","text":"<p>Once you've got candidates from both BM25 and vector search, and re-ranked the best ones, you blend them. The fusion logic depends on your goal: pure precision, diversity, confidence thresholds, etc.</p>"},{"location":"writing/vector-search-at-scale/#building-feedback-loops","title":"Building Feedback Loops","text":"<p>Most retrieval problems aren't one-off issues. They're patterns. Instead of debugging individual queries, cluster them. Use a mix of token overlap and embedding distance. Add UMAP or HDBSCAN if needed.</p> <p>The goal isn't just analysis\u2014it's systematic insight:</p> <ul> <li>Which queries have zero recall?</li> <li>Which are poorly reranked?</li> <li>Which embeddings collapse semantically distinct queries?</li> </ul> <p>Once you know that, you can prioritize improvements\u2014embedding quality, routing rules, metadata enrichment, or prompt tuning\u2014at the cluster level. Much higher leverage than spot fixes.</p>"},{"location":"writing/vector-search-at-scale/#why-this-matters-for-rag","title":"Why This Matters for RAG","text":"<p>If your retrieval is weak, your LLM has to do all the heavy lifting. That means more tokens, more hallucinations, slower responses. And ironically, worse answers.</p> <p>Your retrieval stack should do two things: 1. Return the most relevant docs 2. Let you understand why it didn't</p> <p>Without that, you're just doing GPT improv with 3 PDFs in context.</p> <p>Don't treat retrieval as a \"vector db\" checkbox. Treat it as a system. The best stacks layer:</p> <ul> <li>Cheap recall</li> <li>Precise reranking</li> <li>Old-school lexical sanity checks</li> </ul> <p>1 line summary: RAM-level quantized vectors give you scale, Disk-level full vectors give you fidelity, BM25 gives you robustness, Rerankers give you actual relevance, Query clustering gives you insight.</p> <p>What's expensive isn't reranking. What's expensive is debugging bad search with no observability.</p> <p>If you're building RAG at scale and want to audit your retrieval infra, I do this for a living. We go from \"it kind of works\" to \"we know exactly what's wrong and how to fix it.\"</p>"},{"location":"writing/verloop-ml-prep-guide/","title":"Verloop NLP Interview Prep Guide","text":"<p>Update, September 2021: This guide is a little outdated, but not obsolete. I no longer work at Verloop.io. </p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#preparation-guide","title":"Preparation Guide","text":"<p>I've been an early Machine Learning Engineer at Verloop.io for almost 1.5 years, primarily working on NLP problems and now more in an Engineering Manager-ish role. </p> <p>This is the guide which I sometimes send to our candidates after they submit the Programming Challenge. If a candidate has relevant open source code sample, specially to other repositories we may choose to waive off the Programming Challenge completely. </p> <p>I originally wrote this to give a chance to folks coming from non-NLP background to get a sense of the problem space a little better. I'd hoped that it'd absolve smart people of the assumption that Churn, Text Generation and Image Segmentation can be all solve with the same idea-kit, but no luck. </p> <p>I hope this is most useful to candidate interviewing for ML roles in companies similar to us in terms of size, scale and challenges. This is also useful to:</p> <ul> <li>Early career folks - typically less than 2 years of NLP experience</li> <li>Folks coming from Computer Vision/Tabular Data/Classical ML background</li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#the-role","title":"The Role","text":"<p>Machine Learning Engineers at Verloop develop technologies that influence how users interact, engage and feel about chat and customer service. As a/an MLE, you will specialize in supervised/unsupervised learning and apply techniques to various problems, mostly dealing with high volume natural language processing applications.</p> <p>At Verloop, the same team of engineers owns the entire stack from research to production. This means that the following roles HAVE NOT EXISTED at Verloop:</p> <ol> <li> <p>Data Scientist: </p> <ul> <li>Does: Data Analytics, Exploration, Model A/B Testing and Data Modeling</li> <li>Tools: Everything from Data Viz to Excel, Pandas, spaCy and Scikit-Learn</li> </ul> </li> <li> <p>Data Engineer: </p> <ul> <li>Does: Builds ata pipelines, storage, data version control, monitoring</li> <li>Tools: Kafka, Airflow etc.</li> </ul> </li> <li> <p>ML Researcher: </p> <ul> <li>Does: Trains forward looking, NOT production ready models. Focussed on experimentation</li> <li>Tools: huggingface/transformers, spaCy, flair (by Zolando Research)</li> </ul> </li> </ol> <p>All of the above is done by MLE at Verloop. Every engineer is expected to build a strong suite in one or more of the above roles, while maintaining a high minimum at the others. </p> <p>Verloop MLE often contribute to adjacent/overlapping challenges such as DevOps for Machine Learning aka ML Ops. These are often problems in model serving, automated deployment, data versioning, model monitoring, alerting and other parts of developer tooling.</p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#programming-tips","title":"Programming Tips","text":"<p>Write your code (unless otherwise stated) as if you are deploying this code to production. So anything which you'd want to pay attention to for your production code should ideally be there. </p> <p>That said, here are some things which we pay attention to in your code sample: </p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#api-object-oriented-design","title":"API &amp; Object Oriented Design","text":"<ul> <li>Almost always useful to separate views from models</li> <li>If you are implementing REST, do REST properly</li> <li>Keeping common sense names for endpoints make everyones life easier</li> </ul> <p>See 12 Factor App for the gold standard on App Dev practices, here we select only a few and explain what we pay attention to</p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#code-readability-style-design","title":"Code Readability, Style &amp; Design","text":"<ul> <li>Use tools like <code>isort</code> in Python to neatly organize your imports. Avoid using <code>fastai</code> style imports of <code>from X import *</code></li> <li>Docstrings for functions, inline comments where applicable to explain the \"why\" or \"how\" - but not what</li> <li>We like the style to be consistent. E.g. in case of Python, following PEP8 or Google Styleguide will improve your code readability by a lot </li> <li>In Python, type hints will make your and our life easier when we have to debug something</li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#developer-hygiene","title":"Developer Hygiene","text":"<ul> <li>Use a <code>requirements.txt</code>, conda <code>environment.yml</code> or Dockerfile to declare your dependencies</li> <li>Write tests!</li> <li>Use logging. Generously with levels. But not so much that it slows your production performance. </li> <li>README with comments, notes, assumptions and what the code is doing</li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#machine-learning-tips","title":"Machine Learning Tips","text":"<p>In contrast to interview processes which go wide, covering everything from Probability, Statistics, Linear Algebra to Deep Learning and everything in between - we go deep on primarily one aspect: Applied Natural Language Processing. We operate close to Research, occassionally doing research even. </p> <p>These are supposed to be indicative/descriptive of the technical skill we desire. These are not prescriptive i.e. you do not have to do the course to clear our interviews. </p> <p>We just have seen everyone with equivalent skill as these courses do well in our interview process.</p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#deep-learning","title":"Deep Learning","text":"<ul> <li>Basics for Beginners: FastAI Part 1 &amp; 2<ul> <li>Trivia: 2 out of our team of 6 have been fast.ai International Fellows. </li> </ul> </li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#natural-language-processing-deep-dive","title":"Natural Language Processing Deep Dive","text":"<ul> <li> <p>NLP: Code First Introduction by FastAI\u2019s Dr. Rachel Thomas</p> </li> <li> <p>NLP with Deep Learning: cs224n by Stanford</p> <ul> <li>For a SDE 2 role, having Deep Learning skills equivalent to this course is necessary but not sufficient</li> </ul> </li> <li> <p>(Hard Mode) Yandex Data School\u2019s NLP Course</p> </li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#general-interview-tips","title":"General Interview Tips","text":"<p>Think Out Loud -- We want to understand how you think, so explain your thought process and decision making throughout the interview. </p> <p>Remember we\u2019re not evaluating your technical ability alone, but also how you solve problems.</p> <p>Clarify -- Many questions will be deliberately open-ended to provide insight into what parts and information you value within the technological puzzle.</p> <p>We\u2019re looking to see how you engage with the problem and your primary method for solving it. </p> <p>Be sure to talk through your thought process and feel free to ask specific questions if you need clarification.</p> <p>Do your Homework -- The primary reason that this is a Take Home Challenge plus a discussion is that we don't want to evaluate your ability to think on your feet. </p> <p>That said, if you come to the interview without enough homework on your own submission and question -- you'll be put in a place where a lot of thinking will in fact be on your feet.</p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#machine-learning-interview-technical-prep","title":"Machine Learning Interview Technical Prep","text":"<ul> <li> <p>At least one interview will be focused on your expertise within Machine Learning. General knowledge of the field and its main concepts should be demonstrated throughout the interview, such as supervised learning, unsupervised learning, overfitting, boosting, and regularization.</p> </li> <li> <p>Experience with common learning paradigms such as decision trees, k-means, LSTMs and understanding of how to apply those techniques to various problems is important.</p> </li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#how-to-prepare","title":"How to Prepare?","text":"<ul> <li> <p>Get extremely comfortable with writing code to do Machine Learning (and by extension, Deep Learning). This means knowing what to google, what libraries to use and so on is very valuable.</p> </li> <li> <p>Be mentally prepared to talk about your approaches and results. This is effectively a defence of your work. Help us understand why you made certain decisions, choices, and what went in your head.</p> </li> <li> <p>Get comfortable working with real world text datasets, for instance multi-lingual datasets, code-mixed. Look for sample efficient learning methods and learn where these models fail or mislead.</p> </li> </ul>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/verloop-ml-prep-guide/#things-to-think-about","title":"Things to Think About","text":"<ul> <li> <p>This round is case-study driven. We value initiative and experimentation speed+quality over success. This means that even if you try an approach, which does not work for some reason - show us the work that you did: We\u2019d love to learn what you tried.</p> </li> <li> <p>Ask yourself why would they have selected this problem for the challenge? What are some basics in this domain I should know about?</p> </li> <li> <p>What is the highest level of accuracy that others have achieved with this dataset or similar problems/datasets?</p> </li> <li> <p>What types of visualizations will help me grasp the nature of the problem/data?</p> </li> <li> <p>Which modelling techniques are good at capturing the types of relationships I see in this data?</p> </li> <li> <p>What are some of the weaknesses of the model and how can the model be improved with additional work?</p> </li> <li> <p>How do I measure performance? Does it help to have confidence values against the prediction?</p> </li> <li> <p>What is latency and compute needs for this model?</p> </li> </ul> <p>This document is inspired by similar documents by Google Inc. I borrowed from the best when relevant.</p>","tags":["career","machine learning","verloop","tech"]},{"location":"writing/why-i-quit-data-science/","title":"Why I Quit Data Science","text":"<p>Question from a friend: I am interested in knowing how did you come to this decision of moving to SWE from DS/MLE. Since I've been asked a variant of this question quite a few times, I thought it would be good to share my answer.</p>"},{"location":"writing/why-i-quit-data-science/#what-kind-of-research-did-you-do-to-get-to-this-decision","title":"What kind of research did you do to get to this decision?","text":"<p>I spoke to a lot of people who were both big companies and startups. I also spoke to folks across multiple markets: Singapore, India, US &amp; Europe. I primarily spoke to people with more than 10-12 years of experience. This is a big difference in my perspective.</p>"},{"location":"writing/why-i-quit-data-science/#what-were-your-considerations-while-making-this-decision","title":"What were your considerations while making this decision?","text":""},{"location":"writing/why-i-quit-data-science/#skills","title":"Skills","text":"<p>This is how I understand the world today: There are 3 primary functions around data: data engineering, modeling (e.g. predictive) and data analytics.</p> <p>I could keep going deeper into modeling e.g. learning more about CNNs and Transformers. Between writing the NLP Book and professional Machine Learning, I'd guess that I'm in top 20-30% of the world doing this. The journey to get from here to being the best is hard and I'm not sure if I'm going to be able to do it. </p> <p>The field also suffers a bit from the Red Queen effect on the applied side of things. I'm not sure if I want to keep doing it 5, 10, or 20 years from now. I started doing Machine Learning because I was interested in the field and I was curious about how it would work. </p> <p>It's no longer about the thrill of solving a puzzle/problem anymore. The roles I've access to, have the drudgery of making the same pipelines work in similar ways and then applying them to different problems.</p> <p>I'd much rather add another skill and get to the top 25% in it -- and then quickly rise to the top on it's intersection. This will also be easier as I've tons of novelty and new ideas to learn. </p> <p>Since analytics roles are neither well respected nor well paid, the process of elimination works. I'd rather be a platform enginer than a data/product analyst.</p>"},{"location":"writing/why-i-quit-data-science/#competition","title":"Competition","text":"<p>Within this, let's talk separately about pre-series D startups and big companies (e.g. FAANG/MAGA) for modeling roles. Startups are usually open to hiring folks without a MS/PhD degree, while big companies are more open to hiring folks with a MS/PhD.</p> <p>For modeling roles at big companies you will be competing with folks with a PhD. For startups, I often see they end up hiring better trained folks as they scale up and relegate older, less 'specialised' folk to roles closer to engineering (e.g. API Design, uptime) and away from modeling.</p> <p>It's also much harder for me, personally to find truly exceptional Machine Learning mentors, but relatively easier to find proven, battle-tested quite senior engineers. And as much as you might underestimate the role of coaching, I believe that in our craft - it can save you 4-5 years of learning time.</p>"},{"location":"writing/why-i-quit-data-science/#titleimpact","title":"Title/Impact","text":"<p>I've the least confidence on this being true over a longer duration. But I'm mentioning it here since a lot of senior people do think about this. </p> <p>Growing within modeling-related roles is hard and you hit the ceiling as Head of Machine Learning. Notice that in most of the cases, you are not even Head of Data, you're Head of Research or ML or some function within Data. The Head of Data in turns reports to the senior most Engineering Leader e.g. the CTO.</p> <p>This means your influence over things which shape your day: tooling, infrastructure, product direction, org structure, promotions etc. is limited. You can't even learn these things.</p> <p>I'd like to keep my options of becoming a Engineering Manager/VP Engineering/CTO in a few years. I'd much prefer that to Head of Data Science or Analytics. This option is so much more valuable to me that I'm happy to pay a price to \"buy\" it.</p>"},{"location":"writing/why-i-quit-data-science/#what-wasis-the-goal-of-this-particular-switch-what-were-you-trying-to-optimize","title":"What was/is the goal of this particular switch? What were you trying to optimize?","text":"<p>I'm optimizing for being great (but not best) at the intersection of 3 things instead of 1 narrowly, clearly defined role. I'm trying to get to the top of the intersection.</p> <p>I was also bored by the mundanity of problems you encounter in typical early-stage startups. The need to trade off personal-notion-of-quality for speed is sometimes a bit of a problem, but I usually enjoyed the challenge.</p>"},{"location":"writing/why-i-quit-data-science/#why-not-machine-learning-engineering-at-a-big-company","title":"Why not Machine Learning Engineering at a Big Company?","text":"<p>I fear that this role combines the worst of two worlds. You've the skills of a backend engineer: you can design microservices, implement them, scale them, deploy them, and manage the infrastructure. But you also have the skills of a Data Scientist: you can build models, train them, deploy them, and manage the experimentation infrastructure.</p> <p>You don't get paid or recognized for either of them. The backend developer thinks of you as a \"ML guy\" and the Data Scientist thinks of you as a \"Backend guy\". This is made worse at a Big Company because they tend to reward specialists via promotions. You're going to get underpaid for both roles.</p> <p>Not to mention that a large fraction of your knowledge is getting outdated faster than I can learn. Of course, you might be 10x faster, better learner than me - in which case this blog post is not meant for you.</p>"},{"location":"writing/why-i-quit-data-science/#why-not-take-a-1-year-research-focussed-sabbatical","title":"Why not take a 1 year Research focussed Sabbatical?","text":"<p>Well, because companies which ask for skills acquired via a MS/PhD are often not willing to pay for a 1 year research year. It'd not be that much better than being endorsed by Dr. Andrew Ng, writing a book, mentoring folks for ACL papers and speaking at PyCon India. </p>"},{"location":"writing/why-i-quit-data-science/#what-information-did-you-find-for-and-against-this-switch","title":"What information did you find for and against this switch?","text":"<p>Stepping away from Machine Learning Lead roles can be a massive cash and title/designation downgrade. It definitely turned out to be true for me. </p> <p>My alternate job offer was a Series B/C Machine Learning Lead, instead of a Platform Engineer. I would not be happy with the role, but I'd be very happy with the salary. It'd be 3-4x in cash, and 4-5x in total compensation terms. Another way to look at this, I took at 75% cut on my cash compensation.</p> <p>I'm betting that I'll have a lot more fun doing this, but I'm also betting that I'll be a little more successful - which will compensate for this over a 4-10 year chapter of the career.</p> <p>In addition to the cash and title hit, it's a bit of social shaming: People might be inclined to assume that you were not quite good as a Data Scientist and that is why you moved to SWE. I don't care enough about that to influence my decision but I do care about it and hence worth mentioning.</p> <p>My Machine Learning skills will also atrophy with time. I'll be able to get to similar productivity faster in a few years because the half-life of knowledge in the field is really short and tooling improvements make it easier to ship well. </p>"},{"location":"writing/archive/2025/","title":"2025","text":""},{"location":"writing/archive/2024/","title":"2024","text":""},{"location":"writing/archive/2023/","title":"2023","text":""},{"location":"writing/archive/2022/","title":"2022","text":""},{"location":"writing/archive/2021/","title":"2021","text":""},{"location":"writing/archive/2020/","title":"2020","text":""},{"location":"writing/archive/2019/","title":"2019","text":""},{"location":"writing/archive/2018/","title":"2018","text":""},{"location":"writing/archive/2017/","title":"2017","text":""},{"location":"writing/archive/2016/","title":"2016","text":""},{"location":"writing/archive/2015/","title":"2015","text":""},{"location":"writing/category/llm/","title":"LLM","text":""},{"location":"writing/category/observability/","title":"observability","text":""},{"location":"writing/category/open-inference/","title":"open-inference","text":""},{"location":"writing/category/signoz/","title":"signoz","text":""},{"location":"writing/category/arize/","title":"arize","text":""},{"location":"writing/category/phoenix/","title":"phoenix","text":""},{"location":"writing/category/machine-learning/","title":"machine-learning","text":""},{"location":"writing/category/rag/","title":"RAG","text":""},{"location":"writing/category/metrics/","title":"metrics","text":""},{"location":"writing/category/ai/","title":"AI","text":""},{"location":"writing/category/niranting/","title":"niranting","text":""},{"location":"writing/category/production/","title":"production","text":""},{"location":"writing/category/careers/","title":"careers","text":""},{"location":"writing/category/talks/","title":"talks","text":""},{"location":"writing/category/data/","title":"data","text":""},{"location":"writing/category/tech/","title":"tech","text":""},{"location":"writing/category/engineering-management/","title":"engineering-management","text":""},{"location":"writing/category/podcasts/","title":"podcasts","text":""},{"location":"writing/category/personal-finance/","title":"personal-finance","text":""},{"location":"writing/category/python/","title":"python","text":""},{"location":"writing/page/2/","title":"Writing","text":""},{"location":"writing/page/3/","title":"Writing","text":""},{"location":"writing/page/4/","title":"Writing","text":""},{"location":"writing/page/5/","title":"Writing","text":""},{"location":"writing/page/6/","title":"Writing","text":""},{"location":"writing/page/7/","title":"Writing","text":""},{"location":"writing/page/8/","title":"Writing","text":""},{"location":"writing/page/9/","title":"Writing","text":""},{"location":"writing/page/10/","title":"Writing","text":""},{"location":"writing/page/11/","title":"Writing","text":""},{"location":"writing/page/12/","title":"Writing","text":""},{"location":"writing/page/13/","title":"Writing","text":""},{"location":"writing/page/14/","title":"Writing","text":""},{"location":"writing/page/15/","title":"Writing","text":""},{"location":"writing/page/16/","title":"Writing","text":""},{"location":"writing/page/17/","title":"Writing","text":""},{"location":"writing/page/18/","title":"Writing","text":""},{"location":"writing/archive/2025/page/2/","title":"2025","text":""},{"location":"writing/archive/2025/page/3/","title":"2025","text":""},{"location":"writing/archive/2025/page/4/","title":"2025","text":""},{"location":"writing/archive/2024/page/2/","title":"2024","text":""},{"location":"writing/archive/2022/page/2/","title":"2022","text":""},{"location":"writing/archive/2021/page/2/","title":"2021","text":""},{"location":"writing/archive/2020/page/2/","title":"2020","text":""},{"location":"writing/archive/2020/page/3/","title":"2020","text":""},{"location":"writing/archive/2020/page/4/","title":"2020","text":""},{"location":"writing/archive/2019/page/2/","title":"2019","text":""},{"location":"writing/archive/2019/page/3/","title":"2019","text":""},{"location":"writing/category/rag/page/2/","title":"RAG","text":""},{"location":"writing/category/rag/page/3/","title":"RAG","text":""},{"location":"writing/category/careers/page/2/","title":"careers","text":""},{"location":"writing/category/careers/page/3/","title":"careers","text":""},{"location":"writing/category/careers/page/4/","title":"careers","text":""},{"location":"writing/category/careers/page/5/","title":"careers","text":""},{"location":"writing/category/machine-learning/page/2/","title":"machine-learning","text":""},{"location":"writing/category/machine-learning/page/3/","title":"machine-learning","text":""},{"location":"writing/category/machine-learning/page/4/","title":"machine-learning","text":""},{"location":"writing/category/machine-learning/page/5/","title":"machine-learning","text":""},{"location":"writing/category/machine-learning/page/6/","title":"machine-learning","text":""},{"location":"writing/category/niranting/page/2/","title":"niranting","text":""},{"location":"writing/category/niranting/page/3/","title":"niranting","text":""},{"location":"writing/category/niranting/page/4/","title":"niranting","text":""},{"location":"writing/category/niranting/page/5/","title":"niranting","text":""},{"location":"writing/category/tech/page/2/","title":"tech","text":""},{"location":"writing/category/tech/page/3/","title":"tech","text":""},{"location":"writing/category/tech/page/4/","title":"tech","text":""}]}