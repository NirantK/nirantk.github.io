{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c7d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058b3b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-01 14:28:13</td>\n",
       "      <td>Please leave a one line intro when you join an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-01 14:29:24</td>\n",
       "      <td>I'm Pranjal Mehta. Cofounded ePlane.ai (electr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-01 14:29:26</td>\n",
       "      <td>Hey folks,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-01 14:29:50</td>\n",
       "      <td>I'm Nirant K. ML/LLM Consultant out of BLR. Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-01 14:30:27</td>\n",
       "      <td>Hey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime                                            Message\n",
       "0  2023-03-01 14:28:13  Please leave a one line intro when you join an...\n",
       "1  2023-03-01 14:29:24  I'm Pranjal Mehta. Cofounded ePlane.ai (electr...\n",
       "2  2023-03-01 14:29:26                                        Hey folks, \n",
       "3  2023-03-01 14:29:50  I'm Nirant K. ML/LLM Consultant out of BLR. Mo...\n",
       "4  2023-03-01 14:30:27                                                Hey"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"20230505_Messages.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a210b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by week\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
    "df['Week'] = df['Datetime'].dt.isocalendar().week\n",
    "df['Date'] = df['Datetime'].dt.date\n",
    "\n",
    "# Group by Date\n",
    "daily_df = df.groupby('Date').agg({'Message': ' \\n '.join}).reset_index()\n",
    "daily_df = pd.DataFrame(daily_df)\n",
    "len(daily_df)\n",
    "\n",
    "# # Group by Week\n",
    "# weekly_df = df.groupby('Week').agg({'Message': ' \\n '.join}).reset_index()\n",
    "# weekly_df = pd.DataFrame(messages_df)\n",
    "# print(weekly)\n",
    "# print(weekly_df[\"Message\"][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655b8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"wc\"] = daily_df[\"Message\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8296bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      63.000000\n",
       "mean      900.492063\n",
       "std       727.693703\n",
       "min         8.000000\n",
       "25%       286.500000\n",
       "50%       695.000000\n",
       "75%      1498.000000\n",
       "max      2540.000000\n",
       "Name: wc, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_df[\"wc\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77948434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370e4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19737966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text = daily_df[\"Message\"][54]\n",
    "texts = text_splitter.split_text(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61444aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "docs = [Document(page_content=t) for t in texts[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c8699fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# OpenAI not needing data\n",
      "\n",
      "- OpenAI claims they don't need more data, but this is highly doubtful as they would happily take more data if it was available.\n",
      "- They have exhausted text data for GPT and need more data for fine-tuning layers like RLHF.\n",
      "- OpenAI doesn't use API data, and chatting on their website probably won't be used as is (which they state in the free version).\n",
      "- Ideally, if they really don't need data, they should give a zero knowledge guarantee.\n",
      "- OpenAI doesn't use input or output, but they could use intermediate layer information. It's unclear if their team can eyeball it to improve the system.\n",
      "- They do need more data, just not API platform customer data.\n",
      "- OpenAI is actively seeking more data to train their models from the industry because they have exhausted internet and academic datasets.\n",
      "- Over time, they may get to your request.\n",
      "\n",
      "# Data Partnership\n",
      "\n",
      "- OpenAI has data partners, but a list of them is not readily available.\n",
      "- Their platform API data policy is public information, however.\n",
      "\n",
      "# Multinationals using OpenAI\n",
      "\n",
      "- OpenAI's models are quite good, and it's just a matter of time before more multinationals start using them.\n",
      "\n",
      "# Replit Codegen model vs OpenAI Codex\n",
      "\n",
      "- The Replit Codegen model is better than OpenAI Codex in many human evaluation tasks and, at 2.7B params, is way smaller than OpenAI Codex. OpenAI Codex is extremely over-trained by Chinchilla metrics.\n",
      "- Apparently, the Replit Codegen model was built in under two weeks.\n",
      "\n",
      "# ML on Edge\n",
      "\n",
      "- ML on Edge is a significant development.\n",
      "- Some of this stuff, like flash attention, is generally applicable even on servers.\n",
      "- There has been a world's first on-device demonstration of stable diffusion on Android.\n",
      "\n",
      "# Palantir launches ChatGPT for war\n",
      "\n",
      "- Expect Anduril to come up with something similar.\n",
      "- The full video link is available for those interested in the involvement of tech in defense and future wars.\n",
      "\n",
      "# Music Generation\n",
      "\n",
      "- Anyone who has worked with music generation, audio generation, jingle generation, song generation, or new AI instruments is encouraged to share and open discussions over DM.\n",
      "\n",
      "# GPT-4 Multimodal Model\n",
      "\n",
      "- GPT-4 takes text in and can generate a prompt for an SD/Midjourney landscape.\n",
      "- They use photoroom.\n",
      "- GPT-4 is not yet available, but you can use https://minigpt-4.github.io/ or https://llava-vl.github.io/.\n",
      "- GPT-4 doesn't have image understanding.\n",
      "\n",
      "# Dust.tt\n",
      "\n",
      "- Dust.tt is an internal tool for now.\n",
      "- It's better than modal or runpod.\n",
      "- Dust.tt is amazing!\n",
      "\n",
      "# Google Colab\n",
      "\n",
      "- Google Colab may not be applicable for everyone, but the fastest and best experience would be to apply for Google Cloud startup credits and use a dedicated A100 for Colab.\n",
      "- Jupiter on VS Code with Banana on the backend will cost ‚Çπ150/hour.\n",
      "- Google Colab is $12 and comes with 50GB.\n",
      "- Compute units mean one unit of sustained usage of one vCPU or 1 GB of memory.\n",
      "- T4 24GB is a good option for this rate.\n",
      "- Google Colab is not persistent.\n",
      "\n",
      "# Weaviate\n",
      "\n",
      "- For anyone using Weaviate, it's unclear if you should store all of your metadata in Weaviate or in a different DB.\n",
      "- It's unclear if this is required or recommended for other Vector DBs like Weaviate.\n",
      "\n",
      "# Other Weblinks\n",
      "\n",
      "- ChartGPT: https://www.chartgpt.dev/.\n",
      "- Kandinsky-2: https://github.com/ai-forever/Kandinsky-2.\n",
      "- Accel's slide deck on opportunities in Generative AI: https://twitter.com/matthieurouif/status/1650904940036890626.\n",
      "- AI-generated pizza commercial: https://www.linkedin.com/posts/genai-center_ai-generated-pizza-commercial-tools-used-activity-7056952600516046848-mvqm?utm_source=share&utm_medium=member_ios.\n",
      "- Google researchers achieve performance breakthrough on ImageNet: https://www.reddit.com/r/StableDiffusion/comments/12yzd2a/google_researchers_achieve_performance/.\n",
      "- Qualcomm's earlier work on Stable Diffusion: https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"This is a group chat. Write a detailed, exhaustive research bullet points for each section in Markdown format for titles:\n",
    "\n",
    "{text}\n",
    "\n",
    "Research with weblinks where relevant:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(ChatOpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "output_text = chain({\"input_documents\": docs}, return_only_outputs=True)[\"output_text\"]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e55130b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='OpenAI not needing data: Highly doubt that claim, they like everyone else would happily take more data. Esp when they are out of training data for GPT (they have exhausted text data). But also as feedback for fine-tuning layers like RLHF. The key there is they don\\'t use API day, chatting on website probably will be used as is (which they say as well, in the free version). Ideally if they really don\\'t need data, they should give a zero knowledge guarantee. They don\\'t use input or output. But could they use intermediate layer info, can their folks eyeball it to improve the system. All those are probably open questions. \\n I think multinationals too will move to start using them. Their models are quite good and it is just a matter of time. \\n They do need more data, just not api platform customer data. \\n Data Partnership: Interesting. \\n Anyone have a list of data partners for Open AI? \\n Nah... They don\\'t talk about how they train their new models either in terms of data or architecture...  Their platform api  data policy is public info though... \\n I spent a day in march at openai office when they have invited some startups. They are very much looking to get more data to train their models from the industry because they have exhausted internet and academic datasets. However their organization is so small that they are able to cater to partnership proposals only from the biggest tech companies right now, all of which are lining up at their door. Over time they may get to your request. \\n Woah, it would have easily made 90%\\'ile in my undergrad randomized algo course at IIT. \\n If it helps, BITS Pilani did not have a randomized algo course ‚Äî we had a design and analysis of algorithms, which was more analysis than design: And even Turbo does better than on those questions (specially speed!) than I do even today üòÖ \\n Replit Codegen model is better than OpenAI Codex in many human eval tasks and at 2.7B params, wayyy smaller than OpenAI Codex, extremely over-trained by Chinchilla metrics \\n Apparently they built this under 2 weeks \\n https://twitter.com/Mascobot/status/1651022921056555008?t=3qozUieRAPdsnScv3XnQLw&s=19 \\n Widely known no? Even GPT3.5-turbo and GPT4 are a Codex iteration, not vanilla LLM? \\n Widely known to maybe scientists, not discussed much I feel \\n Fair, I might be suffering some form of inside-track groupthink here üòÖ \\n To many, I wouldn\\'t think it does \\n Aligned, it\\'s a utility like ec2 in 3 years or sooner \\n 100% - this is the difference between AI now and 5 years back. \\n The UX of consuming models was not great up until openai came into the picture. \\n Still kinda sad that HF hasn\\'t learned this well enough tbh \\n Once we have a well supported IR which works across all platforms, serving and consuming models from source would become a lot easier too. \\n They were born at a different time. Compared to their competition at that time(2017-18) they have done well. \\n Almost everyone who does NLP now makes their models work their transformer library. \\n In 2018 we spent half a day finding out how to use a model someone trained lol \\n UX bhi and ML application development pipeline bhi üòÖ \\n Stanford NLP ü§åüèº \\n https://www.reddit.com/r/StableDiffusion/comments/12yzd2a/google_researchers_achieve_performance/ \\n Anyone working on ML on edge here? \\n This looks big \\n Some of this stuff like flash attention is generally applicable even on servers \\n Wow! \\n https://www.qualcomm.com/news/onq/2023/02/worlds-first-on-device-demonstration-of-stable-diffusion-on-android \\n Some earlier work from Qualcomm \\n Pretty cool. Quite interesting that there is a huge jump from chatGPT. Will try this on the JEE ones we all were trying. \\n Palantir launches ChatGPT for war  \\n Damn expect anduril to come up with something too \\n The full video link for those who are interested in involvement of tech in defence and future wars \\n this makes me sad \\n I hate that their tech usecases go from disaster relief efforts straight to drone warfare \\n Well for one, this is clear proof that programmers are god living among us ‚ò∫Ô∏è \\n Only 2 ML engineers that too :) \\n Any idea how 10 days of traning time translates to total gpu time spent, i.e. including experiments? \\n I‚Äôve worked with Pete Warden and the tinyml folks at Google, used to be with the company that owns the accelerators for their voice wake, happy to chat ! \\n Wonderful! \\n Will DM \\n Anyone knows how they did it with 1/10th the parameter count? \\n Keeping it coming pls. Beginner insights most welcome Some of us still noobs in the group. 3 month old industry for us üòÇ \\n Anyone here worked with music generation / audio generation / jingle generation / song generation / new AI instruments => pls do share and open for discussions over DM üôè üéº \\n Doesn\\'t look like they\\'ve some unfair data (qty or quality advantage) ‚Äî they just decided to test Chinchilla limits and that worked. They trained for a lot more tokens than most people have tried for similarly sized models.  \\n PSA: Dedicated group for music, images, video: https://chat.whatsapp.com/GThJJhoF3cL7QCmrfIoY8J \\n If anyone remotely knows this person. An expert talk by then would be helpful ++ \\n them * \\n Replit Demo Day videos are soon going to be on Youtube from what I hear \\n To all the silent VCs in this group feeding this chat thread to an LLM, pls help üôè üòÇ \\n Slide Deck from Accel\\'s @919945307938 on opportunities in Generative AI, finetuning vs prompting and so on. Worth your 5 minutes.  \\n yeah will be up tomorrow - and the model should be released soon as well \\n Thanks! \\n Thanks Anshul sir! Anshul sir @919970204619 leads Replit India \\n Thanks! Was this a closed session? \\n Yes Sudharshan .. i did this for Accel funded startups - founders and tech teams \\n Alright thanks, was going to ask for invites to future sessions - this is really good \\n https://twitter.com/matthieurouif/status/1650904940036890626 \\n The gpt-4 multimodal model has an api yet? I thought it was chatgpt only \\n GPT4 takes text in and can generate prompt for a SD/Midjourney landscape \\n They use photoroom  \\n Not out, you can use https://minigpt-4.github.io/ or https://llava-vl.github.io/ \\n Doesn\\'t have image understanding \\n https://dust.tt is awesome! \\n It\\'s internal for now \\n better than modal or runpod? \\n Shared interest, please keep me posted on your connects ! \\n It has an interesting ui for chaining prompts. I prefer python f-strings ofcourse, but the concept is interesting in its own \\n Dust.tt is amazing! \\n Nice looking at the docs \\n i don\\'t get it. what\\'s novel about this? segment, get image caption using any model blip, sam etc. then prompt gpt-4 to create background for photoshoot. then inpaint. right? \\n https://www.linkedin.com/posts/genai-center_ai-generated-pizza-commercial-tools-used-activity-7056952600516046848-mvqm?utm_source=share&utm_medium=member_ios \\n @917054124184 is playing with music gen these days \\n Novelty isn\\'t the only dimension. Ease of use is greatly improved and tons of Shopify stores which sell everything from soaps and other trinkets would love this.  \\n oh totally agreed! my initial impression from the post here was that there\\'s something new... technically speaking (as a lot of folks here are technical).  \\n The application. \\n Went through this, this is very useful \\n Resharing the link, as someone has recently joined and is curious about the deck DM\\'ed  \\n Folks any recommendations other than Google Colab for accessing GPUs ? (I\\'m ok with 5-6$ per month cost also) - mostly hobbyist dabbling \\n How\\'s the availability on Amazon Sagemaker? \\n Might not be applicable for everyone, but I think by far the fastest and best experience would be to apply for google cloud startup credits, and use a dedicated A100 for colab. \\n Jupiter on VS Code with Banana on backend will cost ‚Çπ150/hour.  \\n ^ banana.dev also has a great free tier for 1 hour of use \\n Should still be good for tinkering use-cases I\\'d guess? \\n Really nice for notebooks / automation / scripting  \\n Yes. Community model templates allow one click deployment. For 50+ models. Simple docs. \\n Kaggle \\n I want to experiment with deploying StableDiffusion and running Gradio/Automatic1111 ... will also want to keep some of the models like Lyriel / Deliberate + Controlnets  etc .. so need storage ~50GB ... I\\'m newbie to serverless .. but I don\\'t think serverless GPUs will cut it for this usecase \\n You can try paperspace, they are undercutting big tech on GPUs. https://www.paperspace.com \\n they have 5GB space on the Gradient .. üôÅ \\n Google Colab is $12 - and 50GB \\n I have been using my Mac Pro as a server at home + socket + multiple LMs. Works like a charm :) \\n You can keep the model storage on hugging face and then use any serverless gpu player to import your model from there directly. \\n Will try this ! \\n This doesn\\'t work, start up credits cannot be used for colab, they have separate billing \\n Yes, but now you can connect a gce vm to colab, giving you persistent sessions and dedicated compute \\n https://research.google.com/colaboratory/marketplace.html \\n Can recommend runpod, stop the instance when not using, will only be charged for storage \\n Buy a 3080 machine. Looks like we are moving back to a world where we all have a desktop at home üòÖ \\n On a more serious note, runpod, lambdalabs, fluidstack. Keep moving around üòÖ \\n Finally buckled to Google Colab - 100 compute units for 12$ - 2 units per hour for StableDiffusion stuff that I\\'m doing - so 50 hours of stable diffusion for 1000 rs or 20rs / hour FYI. \\n Does anyone know what compute units mean? \\n I asked gpt. this maybe helpful \\n Back to On Prime? üòÇ \\n What GPU do they offer at this price? How much RAM for SD? \\n T4 24GB think for this rate \\n \"No One Knows What It Means But It\\'s Provocative, It Gets The People Going\" ü§£ \\n Are these instances persistent? Or do yoh still have to do the google drive hack \\n not persistent \\n Which platform is this? \\n Google cloud \\n For non persistent / spot instances of GPUs GOOG was always in under supply while we were testing.  \\n https://www.chartgpt.dev/ \\n https://news.ycombinator.com/item?id=35697627 \\n For anyone using Weaviate, do you store all of your metadata in Weaviate or in a different DB? \\n Context: I\\'m using Pinecone and I\\'m saving most of my content in a different db (pinecone has a limit on size of meta data). Want to know if that is required / recommended for other Vector DBs like Weaviate \\n whats the major difference between storing it in vector dbs and something like a numpy array? is retrieval super fast for say 1000s of vectors \\n Yep, our app allows users embed content so we need containerization + expect this to increase significantly (ü§û). If you are building a vertical app (only embedding content yourself), numpy is probably ok - others probably have a better view tho \\n https://www.youtube.com/watch?v=7TCqGslll-4 \\n https://github.com/ai-forever/Kandinsky-2', metadata={})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
