[27/07/23, 11:40:58 AM] GPU, Performance & Infra: ‎Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them.
[27/07/23, 11:40:58 AM] Nirant : ‎Nirant  created this group
[18/11/23, 8:46:46 AM] GPU, Performance & Infra: ‎You joined from the community
[18/11/23, 8:46:47 AM] ~ rohan~: ‎Your security code with ‪+91 75677 92829‬ changed.
[18/11/23, 8:46:47 AM] ~ Shubham Rao: ‎Your security code with ‪+91 94066 11134‬ changed.
[18/11/23, 8:46:48 AM] ~ D: ‎Your security code with ‪+91 72289 20167‬ changed.
[18/11/23, 8:46:48 AM] ~ Sushiee: ‎Your security code with ‪+91 77383 28985‬ changed.
[18/11/23, 8:46:48 AM] ‪+91 78993 34064‬: ‎Your security code with ‪+91 78993 34064‬ changed.
[18/11/23, 8:46:48 AM] ~ Riken Shah: ‎Your security code with ‪+91 79900 89211‬ changed.
[18/11/23, 8:46:48 AM] ‪+91 88060 52222‬: ‎Your security code with ‪+91 88060 52222‬ changed.
[18/11/23, 8:46:48 AM] ~ Karan: ‎Your security code with ‪+91 93403 05968‬ changed.
[18/11/23, 8:46:48 AM] ‪+91 97864 41234‬: ‎Your security code with ‪+91 97864 41234‬ changed.
[18/11/23, 8:46:48 AM] ‪+91 98341 69047‬: ‎Your security code with ‪+91 98341 69047‬ changed.
[18/11/23, 8:46:48 AM] ~ Pulkit: ‎Your security code with ‪+91 99078 56885‬ changed.
[18/11/23, 8:50:20 AM] ~ Vatsal: ‎~ Vatsal joined from the community
[18/11/23, 8:55:34 AM] ~ Sai Tej: ‎~ Sai Tej joined from the community
[18/11/23, 9:37:29 AM] ~ Shobhan: ‎~ Shobhan joined from the community
[18/11/23, 1:13:38 PM] ~ Sandeep: ‎~ Sandeep joined from the community
[18/11/23, 1:50:53 PM] ~ Kaustav: ‎Your security code with ‪+91 99016 56650‬ changed.
[18/11/23, 1:52:23 PM] Ojasvi Yadav: Anyone running a multi GPU PC?
[18/11/23, 1:55:55 PM] Ojasvi Yadav: If yes, what's the PSU size you're using?
[18/11/23, 1:56:23 PM] Dr. Pratik Desai: I have 2 3090s, and my 1000Watts burned out and had to replace with 1600Watts
[18/11/23, 1:56:43 PM] Dr. Pratik Desai: It has been working out nice since then
[18/11/23, 1:58:09 PM] Dr. Pratik Desai: If you have to use two 1600Watts, for more than 2 GPUs, you this to sync PWM https://www.amazon.com/dp/B09Q11WG4Z
[18/11/23, 1:59:41 PM] Dr. Pratik Desai: And this the board from my wish list that every one is using for 7-8 GPU rig https://www.asrockrack.com/general/productdetail.asp?Model=ROMED8-2T#Specifications
[18/11/23, 9:36:51 PM] ~ Sai Tej: ‎Nirant  removed ~ Sai Tej
[18/11/23, 9:41:55 PM] ~ Subbu Rama: ‎~ Subbu Rama joined from the community
‎[18/11/23, 10:08:16 PM] ~ Nikhil Satani: ‎image omitted
[18/11/23, 10:09:05 PM] ~ Nikhil Satani: Using two 3060s with 850 w gold plus
[18/11/23, 11:38:22 PM] ~ G Kuppuram: ‎~ G Kuppuram joined from the community
[19/11/23, 12:30:12 AM] ‪+91 88060 52222‬: ‎Your security code with ‪+91 88060 52222‬ changed.
[19/11/23, 6:54:48 AM] ~ Sachin Kalsi: ‎~ Sachin Kalsi joined from the community
[19/11/23, 8:08:17 AM] Zainab Bawa: ‎Zainab Bawa joined from the community
[19/11/23, 10:50:54 AM] Bhavya R: ‎Bhavya R joined from the community
[19/11/23, 12:54:39 PM] ~ Nayan Shah: https://www.substratus.ai/blog/calculating-gpu-memory-for-llm is this kind of formulae accurate while calculating the memory requirement , think quality of gpu also matters but anything else to consider while doing this
[19/11/23, 1:06:25 PM] Abhishek Mishra: So this thing is a bit nuanced here and almost everything that people add like flash attention 2, latest gen GPUs kind of change these calculations completely
[19/11/23, 1:07:11 PM] Abhishek Mishra: ‎This message was deleted.
[19/11/23, 1:07:48 PM] Abhishek Mishra: ‎This message was deleted.
[19/11/23, 1:10:40 PM] Abhishek Mishra: This article covers serving, i answered from perspective of fine tuning so my examples are off, sorry for that
[19/11/23, 1:50:26 PM] ~ Nayan Shah: Ok thanks for the input 🙌
[19/11/23, 2:01:39 PM] ~ Adarsh: You can check this out: tokentally.streamlit.app

I'll add logic for all famous inference engines too in coming days
[19/11/23, 2:02:01 PM] ~ Nayan Shah: 🫡
[19/11/23, 9:35:53 PM] RISHAV: ‎RISHAV joined from the community
[20/11/23, 7:29:28 AM] ~ Srinivas Reddy Aellala: ‎~ Srinivas Reddy Aellala joined from the community
[20/11/23, 11:00:27 AM] ~ Ajay: ‎~ Ajay joined from the community
[20/11/23, 11:12:02 AM] ~ Shreya Sajal: ‎~ Shreya Sajal joined from the community
[20/11/23, 1:42:19 PM] ~ Muhammad Hammad Khan: ‎~ Muhammad Hammad Khan joined from the community
[20/11/23, 1:46:30 PM] ~ MD Fazal: ‎~ MD Fazal joined from the community
[20/11/23, 2:28:51 PM] ~ Avi: ‎~ Avi joined from the community
[20/11/23, 6:26:40 PM] ~ Arko C | xylem.ai: Which are some of the GPUs that folks are using apart from V/A/H100?
[20/11/23, 6:26:58 PM] ~ Arko C | xylem.ai: For LLM deployments
[20/11/23, 6:35:34 PM] Nirant : 3090 and 4090 for ML models
[20/11/23, 6:44:39 PM] ~ Avi: L40/L40s are decent options for 7B/13B LLM deployments..
[20/11/23, 6:45:55 PM] ~ Arko C | xylem.ai: Got it. @917356725027 was also sharing the same. Thanks a lot!
[20/11/23, 6:47:15 PM] ~ Chirag Jain: A10 in "smaller" LLM cases is okay too
[20/11/23, 8:53:32 PM] ~ JASKAMAL KAINTH: Can someone share any resource where we have qps/tps  benchmark numbers (p95,p99, etc) for any basic inference tasks (summarization, q/a, etc) with these 7b, 13b models?
[20/11/23, 8:56:45 PM] ~ Chirag Jain: we have some numbers here on a bunch of gpus
https://blog.truefoundry.com/tag/llms-genai/

But I would still recommend running your own red lining benchmarking because this field moves fast and model inference engines get better every month

L40S should be roughly similar or slightly better than A100 ‎<This message was edited>
[20/11/23, 8:58:08 PM] ~ Chirag Jain: summarisation would be the hardest to optimise
It contains both large number of prefill tokens and decent amount of output tokens
[21/11/23, 12:49:27 AM] ~ Lakshya: ‎Nirant  removed ~ Lakshya
[21/11/23, 12:54:10 AM] ~ Sharwon: ‎Nirant  removed ~ Sharwon
[21/11/23, 12:54:42 AM] ~ Chinmay Shah: ‎~ Chinmay Shah joined from the community
[21/11/23, 12:57:02 AM] ~ Sathvik: ‎Nirant  removed ~ Sathvik
[21/11/23, 1:01:29 AM] ~ Anurag Singh: ‎Nirant  removed ~ Anurag Singh
[21/11/23, 1:11:30 AM] ~ Enrique Ferrao: ‎Nirant  removed ~ Enrique Ferrao
[21/11/23, 4:34:13 PM] ~ Shikhil Kumar Gupta: ‎Your security code with ‪+91 6361 299 331‬ changed.
[22/11/23, 10:15:25 AM] ~ Anindyadeep Sannigrahi: This is quite helpful, I also watched the video on that... Makes me remember of the physics formula...
[22/11/23, 10:16:28 AM] ~ Nayan Shah: Can u share the video .
[22/11/23, 10:16:39 AM] ~ Anindyadeep Sannigrahi: Yeahh that's true, benchmarking is very dependent on the type of gpu, the framework, the backend (like rust or CPP ....) And the transformer type... (Like one using flash ATTN or one using flash ATTN v2)
[22/11/23, 10:17:01 AM] ~ Anindyadeep Sannigrahi: Not sure whether that is present on youtube
[22/11/23, 12:06:38 PM] ‪+91 85939 07553‬: ‎‪+91 85939 07553‬ joined from the community
[22/11/23, 5:31:39 PM] ~ Rahul: ‎~ Rahul joined from the community
[22/11/23, 5:43:35 PM] ~ Debdoot: ‎~ Debdoot joined from the community
[22/11/23, 9:25:44 PM] ~ Aashraya Sachdeva: ‎~ Aashraya Sachdeva joined from the community
‎[23/11/23, 12:32:57 AM] Ojasvi Yadav: ‎image omitted
[23/11/23, 12:33:00 AM] ~ Aayush Mudgal: ‎Your security code with ‪+1 (347) 279‑3767‬ changed.
‎[23/11/23, 12:33:59 AM] Ojasvi Yadav: ‎image omitted
[23/11/23, 12:34:33 AM] Ojasvi Yadav: Would appreciate if someone could share pictures of their dual GPU setups
[23/11/23, 1:35:00 AM] ~ rohan~: The only issue I could imagine is it could lead to higher temperatures and cause throttling. Specially the GPU on the top (I can see its intakes are almost fully covered)

Try running FurMark (https://geeks3d.com/furmark/) for 15-20mins and monitor the temps and see how hot they get. Anywhere in between 60-90 is okay. If you are on the higher end, check out the GPU clocks also to make sure its not throttling
[23/11/23, 9:15:39 AM] ~ Yash: No it’s the usual but you will need a cabinet front fan to push air in and dissipate the heat between
[23/11/23, 9:15:54 AM] ~ Yash: See the photos in this for reference https://pcpartpicker.com/b/VHQqqs
[23/11/23, 9:18:24 AM] ~ Yash: Additionally if anyone is looking for GPU (non nvlink) here’s a pretty solid quadro card with a solid deal
https://elitehubs.com/products/nvidia-quadro-rtx-a2000-12gb-workstation-graphics-card
‎[23/11/23, 9:37:06 AM] Ojasvi Yadav: ‎image omitted
[23/11/23, 9:54:48 AM] ~ Shagun Sood: ‎Your security code with ‪+91 94803 91910‬ changed.
[23/11/23, 1:32:46 PM] ~ Amartya | CodeAnt(YC W24): ‎~ Amartya | CodeAnt(YC W24) joined from the community
[23/11/23, 4:50:08 PM] ~ sahir: ‎Your security code with ‪+91 90210 77561‬ changed.
[23/11/23, 5:18:48 PM] ~ dhruv: Long shot but anyone here from London/UK who has built a GPU rig? 
Looking for any recommended places to buy, if there’s any pre-built options available
[23/11/23, 6:38:37 PM] ~ chetanya rastogi: ‎Your security code with ‪+1 (650) 334‑6780‬ changed.
[23/11/23, 6:51:44 PM] ~ sahir: ‎Your security code with ‪+91 90210 77561‬ changed.
[24/11/23, 12:33:54 PM] ~ Umang Keshri: ‎~ Umang Keshri joined from the community
[24/11/23, 7:09:32 PM] ~ Bharath Venkatesh: ‎~ Bharath Venkatesh joined from the community
[25/11/23, 1:19:53 AM] ~ Ankit Sharma: ‎~ Ankit Sharma joined from the community
[25/11/23, 10:00:59 AM] ~ Nishkarsh | usefindr.com: ‎~ Nishkarsh | usefindr.com joined from the community
[25/11/23, 2:09:14 PM] Lavish autoGPT: ‎Your security code with Lavish autoGPT changed.
[26/11/23, 9:02:59 PM] ~ Pulkit: ‎Your security code with ‪+91 99078 56885‬ changed.
[26/11/23, 10:44:10 PM] ~ Kartikeya Bhardwaj: ‎~ Kartikeya Bhardwaj joined from the community
[27/11/23, 1:34:59 AM] ~ Apoorv Saxena: ‎~ Apoorv Saxena joined from the community
[27/11/23, 5:15:23 PM] ~ Kaustav: ‎Your security code with ‪+91 99016 56650‬ changed.
[28/11/23, 7:00:26 AM] ~ Manu Hegde: ‎Your security code with ‪+1 (425) 215‑6366‬ changed.
[28/11/23, 9:41:50 PM] ~ Manasi: ‎~ Manasi joined from the community
[29/11/23, 7:45:37 AM] ~ Aditya Mandke: ‎Your security code with ‪+1 (858) 228‑7027‬ changed.
[29/11/23, 3:03:08 AM] ~ Harsh Gupta: ‎~ Harsh Gupta joined from the community
[29/11/23, 12:16:31 PM] ~ Akshat Khare: ‎~ Akshat Khare joined from the community
[29/11/23, 12:23:53 PM] ~ Akshat Khare: How to get a GPU cloud solution with docker command access?
Easy way wanted. I use Jarvislabs and love it but can't build docker with saved weights and create replicate demos 🥲
[29/11/23, 2:11:57 PM] ~ Badal: Haven't used Jarvislabs but can't you just install docker and push?
[29/11/23, 2:16:19 PM] ~ Akshat Khare: i dont think its possible to install docker in a container already. have you tried doing it?
[29/11/23, 2:19:45 PM] ~ Sushant: not sure on jarvislabs

but docker inside docker is def possible....mostly this is done in jenkins deployments to cloud..
it should only be used as. last resort though..

and your outer container should have all permissions assigned...
[29/11/23, 2:20:25 PM] ~ Badal: Yeah best would be downloading weights on your own local machine I guess
[29/11/23, 2:20:34 PM] ~ Akshat Khare: ok sounds intimidating
[29/11/23, 2:21:33 PM] ~ Akshat Khare: i have a mac with 256 gb, not much disk to even store weights, i will get my hands on a bare metal machine with disk too soon maybe. thanks for the advice. ‎<This message was edited>
[29/11/23, 4:13:57 PM] ~ Kesava Reddy ☁️🧠🤝🛜: Hi Akshat,

On our E2E Cloud..TIR platform we have this by default.

https://www.e2enetworks.com/

https://docs.e2enetworks.com/AI_ML/tutorials/custom_inference.html
[29/11/23, 4:30:19 PM] ~ Akshat Khare: Wow Kesava
That might be exactly what I need
[29/11/23, 4:32:02 PM] ~ Akshat Khare: Let me try this out, if you have some free trial credits or such, would be icing on the cake as I am broke 🫡 (kidding)
[29/11/23, 4:34:00 PM] ~ Kesava Reddy ☁️🧠🤝🛜: Sure... Will give you credits for a couple days to test .. Requesting the account manager to get in touch with you
[29/11/23, 5:33:49 PM] Aditya Agrawal SU: ‎Your security code with Aditya Agrawal SU changed.
[29/11/23, 6:11:53 PM] ~ Shikhil Kumar Gupta: ‎Your security code with ‪+91 6361 299 331‬ changed.
[29/11/23, 7:15:03 PM] Kartik Mandaville Albus: ‎Kartik Mandaville Albus joined from the community
[29/11/23, 7:15:32 PM] Kartik Mandaville Albus: Pinecone coming up with a serverless infra! Makes it easier to scale now
[30/11/23, 7:27:14 AM] ~ Shikhil Kumar Gupta: ‎Your security code with ‪+91 6361 299 331‬ changed.
[30/11/23, 5:57:22 PM] ~ anshul: ‎Your security code with ‪+91 99206 34169‬ changed.
[01/12/23, 10:43:16 AM] ~ Abhinav Jain: ‎Your security code with ‪+91 98338 06825‬ changed.
[01/12/23, 2:21:31 PM] ~ Shubham: ‎~ Shubham joined from the community
[01/12/23, 2:29:34 PM] ~ Ashwin: ‎Your security code with ‪+91 86607 99753‬ changed.
[02/12/23, 10:27:39 PM] ~ Maaz Karim: ‎~ Maaz Karim joined from the community
[02/12/23, 11:06:56 PM] ~ Tarun: ‎Your security code with ‪+1 (510) 992‑0614‬ changed.
[03/12/23, 3:26:41 PM] ~ Shouvik Ghosh Roy: ‎Your security code with ‪+91 98806 03437‬ changed.
[04/12/23, 11:31:09 AM] ~ Shaurya Gupta: ‎~ Shaurya Gupta joined from the community
[04/12/23, 11:47:52 PM] ~ Utkarsh Saxena: ‎Your security code with ‪+91 76249 40989‬ changed.
[05/12/23, 6:34:48 AM] ~ Diwank: ‎Your security code with ‪+91 81302 87910‬ changed.
[05/12/23, 6:00:21 PM] ~ Amit Bhor: ‎Your security code with ‪+91 99700 65570‬ changed.
[06/12/23, 7:45:03 AM] ~ Bhaskar: ‎~ Bhaskar joined from the community
[06/12/23, 10:55:22 PM] ~ Aakash Kambuj: ‎Your security code with ‪+1 (425) 802‑0328‬ changed.
[06/12/23, 11:21:46 PM] ~ Rohan: ‎~ Rohan joined from the community
[06/12/23, 11:38:06 PM] ~ prasanna kumar: ‎~ prasanna kumar joined from the community
[07/12/23, 12:24:04 AM] ~ Akash Singh: Amd new gpu is competitive with nvidia hgx. More support to DL Frameworks.

https://www.youtube.com/live/tfSZqjxsr0M?si=7QWKzUYXNRtp9ucm
[07/12/23, 11:26:21 AM] ~ cGh: They beat Nvidia in inference. 
Not training
[07/12/23, 11:59:17 AM] ~ Rishav Chandra Varma: ‎~ Rishav Chandra Varma joined from the community
[07/12/23, 1:23:27 PM] Nirant : Inference is the right place to begin for this for the challenger GPU. The training ecosystem needs more operations, drivers and middleware work.
[07/12/23, 3:48:44 PM] ~ Ananth Radhakrishnan: ‎~ Ananth Radhakrishnan joined from the community
[07/12/23, 11:03:36 PM] ~ Prashanth Harshangi: ‎~ Prashanth Harshangi joined from the community
[08/12/23, 6:11:00 AM] ~ Sourabh: ‎Your security code with ‪+91 89511 65425‬ changed.
[08/12/23, 10:12:54 AM] ~ Yogesh Sangtani: ‎~ Yogesh Sangtani joined from the community
[08/12/23, 10:38:09 PM] ~ Pravar: ‎~ Pravar joined from the community
‎[08/12/23, 10:51:21 PM] ~ Adarsh: ‎image omitted
[08/12/23, 10:54:15 PM] ~ Prashanth Harshangi: Does this help? https://docs.bitnami.com/azure/how-to/increase-disk-space-azure/
‎[08/12/23, 10:55:44 PM] ~ Adarsh: ‎image omitted
[08/12/23, 10:57:12 PM] ~ Prashanth Harshangi: The option should appear at the compute level (not studio level), so once the vm has been provisioned.
[08/12/23, 10:59:55 PM] ~ Adarsh: The disk size is set even at the compute level😕
[08/12/23, 11:00:15 PM] ~ Adarsh: That's the issue it's termed as a compute instance and not a vm
[08/12/23, 11:03:26 PM] ~ Prashanth Harshangi: Sorry can't help further. Not too familiar with ml studio.
[08/12/23, 11:18:12 PM] ~ Adarsh: Np thank you!
[09/12/23, 7:08:34 PM] ~ Priyesh Srivastava: https://x.com/tsengalb99/status/1733222467953422702?s=20
[09/12/23, 7:08:47 PM] ~ Priyesh Srivastava: best 24GB GPU pls 😂
[09/12/23, 7:27:36 PM] Pratyush Choudhury Together Fund: Yeah, just finished this - some really cool stuff
[09/12/23, 11:01:00 PM] ~ Akash: ‎Your security code with ‪+44 7341 544418‬ changed.
[10/12/23, 12:19:52 AM] Sthit Validity: ‎Sthit Validity joined from the community
[10/12/23, 11:07:43 AM] Sasank Chilamkurthy: Anybody got a google coral board or chip? https://coral.ai/

Not able to find a link to order it anywhere 🙁
[10/12/23, 9:08:08 PM] ~ Sri Krishna: https://www.amazon.in/Google-Coral-Accelerator-coprocessor-Raspberry/dp/B07R53D12W ?
[10/12/23, 9:09:13 PM] Sasank Chilamkurthy: Thanks!
[10/12/23, 9:36:30 PM] Sthit Validity: Trying to understand so what sort of parameter range of llms would this support well ? Something like mistral 7b or more in the sub 1b range ?
[10/12/23, 9:53:09 PM] Arvind Nagaraj | Mitra AI: I have one from 2019. It's just 1 TPU core. Did some tflite projects back in the day. Not worth it.
[10/12/23, 9:55:11 PM] ~ G Kuppuram: https://coral.ai/docs/accelerator/get-started/#next-steps
[10/12/23, 9:56:07 PM] ~ Naveen Pandey: I have it as well. Don’t think it would be able to run LLMs. Experience running 5-10 million parameter model like Yolo wasn’t too promising from a cost benefit perspective. Haven’t tried LLMs yet though.
[10/12/23, 10:04:10 PM] ~ Ria Mirchandani: ‎Your security code with ‪+91 98337 71797‬ changed.
[10/12/23, 10:12:09 PM] ~ Ria Mirchandani: ‎Your security code with ‪+91 98337 71797‬ changed.
[10/12/23, 10:27:33 PM] ~ Ria Mirchandani: ‎Your security code with ‪+91 98337 71797‬ changed.
[10/12/23, 11:58:33 PM] Sthit Validity: Thanks for the insights.
[10/12/23, 11:58:37 PM] Sthit Validity: Makes sense
[11/12/23, 3:31:03 AM] Sasank Chilamkurthy: I just checked the spec, it’s ultra small. has like 8MB on chip ram. Not sure about external memory interface.
‎[11/12/23, 4:22:35 AM] Sasank Chilamkurthy: ‎image omitted
[11/12/23, 8:21:27 AM] Sthit Validity: Yup tempered my excitement initially and decided against it for now. Doesn't fit my use case. Thanks a lot folks. Appreciate it a lot 🫡
[11/12/23, 9:26:25 AM] ~ Kishore M R: What is the use case you wanted to try?
[11/12/23, 9:40:05 AM] Sthit Validity: Swarm of Corals for LLMs :)
[11/12/23, 9:40:18 AM] Sthit Validity: But to much PCIE stop gappjng I feel atleast for now
[11/12/23, 9:40:24 AM] Sthit Validity: To be viable
[11/12/23, 9:40:35 AM] Sthit Validity: Not sure though. Perhaps I haven't looked into it enough
[11/12/23, 9:47:16 AM] Sasank Chilamkurthy: I'm considering building a machine with non-nvidia GPUs but will make it ultra cheap. Before I market it, I want beta users who will want to use it for training/inference. I will want to polish the software experience taking feedback from the beta user.
[11/12/23, 10:09:45 AM] ~ Kishore M R: Software experience is the key.  Training is a different beast and I am not sure if we can neglect NVIDIA. May be AMD some day. But for inference especially some  usecases like standalone POS  or a place with no need for a PC, single board computers could do the job for some computer vision or lighter models already. Other than NVIDIA jetson series which come with onchip GPUs,  Intel NPUs, TI beaglebone AI x64  and rockchip are decent options for common  computer vision use cases. orange pi 5  has impressive hardware  with octacore CPU and 32GB RAM  for the price point and form factor. Where it actually excites is the M.2 key interface that  supports PCIE adapters. With proper power and thermal management we can go for a full size GPU on it . Some youtube videos show similar ones on different boards. I am planning to experiment this setup. ‎<This message was edited>
[11/12/23, 10:17:01 AM] Sasank Chilamkurthy: Yep, software experience is the key! Think of my machine as your personal desktop/pc. I plan to use intel gpus.
[11/12/23, 10:19:48 AM] Sasank Chilamkurthy: In my experience for enterprise inference use cases, laptops are great. When I was CTO at Qure.ai, I built hardware-software combo to ship along with xray systems for on machine xray AI. Tried NUC, Jetson and others - laptop was the best form factor. ‎<This message was edited>
[11/12/23, 10:24:00 AM] ~ Naveen Pandey: This is quite underrated. I was also part of an org where this was a key strategy and served us really well. Just works without wasting a lot of time on optimisation early on.
[11/12/23, 10:24:31 AM] Sasank Chilamkurthy: Interesting. What was the problem statement
[11/12/23, 10:24:49 AM] ~ Naveen Pandey: AV Mapping
[11/12/23, 10:28:59 AM] ~ Kishore M R: My experience is in automotive ECUs Semi autonomous L2+ that have harsh requirements. Very early development setup with laptops do work in lab setup. But even for early  testing on vehicle  , actual hardware is the norm ‎<This message was edited>
[11/12/23, 10:53:56 AM] Sasank Chilamkurthy: Oh yeah automative has crazy requirements I heard.
[11/12/23, 10:54:12 AM] Sasank Chilamkurthy: I heard that half the cores will duplicating the job of other half to verify the correctness
[11/12/23, 11:17:34 AM] Sasank Chilamkurthy: Anybody got experience building crypto GPU rigs? Need some guidance on pcie raisers etc ‎<This message was edited>
[11/12/23, 1:27:06 PM] ~ Adarsh: Try AMD if you can. rocm still isnt as good as cuda. theres a lot of libraries that use cuda rn(including flash-attn)
[11/12/23, 1:27:27 PM] ~ Adarsh: you can check out lamini: https://www.lamini.ai/blog/lamini-llm-finetuning-on-amd-rocm-a-technical-recipe
[11/12/23, 1:27:32 PM] Sasank Chilamkurthy: AMD cards are not as cheap as Intel’s
[11/12/23, 1:27:41 PM] ~ Adarsh: they got some mad throughputs using amd
[11/12/23, 1:28:02 PM] Sasank Chilamkurthy: BTW I got llama stuff running on a steamdeck which uses AMD APU (basically integrated gpu)
[11/12/23, 1:29:34 PM] Sasank Chilamkurthy: I also worked with pytorch team few years back to add rocm support to it 😉
[11/12/23, 1:30:23 PM] ~ Adarsh: Legend
‎[11/12/23, 1:30:30 PM] Sasank Chilamkurthy: ‎video omitted
[11/12/23, 1:35:55 PM] Sasank Chilamkurthy: Added text to speech and speech to text to this. Hoping to demo this as sort of jarvis haha.
[11/12/23, 1:36:20 PM] Sasank Chilamkurthy: Going to that GPAI conference in delhi tomm: https://gpai.ai/
[11/12/23, 1:39:47 PM] ~ Sandeep Srinivasa: how did u do this ? is there any drivers to make accelerated llama work on amd ?
[11/12/23, 1:43:59 PM] Sasank Chilamkurthy: here you go: https://swethatanamala.substack.com/p/how-i-ran-llms-on-steam-deck-handheld
[11/12/23, 1:44:06 PM] Sasank Chilamkurthy: used distrobox and rocm drivers
[11/12/23, 2:24:30 PM] ~ Heerthi Raja H - AI/ML/CV: ‎~ Heerthi Raja H - AI/ML/CV joined from the community
[11/12/23, 6:42:48 PM] ~ Kishore M R: They are called lockstep cores which will parallely verify each instruction step. That level of crazy stuff is only for hard real time functions like engine control braking etc. Various levels of soft realtime and non realtime functions like infotainment exist. Major  challenge in the industry is to safety certify the FSD part that involves processing huge camera and sensor streams in real-time certified way beyond the deep learning challenges.
‎[12/12/23, 12:45:38 AM] Sasank Chilamkurthy: ‎video omitted
[12/12/23, 12:45:55 AM] Sasank Chilamkurthy: My humble demo 🥲
[12/12/23, 12:50:18 AM] Sasank Chilamkurthy: But the model hallucinated 😭
[13/12/23, 11:42:48 AM] ~ Naveen Pandey: ‎Your security code with ‪+91 96633 97033‬ changed.
[14/12/23, 12:35:45 PM] Sasank Chilamkurthy: What do you folks think of so called AI PCs? What kind of new applications will work on these?
[14/12/23, 12:42:52 PM] Nirant : Have a reference link? I'm inclined to believe that by 2025 Q1, Apple will have the best AI PC for Advanced Users — bringing devs and workers both back into the fold. Closely followed by an ASUS or similar, Taiwanese integrated player who can play on end to end integration with Windows.
[14/12/23, 12:43:22 PM] Nirant : For context, I consider modern M3s and most Apple Displays as built for advanced users eg. designers, video editors etc ‎<This message was edited>
[14/12/23, 12:59:31 PM] Sasank Chilamkurthy: Check this out https://www.forbes.com/sites/patrickmoorhead/2023/12/13/what-to-expect-at-intel-ai-event-a-discussion-with-ceo-pat-gelsinger/?sh=5bfad7255401
[14/12/23, 12:59:50 PM] Sasank Chilamkurthy: Intel's AI everywhere event
[14/12/23, 1:00:04 PM] Sasank Chilamkurthy: They're putting arc gpus on their CPUs using packaging tech
[14/12/23, 1:01:49 PM] Sasank Chilamkurthy: But my question what new applications will such AI PCs create? Why will people not simply use cloud and use AI PCs?
[14/12/23, 1:14:23 PM] ~ Kishore M R: Chiplet based HW is the hotcake in system design. Everyone wants to dethrone NVIDIA of its position today and bets on something. The separate accelerators and NPUs are one. Finally,the integration , efficiency and usability software that brings everything together will speak IMO. Havent experienced Openvino which is still a Intel specific stuff
[14/12/23, 1:17:12 PM] Yash Bonde: The innovations in AI PC will also trickle down to massive applications on the edge.
[14/12/23, 1:18:43 PM] Yash Bonde: Think analysing TBs worth of sensor data that often cannot be put directly on the DBs on cloud.
[14/12/23, 1:22:09 PM] Sasank Chilamkurthy: Yes packaging is the hotcake. The main reason being bandwidth has not caught up with flops. The returns in scaling flops are lower than scaling the bandwidth. Even nvidia's server gpus are marvel of 3d packaging tech.
[14/12/23, 1:23:52 PM] Sasank Chilamkurthy: Who collects such massive data at personal level?
[14/12/23, 1:24:31 PM] Sasank Chilamkurthy: What are these AI applications that would be run on the edge. Why should they be run on edge instead of cloud
[14/12/23, 1:24:56 PM] Sasank Chilamkurthy: PS: I'm debating for the sake of it. I believe in edge myself.
[14/12/23, 1:24:59 PM] Yash Bonde: Not on personal level. I’m saying innovations will trickle down in different places.
[14/12/23, 1:26:07 PM] Yash Bonde: I have a few thoughts:
- ocean sensors
- ⁠soil sensors
- ⁠offline / off grid

Often things that cannot be captured by merely looking at it (vs. 🛰️ ‎<This message was edited>
[14/12/23, 1:26:31 PM] Sasank Chilamkurthy: Fairly niche though.
[14/12/23, 1:26:58 PM] ~ Vedant: +1 on this question. been chatting with chatGPT, works perfectly fine. 

to narrow down the context of this debate - for which consumer use-cases does it make sense to run models on the device? 

apart from privacy.
[14/12/23, 1:27:21 PM] Sasank Chilamkurthy: What will people like your mother benefit from having AI on her PC?
[14/12/23, 1:29:50 PM] Yash Bonde: IMO only if OS makers do cool things would it make sense. Otherwise there will always be a glass ceiling.

A better Siri is more useful than 100s of Copilot repos on GitHub.
[14/12/23, 1:30:43 PM] Sasank Chilamkurthy: Let's say OS makers are able to innovate or new ones come up. What will be those useful/important applications?
[14/12/23, 1:32:14 PM] Sasank Chilamkurthy: I think in a sense, my question is not too different from what apple/msft asked themselves when they started. Everyone knew computers were gonna change things. Computers back in those ideas were physically not too different from today's super computers in terms of size. People used to access them using remote terminals. ‎<This message was edited>
[14/12/23, 1:34:27 PM] Yash Bonde: I think AI by itself it pretty limited in its use. The ways it will be integrated to other applications and tools will be waayyyyy more powerful.

Imagine GTA-VII where every NPC is an actual AI. Regressing a bit from the subject of the group though 🥲
[14/12/23, 1:35:29 PM] Sasank Chilamkurthy: May be let's start the discussion in main group
[14/12/23, 3:48:47 PM] ~ Chirag Singla: ‎~ Chirag Singla joined from the community
[14/12/23, 4:57:01 PM] Nirant : Apple be winning with M2 already for small models
https://twitter.com/tbenst/status/1734990690432733650
‎[14/12/23, 5:01:51 PM] ~ rohan~: ‎image omitted
[14/12/23, 5:02:20 PM] ~ rohan~: 4090 can still do it in 8s when optimised 🔥
[15/12/23, 12:09:46 PM] ~ Tara Lodh: ‎~ Tara Lodh joined from the community
[15/12/23, 12:31:43 PM] ~ Sarthak Gupta: ‎~ Sarthak Gupta joined from the community
[15/12/23, 1:37:51 PM] Arvind Nagaraj | Mitra AI: I know this group's focus is silicon gains. But faster-whisper with ctranslate2 is something.
[15/12/23, 3:38:30 PM] ~ Shekar Ramachandran: ‎~ Shekar Ramachandran joined from the community
‎[15/12/23, 6:39:25 PM] Sasank Chilamkurthy: ‎image omitted
[15/12/23, 6:39:30 PM] Sasank Chilamkurthy: https://youtu.be/EP1x_9LMp50?t=2898
[16/12/23, 10:30:01 PM] ~ Shekar Ramachandran: ‎Nirant  removed ~ Shekar Ramachandran
[16/12/23, 10:30:08 PM] ~ Sravan Avvaru: ‎Nirant  removed ~ Sravan Avvaru
[16/12/23, 10:51:21 PM] ~ Pravar: ‎Nirant  removed ~ Pravar
[16/12/23, 10:53:55 PM] ~ Yash Khandelwal: ‎Nirant  removed ~ Yash Khandelwal
[16/12/23, 10:54:09 PM] ~ prabu: ‎Nirant  removed ~ prabu
[16/12/23, 10:54:15 PM] ~ Saketh BSV: ‎Nirant  removed ~ Saketh BSV
[16/12/23, 10:54:27 PM] ~ Shobhit: ‎Nirant  removed ~ Shobhit
[16/12/23, 10:57:48 PM] ~ Gayatri: ‎Nirant  removed ~ Gayatri
[16/12/23, 10:58:34 PM] ~ Vishal: ‎Nirant  removed ~ Vishal
[16/12/23, 10:58:51 PM] ~ Kuldeep Saxena: ‎Nirant  removed ~ Kuldeep Saxena
[16/12/23, 11:03:28 PM] ~ Praveen Sridhar: ‎Nirant  removed ~ Praveen Sridhar
[16/12/23, 11:14:44 PM] ~ Shekar Ramachandran: ‎Nirant  added ~ Shekar Ramachandran
[16/12/23, 11:32:29 PM] ~ Ashish Patel: https://roboticsbiz.com/ai-hardware-what-they-are-and-why-they-matter-in-2020/
[17/12/23, 12:14:34 AM] ~ Ayushi: ‎~ Ayushi joined from the community
[17/12/23, 12:31:26 AM] Sasank Chilamkurthy: People interested in learning about compilers, go through my tutorial series: https://twitter.com/sasank51/status/1736098596045599062
[17/12/23, 2:23:02 AM] ~ Ashvini: ‎~ Ashvini joined from the community
[17/12/23, 12:16:20 PM] ~ Subhojit Basu: ‎~ Subhojit Basu joined from the community
[17/12/23, 12:56:19 PM] ~ Pulkit: Thanks this is very helpful
[17/12/23, 1:05:01 PM] Sasank Chilamkurthy: Stay tuned for more updates!
[18/12/23, 12:56:06 AM] Sasank Chilamkurthy: Next part is out: this one is about creating (call) stack
 https://twitter.com/sasank51/status/1736467068298633686?t=XNmVEN7hQoj7snKqKU1Spw&s=19
[18/12/23, 9:28:41 AM] ~ Sairam Chitreddy: ‎~ Sairam Chitreddy joined from the community
[18/12/23, 6:41:43 PM] ~ Krishna: https://www.linkedin.com/posts/gregory-diamos-1a8b9083_if-you-are-an-ai-startup-blocked-on-gpus-activity-7142379701146181632-TptP?utm_source=share&utm_medium=member_ios

“At Lamini, we figured out how to use AMD GPUs, which gives us a relatively large supply compared to the rest of the market.”

Might be interesting for folks here
[18/12/23, 8:54:39 PM] Sasank Chilamkurthy: Next two parts of my scheme compiler tutorials are out: https://twitter.com/sasank51/status/1736769236796858802
[19/12/23, 7:34:27 PM] ~ Ramakrishnan Raman: ‎~ Ramakrishnan Raman joined from the community
[19/12/23, 8:58:15 PM] Sasank Chilamkurthy: Next in compilers series: https://twitter.com/sasank51/status/1737132414756139299

Hope you’re finding these tutorials useful!
[19/12/23, 9:33:43 PM] Adithya Kannada LLM: This is really cool stuff man
Thanks a lot for this, learning from all these blogs
[19/12/23, 9:35:51 PM] Sasank Chilamkurthy: Thanks! Glad you're leaning from this. Keeps me motivated 😁
[19/12/23, 10:27:02 PM] ~ Adarsh: yeah a lot like those GPU blogs that came out! Great stuff!
[19/12/23, 10:27:48 PM] ~ Pulkit: Hey can you share the link for these blogs?
[19/12/23, 10:28:28 PM] ~ Adarsh: https://codeconfessions.substack.com/p/gpu-computing
[19/12/23, 10:28:45 PM] ~ Adarsh: by Abhinav Upadhyay
[19/12/23, 10:29:17 PM] ~ Pulkit: Thanks!
[19/12/23, 10:29:51 PM] ~ Pulkit: Ive also been reading rest of the articles on the blog, really nice work!
[19/12/23, 11:02:33 PM] ~ Kshiteej: These are really interesting and helpful in demystifying the magic of a programming language!

Also, this (free) book called Crafting Interpreters is a great resource for anyone interested in learning about implementing a language. It dives from deciding the grammar to implementing all the features like name resolution, call stack, scoping and inheritance step-by-step. It also breaks the implementation into several passes which implements the relevant behaviour. And at the end you have an interpreter for your language.

Ref: https://craftinginterpreters.com
[19/12/23, 11:04:29 PM] Sasank Chilamkurthy: You should read this: https://norvig.com/lispy.html

In 10/20 mins, you can write your own language with python. By Peter norvig, legendary AI programmer
[19/12/23, 11:17:48 PM] ~ Pulkit: Any more recommendations like these? Looking for articles around pytorch and jax
[20/12/23, 1:12:35 AM] Sasank Chilamkurthy: One of the best blogs on pytorch internals/architecture: http://blog.ezyang.com/2019/05/pytorch-internals/

Written by one of authors of pytorch
[20/12/23, 1:12:57 AM] Sasank Chilamkurthy: 👆
‎[20/12/23, 4:38:39 AM] ~ Ayush Yadav | SuperHeroAI: ‎image omitted
[20/12/23, 11:29:40 AM] ~ rohit: i want a rtx 30 or 40 series. who’s the best plug for us in india here?
[20/12/23, 11:30:25 AM] ~ rohit: I’ll probably bulk buy the 3090s
[20/12/23, 11:30:41 AM] ~ rohit: thanks for the help in advance 😀
[20/12/23, 11:34:51 AM] ~ rohit: on premise *
[20/12/23, 11:48:24 AM] Sasank Chilamkurthy: Dmed you. Building a product related to this.
[20/12/23, 12:13:32 PM] Nirant : NVIDIA if you can do $100K order?

cc @917407651462
[20/12/23, 12:54:32 PM] ~ Amit Bhor: https://x.com/omarsar0/status/1737168751668187229?t=FVEVmUtul7pyJK-6sUz0pQ&s=08
[20/12/23, 1:02:37 PM] Sasank Chilamkurthy: Powerinfer thing is pretty cool! Is the repo open?
[20/12/23, 1:03:39 PM] Sasank Chilamkurthy: Should be fun benchmarking on mlx on apple
[20/12/23, 2:08:02 PM] Sasank Chilamkurthy: I’m creating a PR for powerinfra along with my comments. Will try to explain the diff. Feel free to participate in the PR discussion. 

https://github.com/ggerganov/llama.cpp/pull/4543
[20/12/23, 2:08:22 PM] Sasank Chilamkurthy: please contribute to this with your comments. will be a good learning experience
[20/12/23, 2:09:08 PM] Sasank Chilamkurthy: Also hopefully it'll be an excercise in how opensource development works.
[20/12/23, 9:01:59 PM] Bharat Shetty: https://github.com/SJTU-IPADS/PowerInfer anyone tried this ?
[20/12/23, 9:02:12 PM] Bharat Shetty: yeah check above
[20/12/23, 9:02:57 PM] Sasank Chilamkurthy: I raised a PR to merge this to llama.cpp. Check my review link for comments
[20/12/23, 9:05:19 PM] Sasank Chilamkurthy: some details here: https://github.com/ggerganov/llama.cpp/discussions/4534#discussioncomment-7900305

original author responded
[21/12/23, 12:06:51 AM] ~ Jeet Kanjani: ‎~ Jeet Kanjani joined from the community
[21/12/23, 12:11:51 PM] ~ Ashok: ‎Your security code with ‪+91 89514 61917‬ changed.
[21/12/23, 8:45:39 PM] ~ Subhojit Basu: Anyone writing Gpu kernels ?
[21/12/23, 10:56:46 PM] ~ Akshat Khare: ‎Your security code with ‪+91 87706 15900‬ changed.
[22/12/23, 11:00:45 AM] ~ Ashok: ‎Your security code with ‪+91 89514 61917‬ changed.
[22/12/23, 12:35:57 PM] ~ Yash Pandya: ‎Your security code with ‪+91 77740 34769‬ changed.
[22/12/23, 12:35:57 PM] ~ Yash Pandya: ‎Your security code with ‪+91 77740 34769‬ changed.
[22/12/23, 12:40:05 PM] Nirant : ‎Nirant  changed the group name to “GPUs, Local LLaMa, Infrastructure”
[22/12/23, 1:12:42 PM] ~ Naresh: ‎~ Naresh joined from the community
[22/12/23, 5:09:38 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[23/12/23, 8:07:18 AM] ~ Aravindh: ‎~ Aravindh joined from the community
[23/12/23, 2:12:01 PM] Adithya Kannada LLM: ‎Your security code with Adithya Kannada LLM changed.
[23/12/23, 11:58:53 PM] Sasank Chilamkurthy: Finally my next blog in scheme compiler series is out: https://twitter.com/sasank51/status/1738627030709916015

Added most important feature of any lisp: i.e lambdas
[24/12/23, 6:15:07 AM] ~ Anshul: ‎Your security code with ‪+91 99702 04619‬ changed.
[24/12/23, 11:09:54 AM] ~ Harmandeep Singh Matharu: ‎~ Harmandeep Singh Matharu joined from the community
[24/12/23, 12:40:05 PM] ~ Darshil Jariwala: ‎~ Darshil Jariwala joined from the community
[25/12/23, 9:28:52 PM] ~ Akshat Khare: ‎Your security code with ‪+91 87706 15900‬ changed.
[26/12/23, 4:31:29 PM] ~ Vrushank | Portkey: ‎Your security code with ‪+91 97008 88848‬ changed.
[26/12/23, 6:03:38 PM] ~ Ambarish Ganguly: ‎~ Ambarish Ganguly joined from the community
[27/12/23, 12:27:15 AM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[27/12/23, 4:26:42 PM] ~ Onkar Mishra: ‎Your security code with ‪+91 99998 25927‬ changed.
[27/12/23, 10:22:15 PM] ~ Shekar Ramachandran: ‎This message was deleted.
[27/12/23, 10:22:16 PM] ~ Shekar Ramachandran: ‎This message was deleted.
[27/12/23, 10:22:16 PM] ~ Shekar Ramachandran: ‎This message was deleted.
[27/12/23, 10:22:17 PM] ~ Shekar Ramachandran: ‎This message was deleted.
[27/12/23, 10:22:18 PM] ~ Shekar Ramachandran: ‎This message was deleted.
[27/12/23, 10:23:02 PM] Ravi Theja: have you explored portkey?
[27/12/23, 10:23:33 PM] ~ Shekar Ramachandran: What is portkey, sorry have not
[28/12/23, 7:28:53 AM] ~ Sudarsan: ‎~ Sudarsan joined from the community
[28/12/23, 8:45:57 AM] ~ Kun: ‎~ Kun joined from the community
[29/12/23, 9:53:03 AM] ~ Onkar Mishra: ‎Your security code with ‪+91 99998 25927‬ changed.
[29/12/23, 1:40:29 PM] ~ Antaripa: ‎~ Antaripa joined from the community
‎[29/12/23, 3:01:43 PM] Sasank Chilamkurthy: ‎image omitted
[29/12/23, 3:01:55 PM] Sasank Chilamkurthy: Made in India motherboards and RAM sticks!
[30/12/23, 4:58:51 PM] Sasank Chilamkurthy: Well another blog on GPUs is here 😁
 https://twitter.com/sasank51/status/1741058461017002056?t=wEEaiIlJHZBbxIln4Xgh9A&s=19
‎[30/12/23, 4:59:35 PM] Sasank Chilamkurthy: ‎image omitted
[30/12/23, 5:15:09 PM] ~ Adarsh: Read the blog @919892727514! Really amazing. I did understand the reason why we expect the memory bandwidth to match the FLOPS. What according to you has been holding back sota GPUs from designing something that matches this? Why hasn't the global memory for these chips been designed with high bandwidths? Is it a technical limitation?
[30/12/23, 5:16:41 PM] Sasank Chilamkurthy: Main reason is neither the speed of memory cells nor number of these memory cells. Problem is interconnect b/w memory and controller.
[30/12/23, 5:17:36 PM] ~ Adarsh: The bandwidth between them that is right?
[30/12/23, 5:17:37 PM] Sasank Chilamkurthy: What SOTA gpus do is that they put HBM memories and try to increase number of interconnects through advanced packaging
[30/12/23, 5:18:05 PM] ~ Adarsh: Ahh okay
[30/12/23, 5:18:52 PM] Sasank Chilamkurthy: We need to scale wires instead of transistors
[30/12/23, 5:19:44 PM] ~ Adarsh: Why not just have bigger boards and fit a lot of them? I might sound so noob rn xd
[30/12/23, 5:20:32 PM] Sasank Chilamkurthy: But these wires need to go connect to CPU right.
[30/12/23, 5:22:00 PM] ~ Adarsh: Right
[30/12/23, 5:22:44 PM] Sasank Chilamkurthy: What intel has done with their latest core ultra/meteor lake is that they've used older silicon wafer as sort of PCB board!
[30/12/23, 5:23:35 PM] Sasank Chilamkurthy: They put different dies like GPU/CPU/IO on this wafer (but not memory, sadly)
[30/12/23, 5:23:43 PM] ~ Adarsh: Silicon wafer as pcb?? Damn
[30/12/23, 5:24:47 PM] Sasank Chilamkurthy: Advanced packaging is where future is
[30/12/23, 5:26:37 PM] ~ Kishore M R: Chiplets
[30/12/23, 5:29:29 PM] Sasank Chilamkurthy: Chiplets indeed! But I prefer intel's version over AMD's because of intel 90nm substrate
[30/12/23, 11:50:50 PM] Adithya Kannada LLM: So is m1 line chip with embedded gpu benifiting from less distance and better connectivity?
[31/12/23, 8:40:58 AM] ~ Adithya: ‎~ Adithya joined from the community
[31/12/23, 10:48:43 AM] Sasank Chilamkurthy: both of them share the same memory - the limitation is communication b/w memory and gpu
[31/12/23, 11:22:19 AM] ~ Ashish Patel: https://www.notebookcheck.net/Apple-s-first-public-LLM-is-called-Ferret-powered-by-8-Nivida-A100-GPUs.787395.0.html#:~:text=Apple%20is%20getting%20serious%20about,using%208%20Nvidia%20A100%20GPUs.
[31/12/23, 1:16:41 PM] ~ Bibek: Or establish wireless communication.
[31/12/23, 1:22:08 PM] ~ Kishore M R: There are ARM versions being built as well. Industry specific application.
[31/12/23, 1:22:30 PM] Sasank Chilamkurthy: interesting. any links?
[31/12/23, 3:13:52 PM] ~ Ashish Patel: https://www.xda-developers.com/best-gpus-for-deep-learning/
[31/12/23, 5:14:31 PM] ~ Vamshi: I was in the low power inference acceleration space for a while.

For the longest time, the general consensus on the ml compute market was that if there’s one thing that was certain, the future was extremely heterogenous.

Different architectures for different compute use cases.

Compute in memory and analog approaches were (and  possibly still are) the distant future.
[31/12/23, 5:15:38 PM] ~ Vamshi: One wonders whether there will be a “fits all” disruption in inference and training in a manner similar to the watershed moment in the software for it.
[31/12/23, 5:17:01 PM] ~ Vamshi: I know some folks going the memristor route got a bit of press recently but still seems quite experimental …
[31/12/23, 5:17:28 PM] ~ Vamshi: Anyone here working on new hardware ? Would be very interesting to hear from you
[31/12/23, 5:18:16 PM] Sthit Validity: What's the memristor route ?
[31/12/23, 5:20:42 PM] ~ Vamshi: Rain AI gets the maximum press for this in recent times
[31/12/23, 5:20:47 PM] ~ Vamshi: You can look them up
[31/12/23, 5:20:57 PM] ~ Vamshi: But this idea has been around for a while
[31/12/23, 5:21:41 PM] ~ Vamshi: So it’s not unique to them, but I believe they were “blessed” with the right networks
[31/12/23, 5:22:23 PM] ~ Vamshi: Used to be called Rain Neuromorphics iirc
[31/12/23, 5:49:01 PM] ~ Vamshi: This maybe dated now, but has some details:
[31/12/23, 5:49:22 PM] ~ Vamshi: https://arxiv.org/pdf/2006.01981.pdf
[31/12/23, 5:54:35 PM] ~ Kishore M R: I dont think so available in public domain now. Internal stuff. ‎<This message was edited>
[31/12/23, 9:19:06 PM] ~ Ashish Patel: https://www.tomshardware.com/pc-components/gpus/amd-strikes-back-at-nvidia-with-new-mi300x-benchmarks-mi300x-shows-30-higher-performance-than-h100-even-with-an-optimized-software-stack
[31/12/23, 11:19:22 PM] Sthit Validity: Thanks. Will have a look 🫡
[01/01/24, 6:59:48 PM] ~ Adhish Thite: ‎~ Adhish Thite joined from the community
[01/01/24, 9:47:32 PM] ~ KS: Is there a good resource to learn more about the things discussed?
‎[01/01/24, 10:10:06 PM] Sasank Chilamkurthy: mspec.2006.1638044.pdf • ‎6 pages ‎document omitted
[02/01/24, 1:43:22 AM] ~ Pratyush: ‎~ Pratyush joined from the community
[02/01/24, 1:32:35 PM] ~ Kishore M R: https://www.eetasia.com/bosch-infineon-nordic-nxp-and-qualcomm-establish-quintauris-to-advance-adoption-of-risc-v/
[02/01/24, 3:44:37 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[02/01/24, 4:25:09 PM] ~ Pramod: ‎~ Pramod joined from the community
[02/01/24, 5:08:50 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[02/01/24, 6:11:38 PM] ~ Aastha: ‎~ Aastha joined from the community
[02/01/24, 6:37:37 PM] ~ Adhish Thite: Does anyone have experience with ComfyUI (Stable Diffusion)?

I have a script that generates images based on prompts, however, it utilizes a single GPU. I want my script to utilize all the GPUs so that parallelism is achieved
[02/01/24, 7:25:41 PM] Nirant : DeepMedia group might get better answers?
[02/01/24, 7:26:04 PM] ~ Adhish Thite: Sure, Thanks @917737887058
[02/01/24, 7:26:24 PM] ~ Adhish Thite: Requested to add
[03/01/24, 1:55:22 AM] ~ Jigar: ‎~ Jigar joined from the community
[03/01/24, 1:12:20 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
‎[03/01/24, 9:46:48 PM] Sasank Chilamkurthy: ‎image omitted
[04/01/24, 12:44:32 PM] ~ Sri Krishna: why is Meta getting as many h100s as Azure? https://www.theverge.com/2023/12/4/23987953/the-gpu-haves-and-have-nots
[04/01/24, 12:45:07 PM] Nirant : Open AI company
[04/01/24, 12:45:32 PM] ~ Sri Krishna: but azure is a cloud company i dont get it lol
[04/01/24, 12:45:49 PM] Aashay Sarvam AI: Didn’t modi ji also get India some 16k H100s?
[04/01/24, 12:46:17 PM] Nirant : Adani/Ambani might've kept 15.9k for themselves
[04/01/24, 12:46:29 PM] ~ Sri Krishna: meta is planning on cloud business or something?
[04/01/24, 12:46:50 PM] Nirant : You don't need a cloud business to use that many GPUs any more
[04/01/24, 12:47:37 PM] Dr. Pratik Desai: Yann is cooking something for us
[04/01/24, 12:47:39 PM] ~ Sri Krishna: 3B in just R&D cost then?
[04/01/24, 12:47:48 PM] ~ Akash Singh: Yes and they are coming with llama models chat agent feature in whasapp and fb chats. And ai generated contents on Instagram.
[04/01/24, 12:47:49 PM] Nirant : More SAM!
[04/01/24, 12:48:27 PM] Nirant : No, it's also inference workloads for every RecSys (FB, IG), filters (IG) — and more important: ads offline feature generation which are increasingly neural
[04/01/24, 12:48:54 PM] Nirant : Not counting the chat stuff for business they want to launch
[04/01/24, 12:49:00 PM] Nirant : Yes, beat me to it
[04/01/24, 12:49:28 PM] ~ Sri Krishna: is h100 most efficient there? for inference?
[04/01/24, 12:50:11 PM] Nirant : At Meta's competence levels, I assume they can extract every iota of perf they care about
[04/01/24, 12:51:01 PM] ~ Sri Krishna: hmm. would be great to know if any meta folks here lol — what they use for inference
[04/01/24, 12:51:10 PM] Anshuman Nimble Box: They went to Yotta cloud
[04/01/24, 12:51:25 PM] Anshuman Nimble Box: by far the best!
[04/01/24, 12:51:35 PM] Nirant : Meta is as good, or better than every AI player you can think of. Let me rephrase this with more dramatic examples: Mistral — the entire company was a few bps in the Meta talentbench across engineering and research
[04/01/24, 12:52:21 PM] ~ Sri Krishna: right. i suppose if you ignore cost per hour which is what peasants like us have to consider. meta can just focus on wattage/electricity cost
[04/01/24, 12:53:09 PM] ~ Sri Krishna: they’d also have some sort internal cost/hr metric which they have to consider?
[04/01/24, 12:53:36 PM] Nirant : Yes, that's why orgs do Capex, individuals do Opex. If you're a Fortune500 or Nifty50 org, you should consider owning some infra at this point. In hindsight, Amod Malviya's bet to have cloud as backup, not primary — across Flipkart, Udaan was excellent.
[04/01/24, 12:54:06 PM] Nirant : I'd not have the wisdom or gumption to such capex across talent and compute in 2014
[04/01/24, 12:54:42 PM] Dr. Pratik Desai: I want Yann to gift us llama3 1.3B as good as Mistral7b.
[04/01/24, 12:54:43 PM] Nirant : So the real alpha someone like Amod has over plebs like me isn't just raw IQ points + skill, it's also courage and gumption
[04/01/24, 12:55:06 PM] ~ Sri Krishna: whats the average returns on a single h100 for someone like azure?
[04/01/24, 12:55:51 PM] Nirant : Depends on your amortization duration — do you assume 3 years (as most of Wall Street does) — or 5-10 years?
[04/01/24, 12:56:25 PM] Nirant : On the numerator side, the biggest advantage is that your talent is now free to iterate at lightning speeds without thinking of GPUs as opex or ration
[04/01/24, 12:56:29 PM] ~ Sri Krishna: till it eventually get retired
[04/01/24, 12:57:33 PM] ~ Sri Krishna: but i guess 3-5yrs sounds reasonable
[04/01/24, 12:57:47 PM] ~ Sri Krishna: even K80s got obselete after 6yrs or something iirc
[04/01/24, 12:58:02 PM] ~ Kesava Reddy ☁️🧠🤝🛜: 24 months… provided full 80% utilisation…
[04/01/24, 12:58:07 PM] ~ Sri Krishna: they were around in gcp etc for much more but it was just too slow for mass adoption even if cost were dirt cheap
[04/01/24, 12:58:52 PM] ~ Sri Krishna: right and the price can be changed in future. will do some calc
[04/01/24, 12:59:45 PM] Nirant : Zooming out, there are now 7 basic utilities for any sovereign: Internet, water, air, roads, trains, compute, storage
[04/01/24, 1:00:43 PM] ~ Sri Krishna: expecting a column for sales to saudi in 2024 then? lol
[04/01/24, 1:13:59 PM] ~ Y: ‎~ Y joined from the community
[04/01/24, 1:01:56 PM] ~ Kesava Reddy ☁️🧠🤝🛜: Please add health….
[04/01/24, 1:04:06 PM] Nirant : No modern sovereign treats health as a utility, definitely not the Indian state which spends less on health than it does on advertising that it does things
[04/01/24, 1:05:49 PM] Nirant : Food, health are optional for India anyway — we lose about 1M people every year to just low key fixable shit like Malaria, Tuberculosis, car accidents and malnutrition (including diabetes, which is malnutrition of the rich)

What is 1M deaths to Indian state? Rounding error when you've a 1.4B people to take care of. 

That's the right trade off
[04/01/24, 1:06:20 PM] Nirant : But this convo is for the Policy group and I'll pause here
[04/01/24, 1:06:56 PM] Nirant : I was talking about modern corps needing to vertically integrate into a major innovation unlock: Compute (CPU, GPU, RAM) and storage
[04/01/24, 1:13:10 PM] ~ Sri Krishna: is there any writeup on this? amod talking about it etc?
[04/01/24, 1:13:46 PM] ~ Sri Krishna: who else is doing it in india? like largest overall and relative to their needs as well.
[04/01/24, 2:32:46 PM] ~ Adarsh: I think it's mostly for their research supercluster (RSC)
https://ai.meta.com/blog/ai-rsc/
[04/01/24, 2:35:24 PM] ~ Sri Krishna: makes sense. any update on this in 2023? this says they were planning to add 16k a100s in 2022
[04/01/24, 2:35:53 PM] ~ Adarsh: Not much but I think It was mentioned in the llama 2 paper
[04/01/24, 2:36:03 PM] ~ Adarsh: They trained llama 2 on this cluster
[04/01/24, 2:36:11 PM] ~ Sri Krishna: ah, cool. will check
[04/01/24, 4:20:30 PM] Nirant : Bajaj Finance, Asian Paints, C-DAC from top of my mind
[04/01/24, 4:21:13 PM] ~ Sandeep Srinivasa: asian paints is doing GenAI ? what usecases?
[04/01/24, 4:22:07 PM] Nirant : Classical AI, demand forecasting
[04/01/24, 4:53:11 PM] Dr. Pratik Desai: Bajaj Finance 😂 Those calls are going to be crazy in the coming days.
[05/01/24, 3:47:51 AM] ~ Paulfinneyx: ‎~ Paulfinneyx joined from the community
[05/01/24, 9:59:22 AM] Sasank Chilamkurthy: Axolotl bounties for Triton

https://github.com/OpenAccess-AI-Collective/axolotl/issues/1038
[05/01/24, 9:59:38 AM] Sasank Chilamkurthy: Interesting stuff. Anyone interested in teaming up and hacking at this?
[05/01/24, 10:00:21 AM] Sasank Chilamkurthy: We can have a catch up. Should be fun learning experience even if we don't end up winning the bounty.
[05/01/24, 10:45:29 AM] Ritesh: I would be interested. But am pretty novice in this.
[05/01/24, 10:45:35 AM] Ritesh: DMing
[05/01/24, 10:48:07 AM] Sasank Chilamkurthy: No issues. We'll figure this on the go. We'll meet up and I can give a tutorial.
[05/01/24, 1:11:24 PM] Aashay Sarvam AI: ‎Your security code with Aashay Sarvam AI changed.
[06/01/24, 12:45:53 AM] ~ S: ‎Your security code with ‪+91 75500 68884‬ changed.
[06/01/24, 1:30:26 AM] ~ Yash: Hey interested in this! But I’m currently more on the learning end of the spectrum
[06/01/24, 4:12:20 AM] ~ Chirag Jain: I am also interested
Haven't written Triton kernels but was reading a bunch of code from unsloth
[06/01/24, 8:59:33 AM] ~ Anindyadeep Sannigrahi: ‎Your security code with ‪+91 96472 61597‬ changed.
[06/01/24, 9:20:12 AM] Sasank Chilamkurthy: OK great. Do these tutorials from here https://triton-lang.org/main/index.html

It'll take a day. DM me once done, I'll arrange a meet up.
[06/01/24, 11:13:33 AM] ~ Rahul Deora: ‎~ Rahul Deora joined from the community
[06/01/24, 1:11:37 PM] ~ Abhinand: ‎~ Abhinand joined from the community
[06/01/24, 5:10:11 PM] ~ Ankit: ‎~ Ankit joined from the community
[06/01/24, 8:02:49 PM] ~ Nayan Shah: how are people building an efficient inferencing server for LLMs ??
[06/01/24, 8:03:05 PM] ~ Nayan Shah: to utlize to max as possible with gpu
[06/01/24, 8:03:34 PM] ~ Arko C | xylem.ai: You wanna build from scratch or use existing infra?
[06/01/24, 8:04:28 PM] ~ Nayan Shah: no i was wondering from chat gpt side ?? as it is serving 100 m + requests ...so wanted to know more about it..
[06/01/24, 8:05:40 PM] ~ sidharth016: We recently finetuned quantized models for a particular domain and the gguf models are not that infra hungry. Maybe try that
[06/01/24, 8:05:42 PM] ~ Nayan Shah: wanted to understand , i think i currently dont have any requirement , just asked from learning pov . what are good options for existing infra
[06/01/24, 8:06:49 PM] ~ Nayan Shah: sure will check this out gguf is the file format in which we store model right ??
[06/01/24, 8:07:03 PM] ~ Nayan Shah: just like ggml ig , which works with cpu
[06/01/24, 8:08:58 PM] ~ Anindyadeep Sannigrahi: Uhmm did you fine-tune a gguf model directly?
[06/01/24, 8:09:15 PM] ~ Anindyadeep Sannigrahi: Yes
[06/01/24, 8:10:04 PM] ~ Arko C | xylem.ai: To maximise the gpu usage, you can use multiple techniques, like continouse batching, kernel fusion. Largely focused towards memory optimisation, cause contrary to popular beliefs, GPU memory is what slows it down.

You can try deploying TGI or vLLM on your own to serve your models

Or deploy your models through folks like Anyscale, Together, Xylem AI, Replicate, etc
[06/01/24, 8:10:06 PM] ~ Nayan Shah: is there a documentation or some link u can share on gguf models to understand better i am assuming it will be like onnx , just want to understand concepts etc
[06/01/24, 8:12:05 PM] ~ Arko C | xylem.ai: I am saying this in case you want to serve your current model more efficiently
[06/01/24, 8:12:26 PM] ~ Arko C | xylem.ai: Not fine-tuning a smaller model and making it lighter on compute
[06/01/24, 8:12:41 PM] ~ Arko C | xylem.ai: That has multiple facets to consider on quality n other things before you can even come to deployment ‎<This message was edited>
[06/01/24, 8:14:09 PM] ~ Nayan Shah: yeah i want to understand more on inferencing part of this and how efficient is that can be 👍
[06/01/24, 8:14:57 PM] ~ Nayan Shah: as i feel inferencing server would be huge part of this , models in terms of usabiltiy and making sure it breaks even to have a gpu which we can utlize to the fullest it can be
[06/01/24, 8:15:42 PM] ~ Arko C | xylem.ai: 100% Agreed!
[06/01/24, 8:18:07 PM] ~ Arko C | xylem.ai: @917892792975 wanna add something here?
[06/01/24, 8:23:12 PM] ~ sidharth016: Yes for cpu based inference.
[06/01/24, 8:25:31 PM] Sasank Chilamkurthy: Read my blog: https://chsasank.com/llm-system-design.html
[06/01/24, 8:25:55 PM] Sasank Chilamkurthy: This is on point and I explain why in this
[06/01/24, 8:29:37 PM] ~ Adarsh: Definitely. Met up with srinivas Narayanan(VP applied @openai) yesterday courtesy to the folks at peakxv. One of the few insights to gather were that these big research labs have a big amount of research being done just to optimize for efficient serving. They have proprietary techniques and methods that are best suited for such large scale inference
[06/01/24, 8:30:42 PM] ~ Adarsh: As you pointed out, paged attention, speculative decoding, batching, are a few optimisations you can explore
[06/01/24, 8:31:13 PM] ~ Nayan Shah: i applied to attend this but was not approved 🥺, can someone share some notes /slides or any discussion thread
[06/01/24, 8:31:16 PM] ~ Nayan Shah: if any
[06/01/24, 8:31:47 PM] ~ Adarsh: @917737887058 had some really beautiful notes. I had a peek 👀
[06/01/24, 8:34:30 PM] ~ Adarsh: One more insight - he said no matter how complicated the prompt is, the amount of compute required is the same. For eg. If you ask it to prove fermats theorem or ask it 2+2, the amount of compute spent is the same. I mean it's obvious but I still always wondered if complicated prompts would lead to much more neuron activations and hence much more compute. It's weird
[06/01/24, 8:35:47 PM] Abhishek Mishra: the idea is same as asking an expert to answer something simple vs answering something they're an expert at
[06/01/24, 8:36:07 PM] Sasank Chilamkurthy: This is in fact the main problem with LLMs. Mixture of experts might eventually solve this, but we're not there yet.
[06/01/24, 8:36:32 PM] Abhishek Mishra: requires trivial off the lobe thinking for both.

However, anything requiring chained reasoning whether for LLMs or human would be reflected in compute
[06/01/24, 8:36:37 PM] Sasank Chilamkurthy: Why do I need to remember the whole internet when all I want to 2+2
[06/01/24, 8:36:57 PM] Nirant : Quite amusing that _expert_ is very recurring concept across society, and no consensus across cultures
[06/01/24, 8:37:41 PM] Nirant : Grammar, logic and rhetoric (aka persuasion) are different from each other
[06/01/24, 8:37:50 PM] Nirant : We need to learn all 3 to be effective
[06/01/24, 8:38:11 PM] Nirant : It's not useful to know how to do 2+2, but _when_ 2+2 is useful
[06/01/24, 8:39:05 PM] ~ Adarsh: But if you think in terms of a neural network, a complicated prompt might lead to more activations(i.e forward pass might take some time). Won't it affect latency in that case? Im not very sure if that's how it works 😂 ‎<This message was edited>
[06/01/24, 8:39:22 PM] ~ YP: GPT-4 is great at understanding intent
[06/01/24, 8:39:48 PM] ~ YP: Mostly while writing prompt. I first start with intent and then the problem statement to solve problem on day to day basis
[06/01/24, 8:40:22 PM] ~ Nayan Shah: u mean u use gpt 4 for writing prompt?? i get it checked via chat gpt
[06/01/24, 8:40:36 PM] ~ Nayan Shah: but intresting .
[06/01/24, 8:41:12 PM] Sasank Chilamkurthy: While it makes sense, it's not how these large models work. The whole 1 TB weights has to be activated for every token.
[06/01/24, 8:41:31 PM] ~ YP: No I mean either using chatGPT or GPT-4 itself. If I want to write a code or get started with a project - I'd specify what I am trying to reach at exactly and then ask it to do the coding task, I just get way less errors when doing that
[06/01/24, 8:42:14 PM] ~ Nayan Shah: ohh ok !! setting a task and context , got it 👍yeah that helps as the context is set .
[06/01/24, 8:43:44 PM] ~ Adarsh: Right. I figured he might be referring to the gpu util(loaded weights) as spent compute rather than intricate forward passes lol. But yeah cool stuff
[06/01/24, 8:45:06 PM] Sasank Chilamkurthy: In fact the main bottleneck is this transfer of weights or activation of neurons. We're activating all neurons at once to process anything. The techniques he mentioned is how you can amortize this cost by processing many queries at once. ‎<This message was edited>
[06/01/24, 8:46:42 PM] Sasank Chilamkurthy: If you're running something locally or may be your own finetuned model, you'll not have this amortisation. Activation of neurons becomes your main bottleneck.
[06/01/24, 8:53:30 PM] Sasank Chilamkurthy: I'll write about kernel fusion and why it works someday 😅
[06/01/24, 8:58:22 PM] ~ Adarsh: Would love to read!
[06/01/24, 10:44:57 PM] ~ Srinivasa Raghavan K M: ‎~ Srinivasa Raghavan K M joined from the community
[07/01/24, 2:10:30 AM] ~ Ravikiran Gunale: ‎~ Ravikiran Gunale joined from the community
[07/01/24, 1:35:15 PM] ~ Anindyadeep Sannigrahi: Yeahh I mean although with current moe infra it would take the same compute... However if at some day "unbalanced" moe becomes a thing i.e. a combination of small and large experts and prompt is routed according to its complexity that would be awesome
[07/01/24, 1:35:15 PM] ~ Anindyadeep Sannigrahi: That's actually a good thing to research also
[07/01/24, 1:36:01 PM] Sasank Chilamkurthy: That's why I'm betting on small models!
[07/01/24, 1:36:11 PM] Sasank Chilamkurthy: Modular design 😝
[07/01/24, 1:50:47 PM] ~ Adarsh: I mean in an moe, is it possible that 2 concurrent prompts are passed through a router that are then distributed to their respective experts and you get the respective outputs while spending the "same" amount of compute? Idk how moe's work yet but is it possible or does it work in a similar fashion?
[07/01/24, 2:13:30 PM] ~ Anindyadeep Sannigrahi: Haha yee that's the true value ig
[07/01/24, 5:49:21 PM] ~ Anjineyulu: Can u just suggest best practices to start with,like any mindful pitfalls
[07/01/24, 7:20:52 PM] Sasank Chilamkurthy: Minimize data movement - simple mantra
[07/01/24, 7:21:12 PM] Sasank Chilamkurthy: There's lot of CUDA textbooks written
[07/01/24, 8:32:56 PM] ~ Siddharth: I'd love to read that one. I am writing my own stuff in CUDA and Triton just to get good, and the CUDNN and Triton focus on fusion when I still don't quite get it, is a bit frustrating.
[07/01/24, 8:44:24 PM] Sasank Chilamkurthy: This is most concise but boring summary of HPC https://youtu.be/laEV_qkgtsc?si=dv9eknmbjsDKd4fJ
[07/01/24, 8:44:48 PM] Sasank Chilamkurthy: For this
[07/01/24, 8:47:16 PM] ~ Anindyadeep Sannigrahi: Hey actually I have a follow-up question, so the only reason I am bit reluctant to start going deep into cuda kernels or multi node coz I felt that I might get stuck if anything on basic hpcs comw
[07/01/24, 8:47:55 PM] ~ Anindyadeep Sannigrahi: So any suggestions whether that's the issue... Coz seems like no more time to be reluctant anymore on this and anyhow I gotta start, so any suggestions while reading this gpu part of things
[07/01/24, 8:54:30 PM] Sasank Chilamkurthy: Not sure I understand. Just call me up and we'll speak for a few mins
[07/01/24, 8:55:23 PM] ~ Anjineyulu: Is there more improvement compared nvidia trt formats?How much time in a corporate you typically think one must spend to optimize it?
[07/01/24, 8:55:54 PM] Sasank Chilamkurthy: Most 'normal' people don't care.
[07/01/24, 8:56:30 PM] Sasank Chilamkurthy: They just live at high level frameworks. Few venture into low level details like kernels.
[07/01/24, 8:56:41 PM] ~ Anindyadeep Sannigrahi: Yes yes, that how MoEs are designed to take those concurrent load, And yes the compute used for that would be same regardless of the prompt. However the idea of "unbalanced" MoE would be spend less overall compute that would be a function of prompt complexity
[07/01/24, 8:57:51 PM] ~ Anjineyulu: Let me try,what is the learning curve and how much time it takes to get novel ideas and good playbook?
[07/01/24, 9:25:51 PM] Sasank Chilamkurthy: Do these tutorials if you wanna get into gpu programming: https://triton-lang.org/main/index.html
[07/01/24, 9:26:11 PM] Sasank Chilamkurthy: It's higher level than Cuda but close enough to hardware
[07/01/24, 9:26:40 PM] ~ Siddharth: Do you see it taking over CUDA in a meaningful way?
[07/01/24, 9:27:02 PM] Sasank Chilamkurthy: I'm hoping to make this a reality 😝
[07/01/24, 9:28:19 PM] ~ Siddharth: :D I'd love that too. I find triton more intuitive. Was able to knock out a small model with ease, whereas in CUDA, I am still to get backprop to not be so bad.
[07/01/24, 9:29:30 PM] Sasank Chilamkurthy: I'm working on my own version of CUDA/triton kinda language.
[07/01/24, 9:29:45 PM] Sasank Chilamkurthy: My compiler posts in my blog are basically my design process.
[07/01/24, 9:30:48 PM] Sasank Chilamkurthy: I'd say it's not too hard. Just do Triton tutorials. You'll get a hang of it in a few days.
[07/01/24, 10:17:47 PM] ~ Anjineyulu: Oh sure will check out
[07/01/24, 10:33:03 PM] ~ Anindyadeep Sannigrahi: ye
[07/01/24, 11:50:25 PM] ‪+91 99770 01253‬: ‎‪+91 99770 01253‬ joined from the community
[07/01/24, 11:41:07 PM] ~ Sandeep Srinivasa: are you using llvm ?
[07/01/24, 11:42:45 PM] Sasank Chilamkurthy: Yes and MLIR
[08/01/24, 7:12:24 AM] ~ unmeshr: ‎~ unmeshr joined from the community
[08/01/24, 12:40:50 PM] ~ Anindyadeep Sannigrahi: ‎Your security code with ‪+91 96472 61597‬ changed.
[08/01/24, 3:56:41 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[08/01/24, 5:05:43 PM] Sasank Chilamkurthy: Ctx:
https://blogs.nvidia.com/blog/h100-transformer-engine/

Dbt:
Most AI floating-point math is done using 16-bit “half” precision (FP16) and 32-bit “single” precision (FP32). By reducing the math operations to just eight bits, Transformer Engine makes it possible to train larger networks faster without compromising accuracy. 

Please explain what this means?
[08/01/24, 5:05:50 PM] Sasank Chilamkurthy: From @919773065092
[08/01/24, 5:06:24 PM] Sasank Chilamkurthy: I'll explain this. So majority of the time in AI goes into transferring the weights from memory to controller
[08/01/24, 5:07:01 PM] Sasank Chilamkurthy: Controllers themselves are so fast that they're not the rate limiting step
[08/01/24, 5:07:36 PM] Sasank Chilamkurthy: So you're limited by memory bandwidth
[08/01/24, 5:08:14 PM] Sasank Chilamkurthy: Quantization saves you time because you can send just 1 bytes instead of 4 bytes (for fp32) from memory to controller
[08/01/24, 5:08:36 PM] Sasank Chilamkurthy: Read this for explanation.
[08/01/24, 5:10:36 PM] ~ Priyesh Srivastava: no no so like later it goes into how it has some heursitics to switch from fp8 to fp16 and vice versa
[08/01/24, 5:11:30 PM] ~ Priyesh Srivastava: is it basically checking for number of decimal point in weight change/weight value and then just adding or reducing precision in memory
[08/01/24, 5:13:14 PM] ~ Priyesh Srivastava: consuming
[08/01/24, 5:22:23 PM] Sasank Chilamkurthy: It's all about how you quantize. Many ways are possible to quantize the numbers into floating point representation.
[08/01/24, 5:35:43 PM] ~ Shekar Ramachandran: Compiling the model engine with tensorrt LLM helps in quantisation
[08/01/24, 5:36:31 PM] Sasank Chilamkurthy: Or you can always use llama.cpp
[08/01/24, 5:36:33 PM] Sasank Chilamkurthy: Same thing
[08/01/24, 6:03:40 PM] ~ Sandeep Srinivasa: Those of you using axolotl for training...how are you scaling on a kubernetes gpu cluster ?

If not kubernetes..what else are you using for scaling ?
[08/01/24, 6:26:33 PM] ~ Anindyadeep Sannigrahi: ‎Your security code with ‪+91 96472 61597‬ changed.
[08/01/24, 7:32:27 PM] ~ Adithya S K: I havnt worked with kube but generally run on a 4xA100 GPU pre configured instance and i run a docker container to not deal with the hassel of installing everything
[08/01/24, 8:21:21 PM] ~ Adhish Thite: All, I put my office number in the community info by mistake, leaving this group for now, will re-join with my personal number.

Thanks,
Adhish
[09/01/24, 5:50:35 AM] ~ Kesava Reddy ☁️🧠🤝🛜: The success of an AI strategy is largely determined by the *data and the infrastructure*, and less by the model itself. 
Models will be dime a dozen over time. Thoughts...
[09/01/24, 8:11:35 AM] ~ G Kuppuram: Model, hyper parameters, strategy, use case, application and so on
[09/01/24, 12:06:05 PM] Sasank Chilamkurthy: In fact personal data is going to be the most important for assistants. And I believe normal people will get threatened of displacement if compute is not owned by them. But this is off topic discussion 😅
[09/01/24, 12:07:57 PM] Nirant : Normal people don't even own their money. So many can't remember their own 10 digit mobile number, 5 passwords, PAN and Aadhar. The use case for privacy in consumer is vastly over-exaggerated.
[09/01/24, 12:08:18 PM] Nirant : Aaargh. Sorry got nerd sniped, off topic.  ‎<This message was edited>
[09/01/24, 12:24:22 PM] Anshuman Nimble Box: Aadhar surfing 🌊🏄‍♂️
[09/01/24, 3:32:40 PM] Sasank Chilamkurthy: Well, I’ve sold AI to enterprises and I know they care. Well normal people aren’t enterprises.
[10/01/24, 12:29:16 AM] ~ Aadesh: ‎~ Aadesh joined from the community
[10/01/24, 5:55:57 AM] ~ Akash: ‎Your security code with ‪+91 77084 10796‬ changed.
[10/01/24, 10:38:31 AM] ~ Anshul: ‎Your security code with ‪+91 99702 04619‬ changed.
[10/01/24, 12:39:34 PM] ~ Anshul: ‎Your security code with ‪+91 99702 04619‬ changed.
[10/01/24, 2:27:31 PM] ~ Anshul: ‎Your security code with ‪+91 99702 04619‬ changed.
[10/01/24, 5:33:14 PM] ~ Avani vidhani: ‎Your security code with ‪+91 98982 47919‬ changed.
[10/01/24, 5:33:18 PM] ~ Bibek: ‎Your security code with ‪+65 8256 0957‬ changed.
[10/01/24, 5:33:18 PM] ~ Dhanush Ram (DR): ‎Your security code with ‪+91 72003 08008‬ changed.
[10/01/24, 5:33:18 PM] ~ Saurabh Karn: ‎Your security code with ‪+91 81972 66977‬ changed.
[10/01/24, 5:33:18 PM] ~ Sarthak Gupta: ‎Your security code with ‪+91 85889 52201‬ changed.
[10/01/24, 5:33:18 PM] ~ Chirag Gandhi: ‎Your security code with ‪+91 87800 34041‬ changed.
[10/01/24, 5:33:18 PM] RISHAV: ‎Your security code with RISHAV changed.
[10/01/24, 5:33:18 PM] ~ KS: ‎Your security code with ‪+91 96428 50003‬ changed.
[10/01/24, 5:33:18 PM] ~ Adithya: ‎Your security code with ‪+91 97909 89953‬ changed.
[11/01/24, 8:55:24 AM] ~ Mayuresh Bakshi: ‎~ Mayuresh Bakshi joined from the community
[12/01/24, 11:17:27 AM] ‪+91 98808 34200‬: ‎‪+91 98808 34200‬ joined from the community
[12/01/24, 12:25:27 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[12/01/24, 3:38:30 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[12/01/24, 8:49:08 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[12/01/24, 8:56:40 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[12/01/24, 10:52:18 PM] ~ Wasim Madha: ‎~ Wasim Madha joined from the community
[12/01/24, 11:40:39 PM] ~ Pratik: ‎~ Pratik joined from the community
[13/01/24, 12:27:27 AM] ~ Prativa: ‎~ Prativa joined from the community
[13/01/24, 12:50:44 AM] ~ Shobhankita: ‎Your security code with ‪+91 97043 36542‬ changed.
[13/01/24, 9:28:46 AM] ~ Rajat Choudhary: ‎~ Rajat Choudhary joined from the community
[13/01/24, 2:55:56 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[13/01/24, 5:03:46 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[14/01/24, 9:29:29 AM] ‪+91 88009 40288‬: ‎‪+91 88009 40288‬ joined from the community
[14/01/24, 11:37:08 AM] ~ Priyank Agrawal: Any folks here using Modal or Runpod??
[14/01/24, 11:37:59 AM] ~ Priyank Agrawal: Also any alternatives to on demand serverless GPU provider for inferencing
[14/01/24, 11:46:50 AM] ~ Nishkarsh | usefindr.com: Used to use runpod a while ago for experimenting with different GPUs 

imho it’s really good for figuring out basics like how much memory, vCPUs, etc you might want for your app
[14/01/24, 11:50:59 AM] ~ Priyank Agrawal: Intresting thanks!
[14/01/24, 11:55:48 AM] ~ Rahul Deora: Which subscription did you take ? 35$?
[14/01/24, 11:57:58 AM] ~ Nishkarsh | usefindr.com: We used spot instances (RTX 4090) — we were charged per hour for it
[14/01/24, 12:09:49 PM] ~ Satpal: For serverless inference, modal was easier to modify code and experiment. Plus 30$ free credits every month.
Runpod serverless was problematic recently, couldn't build docker image without gpu, some dependencies issues & bugs in their docker template etc.
[14/01/24, 12:16:13 PM] ~ Nishkarsh | usefindr.com: haven’t used modal but same experiences for runpod hence I now always suggest folks to keep it as an experimental option and nothing more
[14/01/24, 7:54:56 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[15/01/24, 12:01:56 PM] ~ Ajey Gore: ‎Waiting for this message. This may take a while.
[15/01/24, 12:04:55 PM] Bharat Shetty: @919420377689 ^
[15/01/24, 12:10:56 PM] ~ Sachin: I can help
[15/01/24, 12:23:01 PM] ~ Hello World: ‎~ Hello World joined from the community
[15/01/24, 12:23:07 PM] ~ Ajey Gore: Will ping you sir
[15/01/24, 12:37:48 PM] Zainab Bawa: Best resource, @6587513767 
Friday's patient explaining was awesome @919420377689 
Thank you for your contributions to the community.
[15/01/24, 1:01:44 PM] ~ Hello World: ‎Nirant  removed ~ Hello World
‎[15/01/24, 3:55:55 PM] ~ Priyesh Srivastava: ‎image omitted
[15/01/24, 11:16:44 PM] ~ Naresh: ‎Waiting for this message. This may take a while.
‎[17/01/24, 4:47:45 PM] ~ Ashish Patel: ‎image omitted
[17/01/24, 4:54:38 PM] ~ Ashish Patel: *A Flaw in Millions of Apple, AMD, and Qualcomm GPUs Could Expose AI Data*

https://www.wired.com/story/leftoverlocals-gpu-vulnerability-generative-ai/
[18/01/24, 9:50:05 AM] ~ Abhishek: ‎~ Abhishek joined from the community
[18/01/24, 10:40:10 AM] ~ Mahesh Sathiamoorthy: ‎~ Mahesh Sathiamoorthy joined from the community
[18/01/24, 1:41:49 PM] ~ Meenu: ‎~ Meenu joined from the community
[18/01/24, 3:47:14 PM] ~ Sri Krishna: ‎Waiting for this message. This may take a while.
[18/01/24, 4:58:56 PM] ~ Aastha: ‎This message was deleted.
[18/01/24, 5:00:22 PM] ~ Bharath: Could you please add a description, Aastha?
[18/01/24, 6:03:07 PM] ~ Harsh Gupta: ‎~ Harsh Gupta joined from the community
[18/01/24, 6:03:39 PM] ~ Aastha: I'm really sorry, shared by mistake. Will delete to alleviate confusion.
[20/01/24, 9:25:31 AM] RISHAV: Hello folks, I have some long running tasks which I need to queue. So currently I was using the redis queue, it seems it has some cons. Any other methods that are generally used?
[20/01/24, 9:53:28 AM] ~ Y: We use our implementation but can you describe the limitation that you are facing
[20/01/24, 11:00:49 AM] RISHAV: I need to find the core problem but, what's happening is that after it executes some number of processes (not sure of the exact number). It fails to process more and I have to restart the Redis docker again and things work normally again.
[20/01/24, 11:13:12 AM] ~ Y: This appears to be a memory issue. If jobs are pending, Redis will keep them in memory and subsequent queuing will fail if memory is not available. You might want to play with `maxmemory`
[21/01/24, 7:20:05 PM] ~ Aayush Mudgal: ‎Your security code with ‪+1 (347) 279‑3767‬ changed.
[21/01/24, 7:20:05 PM] ~ Mudit Tyagi: ‎Your security code with ‪+1 (408) 829‑8258‬ changed.
[21/01/24, 7:20:05 PM] ~ kranthi (:: ‎Your security code with ‪+1 (646) 359‑1360‬ changed.
[21/01/24, 7:20:05 PM] ~ Sathyaprakash Narayanan: ‎Your security code with ‪+1 (831) 529‑7133‬ changed.
[21/01/24, 7:20:05 PM] ~ Muhammad Hammad Khan: ‎Your security code with ‪+44 7391 843340‬ changed.
[21/01/24, 7:20:05 PM] ~ Arihant Barjatya: ‎Your security code with ‪+91 6900 400 740‬ changed.
[21/01/24, 7:20:05 PM] ~ Dhanush Ram (DR): ‎Your security code with ‪+91 72003 08008‬ changed.
[21/01/24, 7:20:05 PM] ~ Vipul: ‎Your security code with ‪+91 73892 76189‬ changed.
[21/01/24, 7:20:05 PM] ~ Chirag Singla: ‎Your security code with ‪+91 75035 07887‬ changed.
[21/01/24, 7:20:05 PM] ~ Omkar: ‎Your security code with ‪+91 77200 33717‬ changed.
[21/01/24, 7:20:05 PM] ~ Junaid: ‎Your security code with ‪+91 77739 73999‬ changed.
[21/01/24, 7:20:05 PM] ~ Vasu 🥸: ‎Your security code with ‪+91 78385 26259‬ changed.
[21/01/24, 7:20:05 PM] ~ aditya anand: ‎Your security code with ‪+91 80862 28356‬ changed.
[21/01/24, 7:20:05 PM] ~ Pushkar Pandey: ‎Your security code with ‪+91 81059 77690‬ changed.
[21/01/24, 7:20:05 PM] ~ Amal David: ‎Your security code with ‪+91 81221 42493‬ changed.
[21/01/24, 7:20:05 PM] ~ Vatsal: ‎Your security code with ‪+91 83100 03574‬ changed.
[21/01/24, 7:20:05 PM] ~ Ujjwal: ‎Your security code with ‪+91 83589 56403‬ changed.
[21/01/24, 7:20:05 PM] ~ Karan Gandhi: ‎Your security code with ‪+91 83683 46655‬ changed.
[21/01/24, 7:20:05 PM] ~ Rahul: ‎Your security code with ‪+91 85535 44367‬ changed.
[21/01/24, 7:20:05 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[21/01/24, 7:20:05 PM] ~ Ashok: ‎Your security code with ‪+91 89514 61917‬ changed.
[21/01/24, 7:20:05 PM] ~ Unni Krishnan: ‎Your security code with ‪+91 95610 95946‬ changed.
[21/01/24, 7:20:05 PM] ~ 🌼: ‎Your security code with ‪+91 96012 92201‬ changed.
[21/01/24, 7:20:05 PM] ~ Nj: ‎Your security code with ‪+91 97069 90543‬ changed.
[21/01/24, 7:20:05 PM] ~ $@!: ‎Your security code with ‪+91 97316 51931‬ changed.
[21/01/24, 7:20:05 PM] ~ Sahil Shubham: ‎Your security code with ‪+91 98716 46149‬ changed.
[21/01/24, 7:20:05 PM] ~ Ramakrishnan Raman: ‎Your security code with ‪+91 98907 12333‬ changed.
[21/01/24, 7:20:05 PM] ~ Hariprasad P S: ‎Your security code with ‪+91 98944 67243‬ changed.
[21/01/24, 7:20:05 PM] ~ Prayash Mohapatra: ‎Your security code with ‪+91 99381 62000‬ changed.
[21/01/24, 7:20:05 PM] ~ AJ: ‎Your security code with ‪+91 99863 55841‬ changed.
[21/01/24, 10:22:44 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[21/01/24, 10:30:40 PM] ~ Tanuj Mendiratta: ‎Your security code with ‪+91 98100 58207‬ changed.
[21/01/24, 10:39:10 PM] ~ Tanuj Mendiratta: ‎Your security code with ‪+91 98100 58207‬ changed.
[22/01/24, 6:38:16 AM] ~ Nipun Jain: ‎~ Nipun Jain joined from the community
[22/01/24, 9:17:48 AM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[22/01/24, 12:12:24 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[22/01/24, 12:20:26 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[22/01/24, 4:20:16 PM] ~ $@!: ‎Your security code with ‪+91 97316 51931‬ changed.
[23/01/24, 1:36:54 AM] ~ Surender: ‎~ Surender joined from the community
[23/01/24, 9:13:30 PM] ~ Deeksha💁‍♀️: ‎~ Deeksha💁‍♀️ joined from the community
[24/01/24, 8:47:48 AM] ~ Srinivasa Raghavan K M: ‎Your security code with ‪+91 88614 63394‬ changed.
[24/01/24, 8:31:56 PM] ~ 🙂: ‎~ 🙂 joined from the community
[24/01/24, 10:13:54 PM] ~ 42p: ‎~ 42p joined from the community
[25/01/24, 11:29:48 AM] Bulia Aurashop: ‎Your security code with Bulia Aurashop changed.
[25/01/24, 12:13:16 PM] ~ 🏫: ‎~ 🏫 joined from the community
[25/01/24, 12:47:17 PM] ~ Avinash: ‎~ Avinash joined from the community
[26/01/24, 5:33:32 PM] ‪+91 85939 07553‬: ‎Your security code with ‪+91 85939 07553‬ changed.
[26/01/24, 5:33:32 PM] ‪+91 87553 59531‬: ‎Your security code with ‪+91 87553 59531‬ changed.
[26/01/24, 5:33:32 PM] ~ Bash: ‎Your security code with ‪+91 87870 63672‬ changed.
[26/01/24, 5:33:32 PM] ~ Sandeep: ‎Your security code with ‪+91 88282 90501‬ changed.
[26/01/24, 5:33:32 PM] ~ Sudhanshu Heda: ‎Your security code with ‪+91 90798 18831‬ changed.
[26/01/24, 5:33:32 PM] Yash Bonde: ‎Your security code with Yash Bonde changed.
[26/01/24, 5:33:32 PM] ~ Badal: ‎Your security code with ‪+91 93138 27344‬ changed.
[26/01/24, 5:33:32 PM] ~ Tejas Vaidhya: ‎Your security code with ‪+91 93400 04079‬ changed.
[26/01/24, 5:33:32 PM] ~ Pranjal Yadav: ‎Your security code with ‪+91 95470 56568‬ changed.
[26/01/24, 5:33:32 PM] ~ ~: ‎Your security code with ‪+91 95661 55731‬ changed.
[26/01/24, 5:33:32 PM] ~ Sushant Kumar: ‎Your security code with ‪+91 96194 46401‬ changed.
[26/01/24, 5:33:32 PM] ~ Kaushik FiddleCube: ‎Your security code with ‪+91 96321 47475‬ changed.
[26/01/24, 5:33:32 PM] ~ Krishna Panchal: ‎Your security code with ‪+91 96877 95488‬ changed.
[26/01/24, 5:33:32 PM] ~ Chirasmita Mallick: ‎Your security code with ‪+91 97171 04683‬ changed.
[26/01/24, 5:35:19 PM] ~ $@!: ‎Your security code with ‪+91 97316 51931‬ changed.
[27/01/24, 1:06:04 AM] Sthit Validity: ‎Your security code with Sthit Validity changed.
[27/01/24, 11:20:43 AM] ~ Surender: ‎Nirant  removed ~ Surender
[27/01/24, 9:09:35 PM] ~ Delip Rao: ‎~ Delip Rao joined from the community
[28/01/24, 11:24:58 AM] ~ Raghav Ravishankar: ‎~ Raghav Ravishankar joined from the community
[28/01/24, 9:57:17 PM] ~ Pratt: ‎~ Pratt joined from the community
[28/01/24, 10:34:36 PM] ~ Adithya S K: ‎Your security code with ‪+91 91 485 743 93‬ changed.
[28/01/24, 10:43:06 PM] ~ Adithya S K: ‎Your security code with ‪+91 91 485 743 93‬ changed.
[28/01/24, 11:12:34 PM] ~ Jaswanth: ‎~ Jaswanth joined from the community
[29/01/24, 8:55:29 AM] ~ Kiri: ‎~ Kiri joined from the community
[29/01/24, 12:45:13 PM] ~ Abheejit: ‎~ Abheejit joined from the community
[29/01/24, 2:11:04 PM] ~ Aditya: ‎~ Aditya joined from the community
[29/01/24, 7:26:48 PM] ~ Utkarsh Saxena: ‎~ Utkarsh Saxena joined from the community
[29/01/24, 9:54:09 PM] ~ Nijil Y: ‎~ Nijil Y joined from the community
[29/01/24, 10:26:18 PM] ~ Narendranath Gogineni: ‎~ Narendranath Gogineni joined from the community
[30/01/24, 12:22:42 AM] ~ Sreechand Tavva: ‎~ Sreechand Tavva joined from the community
[30/01/24, 7:08:23 AM] Bharat Shetty: https://www.artfintel.com/p/where-do-llms-spend-their-flops very nice article on analysing FLOPs and stuff in LLMs
[31/01/24, 12:08:00 PM] ~ Arvind Sankar: ‎~ Arvind Sankar joined from the community
[31/01/24, 5:32:16 PM] ~ Abhishek Satish: ‎~ Abhishek Satish joined from the community
[31/01/24, 9:04:08 PM] ~ Utkarsh Saxena: Hey, I’m working with a government client that needs servers/cloud to be with an Indian company (not Azure, AWS even if they’ve data centres in India). Any advice/ recommendations/ contacts at Indian companies with good offerings, GPU enabled machines etc?
[31/01/24, 9:04:45 PM] ~ Rajat Choudhary: E2E
[31/01/24, 9:06:08 PM] ~ Kishore M R: Jarvislabs
[31/01/24, 9:11:58 PM] Nirant : If it helps build confidence, E2E is listed on Indian stock exchanges as well
[31/01/24, 9:15:01 PM] ~ Adarsh: E2E 👌🏼
[31/01/24, 9:16:44 PM] ~ Shekar Ramachandran: Krutrim
[31/01/24, 9:18:41 PM] ~ Shekar Ramachandran: You can DM me And I can help with the contacts
[31/01/24, 10:05:49 PM] ~ Kesava Reddy ☁️🧠🤝🛜: Are they offering GPU enabled machines?

I doubt they have GPU infra in India.
[31/01/24, 10:06:38 PM] Anshuman Nimble Box: Bruh what?

They're the only players who have any H100 available reserved & ON DEMAND
[31/01/24, 10:06:58 PM] Anshuman Nimble Box: My bad, was talking about E2E.
[31/01/24, 10:08:08 PM] Nirant : brb, buying some E2E stock
[31/01/24, 10:08:34 PM] Anshuman Nimble Box: Sir 🙏
‎[31/01/24, 10:09:26 PM] Nirant : ‎image omitted
[31/01/24, 10:09:39 PM] Dr. Pratik Desai: E2E is in such a demand that @917356725027 et al doesn’t even offer free credits 😂
[31/01/24, 10:10:34 PM] Anshuman Nimble Box: Underrated
[31/01/24, 10:12:14 PM] Adithya Kannada LLM: Hey @917356725027, can we get credits if I buy stock?
[31/01/24, 10:16:25 PM] Anshuman Nimble Box: ‎This message was deleted.
[31/01/24, 10:18:57 PM] Dr. Pratik Desai: Ser, if you need 1080 outside data center, I can share mine from home office 😂
[31/01/24, 10:20:00 PM] Nirant : Motabhai can bring to India without import duty
[31/01/24, 10:20:12 PM] Dr. Pratik Desai: By the way, I know many folks are doing Uber model from their vacant 3090s from crypto mining operations
[31/01/24, 10:20:40 PM] Nirant : This has phone numbers, do you want to block those out and re-share?
[31/01/24, 10:20:49 PM] Dr. Pratik Desai: Do you need one?
[31/01/24, 10:22:10 PM] Anshuman Nimble Box: Thanks for the heads up!
[31/01/24, 10:23:14 PM] Dr. Pratik Desai: 3090s I bought at $800 after 3-4 years are being sold at $1500, that’s better return than SNP500
[31/01/24, 10:23:34 PM] Anshuman Nimble Box: Darn, the resolution is so bad I am not able to redact it lol

Will leave it for another day 🙂
[31/01/24, 10:24:05 PM] Anshuman Nimble Box: Blackstone has entered the chat
[31/01/24, 10:24:47 PM] Anshuman Nimble Box: https://www.bloomberg.com/news/articles/2024-01-29/blackstone-is-building-a-25-billion-ai-data-center-empire
[31/01/24, 10:24:54 PM] Nirant : Always
[01/02/24, 6:58:05 AM] ~ Charu G.: ‎~ Charu G. joined from the community
[01/02/24, 7:32:54 AM] ~ Vivek sridhar: ‎~ Vivek sridhar joined from the community
[01/02/24, 2:26:56 PM] ~ Rishabh: ‎Your security code with ‪+1 (650) 308‑6193‬ changed.
[03/02/24, 12:43:03 AM] ~ Antony Paul: ‎~ Antony Paul joined from the community
[03/02/24, 1:22:09 PM] ~ Bhishm Juneja: ‎~ Bhishm Juneja joined from the community
[03/02/24, 6:21:28 PM] ~ Kaustav: ‎Your security code with ‪+91 99016 56650‬ changed.
[03/02/24, 10:20:23 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[03/02/24, 10:53:47 PM] ‪+91 6360 090 212‬: ‎‪+91 6360 090 212‬ joined from the community
[04/02/24, 8:36:23 AM] ~ Saurabh vij: ‎~ Saurabh vij joined from the community
[04/02/24, 6:05:33 PM] ~ Sparsh Drolia: ‎~ Sparsh Drolia joined from the community
[04/02/24, 11:45:56 PM] Pathik Ghugare: ‎Pathik Ghugare joined from the community
[06/02/24, 8:27:35 AM] ~ Amit Timalsina: ‎~ Amit Timalsina joined from the community
[07/02/24, 8:08:24 AM] ~ Ramnandan Krishnamurthy: ‎Your security code with ‪+91 99401 66031‬ changed.
[07/02/24, 8:08:24 AM] ~ Anmol Chawla: ‎Your security code with ‪+91 99699 42690‬ changed.
[07/02/24, 8:08:24 AM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[07/02/24, 12:28:03 PM] Sasank Chilamkurthy: After a long break (1 month hehe), I wrote another blog again. This time, it’s to show how SSA/LLVM IR is basically functional programming. What this means is that your favourite programming language is transformed to functional program to squeeze the max perf. 

 https://chsasank.com/compiler-intermediate-representations-2-ssa-fun.html

Long live functional programming hehe 😛
[07/02/24, 12:28:35 PM] Sasank Chilamkurthy: PS: I was working on hardware during the break from software 🙂. DM me if you’re curious about my hardware. ‎<This message was edited>
[07/02/24, 2:03:06 PM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[07/02/24, 2:18:29 PM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[09/02/24, 10:13:02 AM] ~ vivek raju: ‎~ vivek raju joined from the community
[09/02/24, 12:05:10 PM] ~ Ankit: ‎Your security code with ‪+91 97404 65237‬ changed.
[09/02/24, 12:05:10 PM] ‪+91 98341 69047‬: ‎Your security code with ‪+91 98341 69047‬ changed.
[09/02/24, 12:05:10 PM] ~ Maheswaran Parameswaran: ‎Your security code with ‪+91 99301 29844‬ changed.
[10/02/24, 11:04:51 AM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[10/02/24, 12:34:50 AM] ~ Tanaya Singh: ‎~ Tanaya Singh joined from the community
[10/02/24, 11:04:57 AM] ~ Pratyush: ‎Your security code with ‪+91 98916 21039‬ changed.
[10/02/24, 8:24:16 PM] ~ Sourabh: Any reviews on how reliable HuggingFace Inference endpoints are?
[10/02/24, 8:24:31 PM] ~ Sourabh: for hosting embedding models
[10/02/24, 9:59:02 PM] ~ Sandeep: ‎~ Sandeep joined from the community
[10/02/24, 10:27:09 PM] ~ Ayushman: I would say fairly fast and good. Didn’t really get a downtime. Also you get cool metrics as well in the deployment dashboard as well as a lot of autoscaling options
[11/02/24, 12:19:44 AM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[11/02/24, 10:25:37 AM] ~ Phani Srikanth: ‎Your security code with ‪+91 96866 43995‬ changed.
[11/02/24, 1:09:24 PM] ~ Sourasis Roy: HF is down today though
[11/02/24, 1:12:05 PM] ~ Priyesh Srivastava: Lmao wtf are ppl doing 😂😂
[11/02/24, 1:12:41 PM] ~ YP: It's been down for day ain't it?
[11/02/24, 1:12:54 PM] ~ YP: Just got HF pro two days before😬
[11/02/24, 1:54:04 PM] ~ Sourasis Roy: yupp. still down
[11/02/24, 7:37:28 PM] ~ Abhishek Maiti: ‎Your security code with ‪+91 98880 17773‬ changed.
[11/02/24, 11:06:40 PM] ~ Ananth Radhakrishnan: ‎Your security code with ‪+91 84410 00854‬ changed.
[12/02/24, 5:39:57 AM] ~ Yash: ‎~ Yash joined from the community
[12/02/24, 10:30:13 AM] Abhishek Mishra: ‎Your security code with Abhishek Mishra changed.
[12/02/24, 3:12:24 PM] ~ Wasim Madha: Hey all, 

I want to run python file in dockers with Cuda, can you please share any resources or steps?
[12/02/24, 3:13:01 PM] Sasank Chilamkurthy: DM me will help you. Did that for years
[12/02/24, 7:52:48 PM] ~ Wasim Madha: Thanks @919892727514 

https://docs.omniverse.nvidia.com/isaacsim/latest/installation/install_container.html

This documentation is really helpfull which describes installation from Nvidia Driver to Nvidia Container Toolkit. 

Main issue, I had snap docker was installed, which created all the problems.
[12/02/24, 7:54:12 PM] Sasank Chilamkurthy: Don't do snap stuff ever
[12/02/24, 10:19:16 PM] ~ romit: Sorry, I didn't understand what you meant by that. Do you mean we should not rely on snap for downloading software or just the Nvidia stuff?
[12/02/24, 10:19:42 PM] Sasank Chilamkurthy: never use snap for anything. It is a very bad package distribution system. ‎<This message was edited>
[12/02/24, 10:36:13 PM] ~ Abhik: I second this as well.
[12/02/24, 10:36:51 PM] ~ Abhik: Flatpak is relatively better but support is less.
[12/02/24, 10:37:23 PM] Pathik Ghugare: bad in the sense?
[12/02/24, 10:54:09 PM] Sasank Chilamkurthy: bad integration with system, uses its own services, has a daemon, it’s just not the same as system packages
[12/02/24, 10:54:25 PM] Sasank Chilamkurthy: you might as well use containers if you are using snap
[12/02/24, 10:54:52 PM] Sasank Chilamkurthy: btw anybody tried using guix?
[12/02/24, 11:42:32 PM] Sasank Chilamkurthy: ‎This message was deleted.
[12/02/24, 11:44:48 PM] Sasank Chilamkurthy: Newest blog of mine: how to install drivers and stuff for Intel GPUs. They offer a really good bang for buck - especially if you're into hacking with new devices. In this blog, I show you how to run and benchmark matrix multiplication on Intel GPUs.

https://chsasank.com/intel-arc-gpu-driver-oneapi-installation.html

PS: Now you know what hardware I am working on 😃
[12/02/24, 11:46:02 PM] Sasank Chilamkurthy: DM me if you want to find more about diff GPUs in the market 😀
[13/02/24, 12:02:11 AM] ~ Nayan Shah: For llm inference latency , 
1 ) input tokens + output tokens both matter 
2 ) only output token matter 
3 ) or input and output tokens both matter but input tokens are negligible .
[13/02/24, 12:03:17 AM] ~ Arko C | xylem.ai: Are you asking about TTFT or Inter-token Latency? ‎<This message was edited>
[13/02/24, 12:04:43 AM] ~ Arko C | xylem.ai: Or the time taken for API call + output generation? ‎<This message was edited>
[13/02/24, 12:05:35 AM] ~ Nayan Shah: For whole token generation to api call for  one inference .
[13/02/24, 12:10:04 AM] ~ Arko C | xylem.ai: so that’s a total of the following:
1. Time To First Token or TTFT (which is the time taken for processing the input and generating the first token for streaming)
2. ⁠Response time for generation of all output tokens (depends on the inference speed of the deployment you’re using)
[13/02/24, 12:10:46 AM] ~ Arko C | xylem.ai: in this case. Option 3. Usually the TTFT is <100ms
[13/02/24, 12:10:47 AM] Pathik Ghugare: I think both tokens matter but as the output gets longer, output tokens add more inference time  as compared to initial input tokens
[13/02/24, 12:11:33 AM] ~ Arko C | xylem.ai: Unless you are using something like Falcon 180B without any inference optimisations on something like Sagemaker. Where the TTFT itself is 1-1.5s ‎<This message was edited>
[13/02/24, 12:11:51 AM] ~ Arko C | xylem.ai: And then the inference speeds will be 10-20 tok/s
[13/02/24, 12:12:16 AM] ~ Nayan Shah: Because of it being the sequential and input token acc to transformer architecture will be processed parallel . And for constant token that will be hopefully constant . Will there are ways to reduce the ttft itself
[13/02/24, 12:13:06 AM] ~ Nayan Shah: Yeah i was thinking similar for higher model params this will be too much right ... There should be way to optimise this as well . Sorry new to this so asking and understanding this better
[13/02/24, 12:16:32 AM] ~ Arko C | xylem.ai: yes, that’s why I said without optimisations. For something like a llama2 7B on our infra (Xylem Inference), the TTFT is under 15ms. We haven’t optimised further for 180B yet. 

We are focused more on the inference speeds and throughputs as that effects the final latency much more, cause TTFT is usually negligible for the “relatively” smaller oss models (even llama2 70B is comparatively smaller compared to the larger ones). ‎<This message was edited>
[13/02/24, 12:17:57 AM] ~ Nayan Shah: Hm thats good to know 🙌
[13/02/24, 12:17:59 AM] Pathik Ghugare: but if I've 'm' output tokens then the decoder will be called m times so it is kinda like O(n+k*m)) complexity right? 
where n is the initial input tokens, 
m is number of output tokens
k is time required by decoder to process the tokens in the context parallely 

so what does that mean as m increases total inference time would increase linearly?
[13/02/24, 12:19:24 AM] ~ Nayan Shah: Not sure , i am hoping that would be the case.
[13/02/24, 12:20:32 AM] ~ Arko C | xylem.ai: there’s something that we are missing here. There’s a hidden piece of “network latency” which is based on which region your VM is located in. ‎<This message was edited>
[13/02/24, 12:23:55 AM] ~ Arko C | xylem.ai: This is overall how it is. But the “k” is very very unpredictable in shared instances. You can have network latency, then based on the load on VM the speeds will go down, etc ‎<This message was edited>
[13/02/24, 12:25:47 AM] Pathik Ghugare: True 
What is the average network latency for, say, the US East regions if I call APIs from the AP-South region?
[13/02/24, 12:31:22 AM] ~ Arko C | xylem.ai: See that is a bit subjective. I would just suggest spinning up an instance for yourself and testing it for your use case to define it.
[13/02/24, 8:37:14 AM] ~ Haresh: ‎~ Haresh joined from the community
[13/02/24, 9:50:21 AM] ~ Tanmay Sachan: ‎~ Tanmay Sachan joined from the community
[13/02/24, 9:56:15 AM] Bharat Shetty: Great read
[13/02/24, 11:01:55 AM] Sasank Chilamkurthy: Anybody worked with MLX and familiar with inner details of MLX? ‎<This message was edited>
[13/02/24, 11:43:24 AM] ~ Kapil: ‎~ Kapil joined from the community
‎[13/02/24, 8:47:12 PM] Sasank Chilamkurthy: ‎image omitted
[13/02/24, 9:59:59 PM] ~ Kshiteej: I think it would also be interesting to see the numbers for different low precision datatypes like float16 and bf16 as few GPU devices have specialized hardware for the same.

Keep up the great work 🙌
Thanks!
[13/02/24, 10:01:16 PM] ~ Nishant: What does it mean with Apple M2 GPU having lower bandwidth than an Apple M1 GPU?
[13/02/24, 10:26:11 PM] Sasank Chilamkurthy: In time, in time.
[13/02/24, 10:26:59 PM] Sasank Chilamkurthy: Wait that's a typo. Lemme fix. ‎<This message was edited>
[13/02/24, 10:27:17 PM] Sasank Chilamkurthy: This I mean.
[13/02/24, 10:30:50 PM] ~ Shishira Nataraj: My stack for local inferencing:

2 X G100V 32GB GPUs

Intel Xeon processor 

256GB RAM 

Running Ubuntu server 



Model I’m running right now: mistral 13B q4 


It’s running well. But the response times are not as good as I expected it to be. 


Each response is taking about 4-5s. Of about 1000 tokens I/O. 


Want to use this server to host some internal RAG tools for about 50 parallel users. 


What should I look at next to improve the response times?
[13/02/24, 10:31:47 PM] ~ Shishira Nataraj: It’s able to process 2 of such prompts at a time while revving the GPU’s to max, before it starts offloading to CPU.
‎[13/02/24, 10:32:05 PM] Sasank Chilamkurthy: ‎image omitted
[13/02/24, 10:32:08 PM] ~ Shishira Nataraj: Have implemented a queue as of now to serve requests
[13/02/24, 10:32:34 PM] Sasank Chilamkurthy: what is a g100v gpu? never heard
[13/02/24, 10:34:28 PM] ~ Shishira Nataraj: I think it is V100. Sorry. 

Nvidea’s tesla series of GPUs
[13/02/24, 10:36:14 PM] ~ Shishira Nataraj: It’s 3-4 years older ones. Doesn’t support fp32 afaik 

Had this server lying around in company which was previously used to inference vision apps. ‎<This message was edited>
[13/02/24, 10:36:57 PM] Sasank Chilamkurthy: Help me with these benchmarks 😀
[13/02/24, 10:37:25 PM] Sasank Chilamkurthy: But jokes aside, what package are you using for inference?
[13/02/24, 10:37:29 PM] ~ Adarsh: they are bad lol. Have it at my uni. no bf16 support. Flash attn also doesn't support it etc.
[13/02/24, 10:38:01 PM] ~ Shishira Nataraj: Llama cpp python
[13/02/24, 10:39:19 PM] Sasank Chilamkurthy: do llama-bench pls
[13/02/24, 10:39:20 PM] ~ Shishira Nataraj: Haha yeah. I know. 

They were terrible for vision apps too. Hence were removed from production racks. We used it to run benchmarks internally that’s all. 


In comparison, will a RTX 4090 outperform them? ‎<This message was edited>
[13/02/24, 10:39:45 PM] Sasank Chilamkurthy: Run this script and you’ll know
[13/02/24, 10:40:18 PM] ~ Shishira Nataraj: I’ll share result tomorrow ‎<This message was edited>
[13/02/24, 10:43:12 PM] Sasank Chilamkurthy: Also I have started working on newer benchmark around llama cpp: https://github.com/chsasank/device-benchmarks/blob/main/llama_bench.sh. 'Beta' right now but you can run it as `bash llama_bench.sh cuda`
[13/02/24, 10:56:16 PM] ~ Kartik Sangani: ‎~ Kartik Sangani joined from the community
[13/02/24, 11:04:52 PM] ~ Ashish Patel: https://wccftech.com/nvidia-chat-with-rtx-ai-chatbot-windows-pcs-tensorrt-llm-available-all-rtx-30-40-gpus/
[13/02/24, 11:29:03 PM] ~ Sid: ‎~ Sid joined from the community
[13/02/24, 11:31:45 PM] ~ Sid: hi guys, I'm planning to get a PC and my budget is around 2lakh.
gpu is selected : rtx 4070 ti super 16gb.
I'm stuck on processor. i7-14700k or i9-14900k or some AMD processor.

tasks to be run on pc are text generation llm, stable diffusion and gaming ofcourse but i play dota 2 mostly.

any suggestions.. let me know if it's not appropriate for the group, I'll remove the post.
[13/02/24, 11:32:22 PM] Ojasvi Yadav: 3090
[13/02/24, 11:32:29 PM] Ojasvi Yadav: Optimise for vram
[13/02/24, 11:32:39 PM] Ojasvi Yadav: 4070 is 12gb afaik?
[13/02/24, 11:32:59 PM] Ojasvi Yadav: You can get a 12gb 3060 for 25k too if that's the vram you want
[13/02/24, 11:33:03 PM] ~ Sid: 4070 ti super is latest release with 16gb VRAM.
[13/02/24, 11:33:12 PM] Ojasvi Yadav: Price?
[13/02/24, 11:33:21 PM] ~ Sid: 82k.
[13/02/24, 11:33:25 PM] Ojasvi Yadav: Dude
[13/02/24, 11:33:34 PM] ~ Sid: i want to get 16gb actually.
[13/02/24, 11:33:40 PM] Ojasvi Yadav: Get dual 3060s 12gb and it would still cost you 50-60k
[13/02/24, 11:33:53 PM] Ojasvi Yadav: And you'll have 24gb vram to play with
[13/02/24, 11:34:08 PM] ~ Sid: i see many options in 12gb but there are only few in 16gb.
and 4090 16gb is 2lakh 😅
[13/02/24, 11:34:10 PM] Ojasvi Yadav: Or up your budget and go for 3090
[13/02/24, 11:34:21 PM] Ojasvi Yadav: Don't go for the 4000 rang brother
[13/02/24, 11:34:36 PM] ~ Sid: we can put two gpu??
[13/02/24, 11:34:42 PM] Ojasvi Yadav: 3000 range offers a better vram  to price ratio
[13/02/24, 11:34:59 PM] ~ Sid: softwares and games support gpu sharing?
[13/02/24, 11:35:07 PM] Ojasvi Yadav: Of course, you need a suitable motherboard though
[13/02/24, 11:35:15 PM] Ojasvi Yadav: AI does, games don't
[13/02/24, 11:35:28 PM] Ojasvi Yadav: If you want to do both, then save up for 3090
[13/02/24, 11:35:29 PM] ~ Sid: okk.. nice to know. I'll check that.
[13/02/24, 11:35:33 PM] Ojasvi Yadav: Best bang for buck
[13/02/24, 11:35:47 PM] Bharat Shetty: this is strange. games havent caught up with times :) ?
[13/02/24, 11:35:49 PM] Ojasvi Yadav: 3090 can play any games you throw at it
[13/02/24, 11:35:49 PM] ~ Sid: any idea about processor?
[13/02/24, 11:36:13 PM] ~ Sid: 3090 is costly. around 1.2L
[13/02/24, 11:36:13 PM] Adithya Kannada LLM: You can probably try this 
https://mdcomputers.in/inno3d-rtx-4060-ti-twin-x2-oc-n406t2-16d6x-178055n.html
[13/02/24, 11:36:13 PM] Adithya Kannada LLM: 4060ti 16 gb vram
[13/02/24, 11:36:28 PM] ~ Bk: ASUS ROG series is quite good.
[13/02/24, 11:38:58 PM] ~ Sid: there was some article which mentions how much RAM do you need to load x billion parameter model. anyone has link of any of those?
[13/02/24, 11:42:57 PM] Bharat Shetty: A single-model parameter, at full 32-bit precision, is represented by 4 bytes. Therefore, a 1-billion-parameter model requires 4 GB of GPU RAM just to load the model into GPU RAM at full precision.
[13/02/24, 11:43:23 PM] Bharat Shetty: https://www.oreilly.com/library/view/generative-ai-on/9781098159214/ch04.html#:~:text=A%20single%2Dmodel%20parameter%2C%20at,GPU%20RAM%20at%20full%20precision.
[13/02/24, 11:43:39 PM] ~ Sid: thanks... 😊
[14/02/24, 12:03:51 AM] ~ Adarsh: https://tokentally.streamlit.app/
You might find something here. A calculator sorts I made for this exact purpose 

https://cursor.sh/blog/llama-inference
This is a really good blog on all inference memory requirements

https://blog.eleuther.ai/transformer-math/ ‎<This message was edited>
[14/02/24, 12:06:05 AM] ~ Adarsh: https://kipp.ly/transformer-inference-arithmetic/

This ones great too
[14/02/24, 9:02:27 AM] ~ Kartik Sangani: ‎Nirant  removed ~ Kartik Sangani
[14/02/24, 10:06:52 AM] ~ Abhinash Khare: ‎~ Abhinash Khare joined from the community
‎[14/02/24, 12:08:08 PM] ~ Arya: ‎image omitted
[14/02/24, 7:03:24 PM] ~ Amit: ‎~ Amit joined from the community
[15/02/24, 9:11:37 AM] ~ Krishna Panchal: craigslist for gpu clusters: gpulist.ai

Check it out if you're looking for gpus. And if you have any extra hardware you want to rent out, posting is free.

A GPU cluster from India is also available. This initiative was started by Nat Friedman.
[15/02/24, 4:33:49 PM] ~ Sagar: ‎~ Sagar joined from the community
[16/02/24, 3:09:53 AM] ~ Sid: ‎This message was deleted.
[16/02/24, 10:11:04 AM] ~ Amit Bhor: Here 

https://twitter.com/daamitt/status/1699697744418640219?t=6wTP-vJZdaR8q45rWviT8A&s=19
[16/02/24, 1:53:57 PM] ~ Nikhil Chintawar: ‎~ Nikhil Chintawar joined from the community
[17/02/24, 11:48:03 PM] ~ Chetan: ‎Your security code with ‪+91 90008 44590‬ changed.
[18/02/24, 10:36:12 AM] ~ Sparsh: ‎~ Sparsh joined from the community
[18/02/24, 11:34:58 AM] ~ Gurminder: ‎~ Gurminder joined from the community
[18/02/24, 3:44:21 PM] ~ Sundar: ‎~ Sundar joined from the community
[19/02/24, 10:34:55 AM] Nirant : Groq CEO demos and note that they mention their compute is going to be smaller, faster, and cheaper: https://www.youtube.com/watch?v=P3AhvahzRjA
[19/02/24, 9:39:07 PM] ~ Deepesh: ‎~ Deepesh joined from the community
[19/02/24, 10:22:56 PM] ~ Kapil: ‎Nirant  removed ~ Kapil
[20/02/24, 11:04:56 AM] ~ sajith: ‎~ sajith joined from the community
[20/02/24, 1:46:25 PM] ~ Prateek🖤: Quick query folks:

Which is best for accelerated model inference? 
Onnx or openvino or torchscript?
[20/02/24, 1:49:01 PM] ~ Sparsh: openvino is for intel only no?

Checkout this - 4x faster inference
https://gist.github.com/ArthurZucker/af34221def212259b43d55a2811d2dbb
[20/02/24, 1:51:15 PM] ~ Adarsh: I think ONNX 

For SD-Turbo:
ONNX + Cuda - 103ms
ONNX + TensorRT - 77ms
TensorRT - 88ms

But don't have a comparison bw ONNX and openvino...

Ref: https://twitter.com/cto_junior/status/1756261153800360062?t=1xeD9p1pKB2ntRqixgClZA&s=19
[20/02/24, 1:52:43 PM] ~ Akshat Gupta: ‎~ Akshat Gupta joined from the community
[20/02/24, 1:54:41 PM] ~ Prateek🖤: Yes
[20/02/24, 1:54:46 PM] ~ Prateek🖤: Let me check this out
[20/02/24, 2:11:40 PM] ~ Prashanth Harshangi: Onnx on tensorrt
[20/02/24, 2:12:32 PM] ~ Sagar: Will vote for this. Avoid torchscript if you are starting exploration .
‎[20/02/24, 2:50:37 PM] ~ Arko C | xylem.ai: ‎image omitted
[20/02/24, 2:54:08 PM] ~ Adarsh: I think an rtx 3050 XD
[20/02/24, 2:54:10 PM] ~ Arko C | xylem.ai: So you are telling me that 576*$20K = $11.5M will get me this? Hypothetically, give me that capital and I will get you over 100K tokens/sec throughput on A100s
[20/02/24, 2:55:27 PM] ~ Adarsh: I think they scale linearly for some reason. Still dont understand how exactly they are doing it.
[20/02/24, 3:00:31 PM] ~ Arko C | xylem.ai: They are distributing the model weights across racks. Each GroqRack has 9 nodes. Each node has 8 GroqCard adapters. And they will deploy a Llama2 70B on 576 chips to get 300 tokens/sec.

Each chip will be $20K. So you will spend $11.5M for this.


So if a Mixtral 8x7B needs 6 racks to just deploy it’s weights. You are scaling linearly by a factor of 6 every time?
[20/02/24, 3:01:40 PM] ~ Arko C | xylem.ai: Fun part of hype cycle. Hardly anyone goes deeper to look at this. 500 tokens/sec is flashy enough to cover this sh*t up.
‎[20/02/24, 3:05:11 PM] ~ Raghav Ravishankar: ‎image omitted
[20/02/24, 3:06:35 PM] ~ Arko C | xylem.ai: It’s Throughput vs Price

The output is highest on y-axis, so they are above others. But the pricing is same as Lepton and Deepinfra on x-axis.
[20/02/24, 3:07:47 PM] ~ Sparsh: Throughput vs Price is the only metric that matters?
Deepinfra is the cheapest available right now & groq matched their pricing
[20/02/24, 3:08:44 PM] ~ Arko C | xylem.ai: Now if Groq is throwing 576 chips to hit this on llama2-70B, no way that Lepton or Deepinfra is throwing that many. Mostly it’s a 4xA100 80GB for them.


So even if you buy the cards.

Groq spends $11.5M

While Lepton and Deepinfra spent 4x$20K = $80K
[20/02/24, 3:08:59 PM] ~ Arko C | xylem.ai: Exactly! Classic raise and burn strategy ‎<This message was edited>
[20/02/24, 3:10:51 PM] ~ Nijil Y: How many parallel sessions/thread have you been able to achieve on a t4/a100 gpu with a llama2 7b unquantized hf model. For inference ‎<This message was edited>
[20/02/24, 3:11:59 PM] ~ Nijil Y: I am not able to run more than 1 at max even by limit layer to 22-30. Pls suggest any pointers
[20/02/24, 3:14:13 PM] ~ Arko C | xylem.ai: Are you talking about batch sizes?
[20/02/24, 3:14:49 PM] ~ Nijil Y: Not exactly. More for multi user/session querying a deployed model
[20/02/24, 3:17:27 PM] ~ Nijil Y: I assume batch size come more during training context and less on inference? I could be wrong
[20/02/24, 3:18:46 PM] ~ Arko C | xylem.ai: Batch size is basically number of parallel requests.

We do 64 on single A100 80GB for non-quantised Llama2-7B.

Not the max. We are pushing it higher. ‎<This message was edited>
[20/02/24, 3:19:19 PM] ~ Nijil Y: Oh neat. And how is the latency?
[20/02/24, 3:20:29 PM] ~ Arko C | xylem.ai: Have to measure again. <30ms for sure. But have to measure again and confirm.

TTFT is what I’m talking about here
[20/02/24, 3:22:21 PM] ~ Nijil Y: Got it. And how many token is it able to churn per request assuming it's handling ~64 request full load (or it doesn't matter? )
[20/02/24, 3:22:31 PM] ~ Nijil Y: Per second
[20/02/24, 3:26:55 PM] ~ Arko C | xylem.ai: It does matter. It’ll have some losses. But we haven’t released the numbers publicly yet on our end for Xylem Inference.
[20/02/24, 3:27:57 PM] ~ Arko C | xylem.ai: Happy to chat offline if you want
[20/02/24, 3:28:34 PM] ~ Nijil Y: Thank you. Will ping. Testing few things to get better data point
[20/02/24, 3:29:20 PM] ~ Arko C | xylem.ai: Sure :)
[20/02/24, 11:39:41 PM] ~ Amit Bhor: How is groq different from TPUs? Anyone knows.
[20/02/24, 11:40:47 PM] ~ rohit: LPU
[20/02/24, 11:41:02 PM] ~ rohit: would be exp on prod tho
[20/02/24, 11:41:11 PM] ~ Nijil Y: Has anyone got access yet?
[20/02/24, 11:41:44 PM] ~ rohit: closed access + few oss labs
[20/02/24, 11:47:20 PM] Jibin Sabu: Releasing API sometime this month, saw the demo though
[20/02/24, 11:48:00 PM] ~ Nijil Y: It's gonna change unit economics a lot 😁
[20/02/24, 11:48:17 PM] ~ Nijil Y: Hope it works as advertised
[20/02/24, 11:48:30 PM] ~ rohit: expensive
[20/02/24, 11:49:57 PM] ~ Nijil Y: 1 mil token was some 0.1 usd. Considering gpu cost seems a lot cheaper at that performance
[21/02/24, 12:07:02 AM] ~ Amit Bhor: So it's being subsidised atm?
[21/02/24, 12:08:05 AM] ~ Nijil Y: I don't think so. The architecture by design allows it to do that. Doesn't have the silicon overhead of gpu is what I presume and purpose designed
[21/02/24, 12:10:06 AM] ~ Amit Bhor: Any good reading on this LPU?
[21/02/24, 12:10:10 AM] Dr. Pratik Desai: Subsidized? Everything is subsidised. Question is marginally, heavily or outrageously subsidized.  https://twitter.com/jiayq/status/1759858126759883029
[21/02/24, 12:12:32 AM] ~ Amit Bhor: From the very little i read the LPU seems something like predictive decoding, so there must be some down sides.
‎[21/02/24, 12:38:28 AM] ~ Arko C | xylem.ai: ‎image omitted
[21/02/24, 12:41:54 AM] ~ Amit Bhor: And batch size of 1, very perplexing
[21/02/24, 12:42:34 AM] ~ Arko C | xylem.ai: How is the architecture allowing it to do that? It has solved for the memory bandwidth issues but at the cost of requiring massive upfront investments.

What you need to account for here, is that none of the API providers are selling APIs as the primary business. It’s to help you deploy and scale custom LLMs and do VPC deployments.

That is not possible with Groq.

So if someone wanna use a base open-source LLM. Then they should go and use Groq, but at that point, just use GPT as the quality will be far better than any other general purpose open source LLM.

Cause what Together, Lepton, Fireworks and we solve for, is literally the issue with training, deploying and scaling custom LLMs. Can be fine-tuning or pre-training.

Which company will reserve 6 GroqRacks to deploy their fine-tuned Mixtral8x7B? Each rack has 9 nodes  and each node has 8 cards, so a total of 432 Gorqchips and each at $20K is a total infra cost of $8.6M ‎<This message was edited>
[21/02/24, 2:05:21 AM] ~ Nijil Y: ya at 20k per card make no sense. either they have to scale big so that unit cost goes down and/or stick with their api at the current cost and build a aws sort system where they recover it over a couple of years.
[21/02/24, 2:05:56 AM] ~ Nijil Y: or finding some way to mux more on board memory in their further models
[21/02/24, 11:03:56 AM] ~ Nishkarsh | usefindr.com: Has anyone received access to/tried out Groq’s inferencing?
[21/02/24, 12:43:25 PM] ~ Mahesh Sathiamoorthy: The CEO said “We're very comfortable at this price” regarding the current low API cost.

https://twitter.com/JonathanRoss321/status/1759820114495492124
[21/02/24, 12:44:30 PM] ~ Harsh Gupta: Chip manufacturing has very low opex. Once they get scale, they can get reduce the cost tremendously
[21/02/24, 9:26:28 PM] ~ Ashish Patel: ‎Your security code with ‪+91 84602 38402‬ changed.
[22/02/24, 1:10:50 AM] ~ Arko C | xylem.ai: I mean he definitely can’t say that he isn’t 👀
[22/02/24, 12:30:21 PM] ~ Tricha: ‎~ Tricha joined from the community
[22/02/24, 12:49:21 PM] ~ Amartya | CodeAnt(YC W24): ‎Your security code with ‪+91 90870 04481‬ changed.
[22/02/24, 12:56:20 PM] ~ Amartya | CodeAnt(YC W24): ‎Your security code with ‪+91 90870 04481‬ changed.
[22/02/24, 4:43:37 PM] ~ Kishore M R: Wanted to build a rtx 4090  or equivalent system to infer quantized 7B  models . Please point to opinions,  suggestions , resources if any. Any preferred resellers in India?
[22/02/24, 4:45:06 PM] Ojasvi Yadav: Save your money and get a 3090
[22/02/24, 4:45:09 PM] ~ Ayushman: ‎Your security code with ‪+91 60007 05207‬ changed.
[22/02/24, 4:45:29 PM] Ojasvi Yadav: Why pay nearly double for the same performance
[22/02/24, 4:45:49 PM] Ojasvi Yadav: You could even get 2 3090s, enabling you to run 70B models
[22/02/24, 4:46:16 PM] Ojasvi Yadav: 2x3090 will cost you around the same as a single 4090
[22/02/24, 4:49:30 PM] ~ Kishore M R: Thanks. If someone has posted their buid spec. Sharing  would be helpful
[22/02/24, 4:56:00 PM] ~ Sid: i got 4070 ti super 16gb with 64gb ram
Llama 70b and llava 33b is running effortlessly using ollama
[22/02/24, 4:58:40 PM] Ojasvi Yadav: Ryzen 7600, 3090+3060 -> 36 GB vram in total, 64 gigs of ram, 3tb storage, 850W PSU
[22/02/24, 4:59:29 PM] Ojasvi Yadav: Ensure to have good cooling as GPU generate the most heat and also tune their fans accordingly
[22/02/24, 5:00:12 PM] ~ Sid: i was thinking of getting two 3090 gpu as @919971004124 suggested but games are not optimized for dual gpus.
and you have to check third party apps for llms are supporting dual gpu or not.
[22/02/24, 5:02:42 PM] Ojasvi Yadav: Budget wise, get as many 3090s as you can afford. 

CPU doesn't matter much, and neither does expensive RAM. Can save cost here in favour of 3090s. If you're going to do inference then 1TB nvme is enough. If you're going to train aim for 2 or 3 TBs.
[22/02/24, 5:03:14 PM] Ojasvi Yadav: Bro which game is not going to run on a 3090
[22/02/24, 5:04:07 PM] Ojasvi Yadav: I play CoD at ultra settings on a 32:9 monitor at 200fps, all running on the 3090
[22/02/24, 5:05:58 PM] Ojasvi Yadav: Games don't use multi GPUs so assign the game to your best GPU from the game settings in windows
[22/02/24, 5:07:37 PM] ~ Kishore M R: I read that having multiple GPUs need proper bridge like nvlink. Seems 3090 supports but I could  not info on building such a link or cost involved. Does the nvidia drivers take care of abstracting the two memories as one?
[22/02/24, 5:13:58 PM] Ojasvi Yadav: Depends on your code. Most local LLM inference engines like ollama take care of automatically distributing the layers across multi GPUs. I don't have nv link. When I run mixtral its layers get divided across my 3090 and 3060. ‎<This message was edited>
[22/02/24, 5:14:14 PM] Ojasvi Yadav: And runs like butter
[22/02/24, 5:23:39 PM] ~ Sid: true
[22/02/24, 6:27:21 PM] ~ Abhishek Maiti: How is your gpu interconnected then? I thought nvlink was the only way?
[22/02/24, 6:29:19 PM] Ojasvi Yadav: Motherboards PCIE lanes
[22/02/24, 6:30:00 PM] Ojasvi Yadav: If you run nvidia-smi and are able to see both your GPUs in your terminal, it will work ‎<This message was edited>
[22/02/24, 6:31:00 PM] ~ Abhishek Maiti: Aah okay, yes.
[22/02/24, 6:32:13 PM] Adithya Kannada LLM: Nvlink works really well with deepspeed and accelerate
[22/02/24, 6:33:18 PM] ~ Abhishek Maiti: Right, just looked it up, pcie does the trick but nvlink is faster
[22/02/24, 6:33:29 PM] Ojasvi Yadav: Few caveats
[22/02/24, 6:33:35 PM] ~ Sid: you need to get a motherboard that support dual gpu's
[22/02/24, 6:34:22 PM] Ojasvi Yadav: You need to have the same generation and I remember reading somewhere that both GPUs need to be the same
[22/02/24, 6:34:24 PM] ~ Kishore M R: Any suggestions?
[22/02/24, 6:35:07 PM] Ojasvi Yadav: Unsure about the second condition but for the first one you do need either both 3000 or 4000 generation
[22/02/24, 6:35:23 PM] Ojasvi Yadav: In 3000 the only GPU with nvlink is 3090, not the others
[22/02/24, 6:35:31 PM] Dr. Pratik Desai: NVIDIA stopped supporting nvlink in RTX GPUs after 3090. @919971004124 that’s why I was kind of hard on getting you 3090s.
[22/02/24, 6:35:45 PM] Ojasvi Yadav: Great minds
[22/02/24, 6:35:55 PM] Dr. Pratik Desai: 4090 is better but no NVlink
[22/02/24, 6:35:57 PM] Ojasvi Yadav: Jinx
[22/02/24, 6:36:48 PM] Ojasvi Yadav: In all practical purposes nvlink is not needed for locall LLMs
[22/02/24, 6:37:07 PM] Ojasvi Yadav: Motherboards PCIE lanes are good enough
[22/02/24, 6:37:27 PM] Ojasvi Yadav: I barely see any difference with the x8 lanes vs the x16 lanes
[22/02/24, 6:38:45 PM] Ojasvi Yadav: Most motherboards with multiple PCIE slots should do the job. But please Ensure they're distant enough, otherwise your GPUs will either not fit or they'll be too close for proper cooling.
[22/02/24, 6:39:00 PM] Dr. Pratik Desai: What I bought $800 new goes for $1000 used. No wonder NVIDIA stock going crazy.
[22/02/24, 6:39:26 PM] Dr. Pratik Desai: It does when you have a 7-8 GPU rig like Anton
[22/02/24, 7:41:32 PM] ~ Harshita: ‎~ Harshita joined from the community
[22/02/24, 8:42:14 PM] Bharat Shetty: which gpu you brought ?
[22/02/24, 8:43:00 PM] Dr. Pratik Desai: 3090s, that was 2022
[22/02/24, 8:43:10 PM] Bharat Shetty: Cool! thanks!
[24/02/24, 4:42:35 PM] ~ Sayan: I am thinking for the same build. I got some quotes from MVP folks they were all 4L+ with threadripper single 4090 going up to 6L. HP has a build with 4090 i9 13 th gen at around 3.5L
[24/02/24, 4:46:36 PM] ~ Abhinash Khare: As others have suggested, if purpose is LLM development RTC 4090 with 24GB can be limiting for some use case. I heard that upcoming top tier 50XX series will come with 32GB this year.
[24/02/24, 10:39:28 PM] ~ Sayan: https://lambdalabs.com/blog/nvidia-rtx-4090-vs-rtx-3090-deep-learning-benchmark#PCIe-gen-4


A very nice data driven blog on what we were discussing yesterday
[24/02/24, 11:19:16 PM] Bharat Shetty: thanks, useful!
[25/02/24, 12:46:45 PM] ~ Mani: ‎~ Mani joined from the community
[25/02/24, 4:16:11 PM] ~ SJ: ‎Your security code with ‪+91 70207 31483‬ changed.
[26/02/24, 10:20:26 AM] ~ Parth Sarthi: ‎~ Parth Sarthi joined from the community
[26/02/24, 10:30:21 AM] ~ Ajay: Hey guys, what is the general framework folks follow to increase throughout per second with inference of self hosted open source models?

For example if I host the Llama-2-70-chat model and I have 5 concurrent users that I want to serve for my chatbot, how would I do that?
[26/02/24, 10:55:31 AM] ~ Shishira Nataraj: Check your GPU utilisation. If you’re crossing 80% per prompt, there’s not much you can do but queue them up and run them sequentially. 

The other option is exploring on fine tuning your models. Maybe trim down unnecessary parameters or merge two smaller models which work the best for you. 

You can try Mixtral 8 x 7b meanwhile. It kinda ensembles 8 different 7b LLMs but only gets context from any two based on prompts. Which reduced your compute while maintaining your response quality.
[26/02/24, 10:57:10 AM] ~ Shishira Nataraj: The other obvious thing is to reduce your i/o tokens limit and turn chat history off. ‎<This message was edited>
[26/02/24, 11:02:36 AM] ~ Ajay: So I've been using ollama to run open source models on my local machine. And in the cloud, I've just used APIs ( Open AI, Bedrock, Azure Open AI ). The problem with APIs is that one of the design partners we're speaking to is very particular that the model has to be deployed in our VPC ( they've agreed that our VPC can host the model and not theirs ) - so I'm actually first trying to understand what is the fastest way to get to a reasonably high throughput deployment in our own GPU instances ( an H100 and a couple of A10s )

Just to make sure I clarify what I mean by throughput, I'm referring to the number of user requests I can serve per second. Each user request is basically a conversation waiting for an answer ( typical chatbot ) ‎<This message was edited>
[26/02/24, 1:53:01 PM] ~ Nilesh Christopher: ‎~ Nilesh Christopher joined from the community
[26/02/24, 2:26:01 PM] ~ rohit: ‎Your security code with ‪+91 98716 26616‬ changed.
[26/02/24, 8:05:10 PM] ~ Sandeep Srinivasa: Hi folks - which nvidia docker image are you guys using for training? I'm having a slightly tough time getting a docker image to work with AWS GPU machines.
[26/02/24, 8:15:17 PM] ~ Harsh Gupta: https://mistral.ai/news/mistral-large/
[26/02/24, 8:15:29 PM] ~ Harsh Gupta: ^ A mistral large model just dropped
[26/02/24, 8:23:43 PM] Pathik Ghugare: Check this out it helps 
https://www.linkedin.com/posts/maxime-labonne_gptfast-accelerate-your-llms-6-7x-pytorch-activity-7167882109926248448-dWvR?utm_source=share&utm_medium=member_ios
[27/02/24, 7:13:28 PM] ~ Ambarish Ganguly: ‎Your security code with ‪+91 90076 47019‬ changed.
[27/02/24, 9:20:38 PM] ~ Surya Penmetsa: ‎~ Surya Penmetsa joined from the community
[28/02/24, 2:02:13 AM] ~ Surya Penmetsa: Which GPU would I need to run a llama2-70b model? What are the VRAM requirements to be able to fit this model on a GPU?
[28/02/24, 7:47:48 AM] ~ romit: If you want to run it at a reasonable accuracy, you can run it in 4bit, which would mean every parameter would be allocated half byte. So in total 35GB of VRAM will be required to run the llama 2 model
[28/02/24, 7:48:36 AM] ~ romit: Either two 4090s or one a100/a6000. Best to take a series one on rent
[28/02/24, 5:26:08 PM] ~ Surya Penmetsa: Noted, thanks Romit!
[28/02/24, 7:52:47 PM] ~ Ishita Jindal: ‎~ Ishita Jindal joined from the community
[28/02/24, 10:46:23 PM] ~ Sri Krishna: good podcast on Groq LPUs with their head of si https://www.youtube.com/watch?v=WQDMKTEgQnY
[29/02/24, 9:22:44 AM] ~ Vivek Kaushal: ‎~ Vivek Kaushal joined from the community
[29/02/24, 10:13:08 AM] ~ Manas Jain: ‎~ Manas Jain joined from the community
[29/02/24, 8:13:50 PM] ~ Gokul Ramakrishnan: ‎~ Gokul Ramakrishnan joined from the community
[29/02/24, 9:01:34 PM] Ritesh: ‎Your security code with Ritesh changed.
[29/02/24, 9:17:30 PM] Ritesh: ‎Your security code with Ritesh changed.
[29/02/24, 10:49:39 PM] Ritesh: ‎Your security code with Ritesh changed.
[01/03/24, 9:23:07 AM] ~ Prakash Sanker: ‎~ Prakash Sanker joined from the community
[01/03/24, 6:57:09 PM] ~ Immidisetty Anudeep: ‎~ Immidisetty Anudeep joined from the community
[01/03/24, 7:16:01 PM] ~ Samruddhi Mokal: ‎~ Samruddhi Mokal joined from the community
[01/03/24, 9:05:48 PM] ~ Dev: ‎~ Dev joined from the community
[01/03/24, 9:44:12 PM] ~ Ashish Sardana: ‎~ Ashish Sardana joined from the community
[01/03/24, 11:39:11 PM] ~ Naman | Repello AI: ‎~ Naman | Repello AI joined from the community
[02/03/24, 8:01:51 AM] ~ prakashpvss: ‎~ prakashpvss joined from the community
[02/03/24, 4:33:35 PM] ~ Aryaman (Repello AI): ‎Your security code with ‪+91 85272 16039‬ changed.
[02/03/24, 11:53:05 PM] ~ Dilip Thomas Ittyera: ‎~ Dilip Thomas Ittyera joined from the community
[03/03/24, 12:59:02 AM] ~ Adithya S K: ‎Your security code with ‪+91 91 485 743 93‬ changed.
[03/03/24, 11:33:14 AM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[03/03/24, 1:05:56 PM] ~ Kishore M R: Need suggestions. I got an offer of used 3090 cards 1.5 years old supposedly  used at a  gaming centre for  65k each. Other than general working condition, is there anything that I should look for before shelling out money? If worth , Is it a good deal? TIA
[03/03/24, 1:08:27 PM] ~ Kishore M R: 10 months warranty available it seems
[03/03/24, 1:09:42 PM] Dr. Pratik Desai: Good price. My 3090s have been running for 18 months 24/7, and still going. eBay used price is $850, so almost same.
[03/03/24, 1:24:58 PM] ~ romit: Hey Kishore, do they have more units? I was also looking for one
[03/03/24, 2:02:27 PM] ~ Kishore M R: Let me check
[04/03/24, 1:17:59 AM] ~ Chirasmita Mallick: ‎Your security code with ‪+91 97171 04683‬ changed.
[04/03/24, 3:24:40 AM] ~ Amit: ‎~ Amit joined from the community
[04/03/24, 1:31:28 PM] ~ HP: ‎~ HP joined from the community
[04/03/24, 5:28:27 PM] ~ Aryaman (Repello AI): ‎Your security code with ‪+91 85272 16039‬ changed.
[05/03/24, 10:40:38 PM] ~ Chirasmita Mallick: ‎Your security code with ‪+91 97171 04683‬ changed.
[06/03/24, 11:54:20 AM] ~ Nikhil: ‎~ Nikhil joined from the community
[07/03/24, 8:36:46 PM] ~ Praveen: ‎~ Praveen joined from the community
[09/03/24, 1:15:58 PM] ~ Atik Shaikh: ‎Your security code with ‪+91 72193 88991‬ changed.
[10/03/24, 11:39:39 PM] ~ rohit: ‎Your security code with ‪+91 98716 26616‬ changed.
[11/03/24, 8:08:42 AM] ~ Srajan Gupta: ‎~ Srajan Gupta joined from the community
[12/03/24, 11:22:32 PM] ~ Nafeen: ‎Your security code with ‪+91 89713 87666‬ changed.
[13/03/24, 1:48:27 AM] ~ Muskan: ‎~ Muskan joined from the community
[13/03/24, 5:51:48 PM] ~ Avinash: ‎Your security code with ‪+91 6301 644 549‬ changed.
[13/03/24, 9:01:06 PM] ~ Achal: ‎~ Achal joined from the community
[14/03/24, 9:32:55 AM] ~ Suryaprakash Konanuru: ‎~ Suryaprakash Konanuru joined from the community
[15/03/24, 5:58:14 AM] ~ Rishabh: ‎Your security code with ‪+1 (650) 308‑6193‬ changed.
[15/03/24, 10:46:00 AM] ~ Sachin Kalsi: https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/

1. Meta’s long-term vision is to build AGI that is open & widely available for everyone
2. Meta handles hundreds of trillions of AI model executions per day
3. Hardware clusters: equivalent to ~600K H100s 
4. LLaMA 3' training hardware, network
[15/03/24, 2:52:51 PM] ~ Karthikeyan Vijayan: ‎~ Karthikeyan Vijayan joined from the community
[15/03/24, 9:35:46 PM] ~ Rahul Deora: Is there any way to compress a large text before passing it into gpt4 to save on costs?
[15/03/24, 9:53:58 PM] ~ Dev: Use prompt compression, there are various ways of doing it alternatively if you can see token efficiency, you can use that to compress your prompts and context appropriately
[16/03/24, 6:25:01 PM] ~ Prashant Srivastav: ‎~ Prashant Srivastav joined from the community
[18/03/24, 4:36:29 PM] ~ विक्रम: ‎~ विक्रम joined from the community
[19/03/24, 1:33:57 AM] ~ Prakash Sanker: ‎Your security code with ‪+91 95381 04545‬ changed.
[19/03/24, 1:34:00 AM] ~ Prakash Sanker: ‎Your security code with ‪+91 95381 04545‬ changed.
[19/03/24, 8:55:06 AM] ~ Sandeep: ‎Your security code with ‪+91 88282 90501‬ changed.
[20/03/24, 8:36:54 PM] ~ Anindyadeep Sannigrahi: ‎Your security code with ‪+91 96472 61597‬ changed.
[20/03/24, 9:52:02 PM] ~ Anindyadeep Sannigrahi: ‎Your security code with ‪+91 96472 61597‬ changed.
[22/03/24, 12:06:42 PM] ~ Rohit Joshi: ‎~ Rohit Joshi joined from the community
[23/03/24, 7:58:29 PM] ~ Ketan Bacchuwar: ‎~ Ketan Bacchuwar joined from the community
[24/03/24, 2:05:39 PM] ~ Narendranath Gogineni: ‎Your security code with ‪+91 91 82429 687‬ changed.
[24/03/24, 6:33:11 PM] ~ Atik Shaikh: ‎Your security code with ‪+91 72193 88991‬ changed.
[24/03/24, 11:21:58 PM] ~ sahir: ‎Your security code with ‪+91 90210 77561‬ changed.
[25/03/24, 8:52:41 PM] ‪+91 98910 58599‬: ‎‪+91 98910 58599‬ joined from the community
[26/03/24, 9:02:43 PM] ~ Suyash: ‎~ Suyash joined from the community
[27/03/24, 12:14:16 PM] ~ Nishant: ‎Your security code with ‪+91 99339 79842‬ changed.
[27/03/24, 7:59:04 PM] Bharat Shetty: https://chsasank.com/sycl-portable-cuda-alternative.html this is a very well written blog on alternative to NVIDIA CUDA framework
[28/03/24, 6:50:00 AM] Anshuman Nimble Box: ‎Your security code with Anshuman Nimble Box changed.
[28/03/24, 10:31:46 AM] ~ Suryansh: ‎~ Suryansh joined from the community
[28/03/24, 4:33:04 PM] ~ Vrushank | Portkey: ‎Your security code with ‪+91 97008 88848‬ changed.
[29/03/24, 8:56:00 AM] Anshuman Nimble Box: ‎Your security code with Anshuman Nimble Box changed.
[29/03/24, 5:21:51 PM] ~ ~I: ‎~ ~I joined from the community
[29/03/24, 7:17:47 PM] ~ sahir: ‎Your security code with ‪+91 90210 77561‬ changed.
[29/03/24, 7:17:50 PM] ~ sahir: ‎Your security code with ‪+91 90210 77561‬ changed.
[30/03/24, 12:27:44 AM] ~ SR: ‎Your security code with ‪+91 81496 33697‬ changed.
[31/03/24, 3:01:24 PM] ‪+91 98808 34200‬: ‎Your security code with ‪+91 98808 34200‬ changed.
[31/03/24, 5:18:42 PM] ~ Omkar Jadhav: ‎~ Omkar Jadhav joined from the community
[31/03/24, 9:27:27 PM] ~ Rohit Ganapathy: ‎~ Rohit Ganapathy joined from the community
[01/04/24, 9:44:44 AM] ~ Atharv: ‎~ Atharv joined from the community
[01/04/24, 3:04:00 PM] ~ Harsh Gupta: ‎Nirant  removed ~ Harsh Gupta
[01/04/24, 3:04:15 PM] ~ Ashish Patel: ‎Nirant  removed ~ Ashish Patel
[01/04/24, 7:06:21 PM] ~ Santi: ‎~ Santi joined from the community
[02/04/24, 12:34:14 PM] ~ Navita: ‎~ Navita joined from the community
[02/04/24, 1:45:27 PM] ~ Anshul Padhi: ‎~ Anshul Padhi joined from the community
[02/04/24, 10:41:29 PM] ~ Ram: ‎Your security code with ‪+91 99625 35360‬ changed.
[03/04/24, 2:28:45 PM] ~ Yogesh Ghaturle: ‎~ Yogesh Ghaturle joined from the community
[04/04/24, 3:50:21 AM] Sthit Validity: ‎Your security code with Sthit Validity changed.
[04/04/24, 3:50:22 AM] Sthit Validity: ‎Your security code with Sthit Validity changed.
[05/04/24, 11:48:28 AM] ~ Amit Bhor: What are the best function calling evals and RAG (specifically G part, not simplistic needle in haystack) evals? Also are there leaderboards for these? TIA
[05/04/24, 11:50:18 AM] Aashay Sarvam AI: Function calling - UC Berkley function calling leaderboard
[05/04/24, 2:05:02 PM] ~ rohan~: +1, this is one of the only ones which compares different function calling models

https://gorilla.cs.berkeley.edu/leaderboard.html ‎<This message was edited>
[05/04/24, 2:09:53 PM] ~ Amit Bhor: Thanks, anything for the Generative part of RAG?
[05/04/24, 2:10:39 PM] ~ Priyesh Srivastava: Hmm like would you not just use normal benchmarks for this
[05/04/24, 2:10:39 PM] ~ Priyesh Srivastava: ?
[05/04/24, 2:11:12 PM] ~ Priyesh Srivastava: Or does context aggresively change generation?
[05/04/24, 2:11:41 PM] ~ romit: Hi folks, does anyone know where can I buy Nvidia 3090 in Bangalore? (second hand)
[05/04/24, 2:11:47 PM] ~ rohan~: I know there a some leaderboard for grounded generation 🤔
Not entirely sure, I haven't gone into RAGs much

Have you tried Ragas?
https://github.com/explodinggradients/ragas

I have heard good things about them
[05/04/24, 2:12:25 PM] ~ rohan~: Looking for one too 😂
[06/04/24, 1:19:25 AM] ~ Balamurali A R: ‎~ Balamurali A R joined from the community
[06/04/24, 1:59:02 PM] ~ .: ‎~ . joined from the community
[07/04/24, 8:38:07 PM] Soumyadeep Mukherjee: ‎Your security code with Soumyadeep Mukherjee changed.
[08/04/24, 1:29:30 PM] ~ Rishi: ‎~ Rishi joined from the community
[08/04/24, 7:45:10 PM] ~ S: ‎Your security code with ‪+91 75500 68884‬ changed.
[08/04/24, 9:15:42 PM] ~ Ashin xavier: ‎~ Ashin xavier joined from the community
[09/04/24, 8:00:03 PM] ~ Olivia Deka: ‎~ Olivia Deka joined from the community
[09/04/24, 8:03:44 PM] ~ Olivia Deka: ‎Your security code with ‪+91 93651 59104‬ changed.
[09/04/24, 10:03:02 PM] ~ Tanuj Bhojwani: ‎Your security code with ‪+91 98671 04169‬ changed.
[11/04/24, 8:08:22 AM] ~ Sourasis Roy: https://x.com/MajmudarAdam/status/1778235769150423121?t=UnKrBcu01lc1g5Lt9Aa7Wg&s=08
[11/04/24, 8:08:49 AM] ~ Sourasis Roy: This guy is trying to learn and make a gpu from scratch
[11/04/24, 8:33:47 AM] ~ Praveen: Woww
[11/04/24, 2:09:48 PM] ~ Tarun: ‎Your security code with ‪+1 (510) 992‑0614‬ changed.
[11/04/24, 6:44:52 PM] ~ Bibek: ‎Your security code with ‪+65 8256 0957‬ changed.
[11/04/24, 7:27:54 PM] ~ Nirmal: ‎Your security code with ‪+91 95001 81814‬ changed.
[12/04/24, 12:12:42 PM] ~ Rahul Sundar: ‎~ Rahul Sundar joined from the community
[13/04/24, 8:59:36 AM] ~ Prateek: ‎~ Prateek joined from the community
[13/04/24, 2:04:04 PM] ~ Pooja Priyadarshini: ‎~ Pooja Priyadarshini joined from the community
[13/04/24, 6:39:56 PM] ~ Akash Singh: ‎Your security code with ‪+91 88080 32969‬ changed.
[13/04/24, 8:38:57 PM] ~ Arko Cy: ‎Your security code with ‪+91 78996 62501‬ changed.
[13/04/24, 9:04:13 PM] ~ Soham (Composio.dev): ‎~ Soham (Composio.dev) joined from the community
[14/04/24, 2:17:01 PM] ~ Sandeep: Renting physical hardware than buying will be better off, with hardware issues risk is offloaded to the vendor and return it when ever you want. Look for laptop of pc hardware renting business for corporates they can give you some leads
[14/04/24, 2:19:28 PM] ~ romit: I think my GPU utilization is quite high to warrant buying a second hand. I was also thinking of renting it, but that can turn out to be expensive fairly quickly
[14/04/24, 2:35:10 PM] ~ Sandeep: How does utilization effect cost  you can use 24/7 in your office and pay around 15 to 20 k fixed rent per month . I am referring to renting physical server not cloud
[14/04/24, 4:13:19 PM] ~ romit: Got it, do you know anyone who gives Gpu for physical renting?
[14/04/24, 7:01:20 PM] ~ romit: I am curious to know, how many of you guys are learning or know CUDA/Triton?

How is it useful in your day to day job? When do you use it?
[14/04/24, 7:01:45 PM] ~ romit: Cross posting from the main channel, asking because this might be the more relevant group
[14/04/24, 7:05:23 PM] ~ Siddharth: Learning both, not useful in my current job, but I am also trying to switch roles.
[14/04/24, 7:12:57 PM] ~ romit: Yes, I am also learning both. It’s been quite insightful to know what goes under the hood. But not able to foresee where will I use it
[14/04/24, 7:16:13 PM] ~ Bharath: Has anyone a sense of how much power consumption cost one could incur for various configs? If someone has done this already, would be helpful
[14/04/24, 9:08:12 PM] ~ Sid: Would your next role entail tuning models on CUDA/Triton ?
[14/04/24, 9:31:37 PM] ~ Siddharth: There are roles here that do require that.
[15/04/24, 2:32:19 AM] ~ Vamshi: https://github.com/tinygrad/open-gpu-kernel-modules
[15/04/24, 2:32:49 AM] ~ Vamshi: Hack to enable faster writes with PCIe even without nvlink
[15/04/24, 2:34:22 AM] ~ Vamshi: Haven’t used it, have only one 4090
[15/04/24, 2:34:29 AM] ~ Vamshi: Maybe interesting to some here ‎<This message was edited>
[15/04/24, 2:39:19 AM] ~ Vamshi: Just info, not a recommendation. Use with caution .. ‎<This message was edited>
[15/04/24, 7:57:07 AM] Sthit Validity: I like this warning 😂
[15/04/24, 7:58:26 PM] ~ Pratyush Sinha: ‎~ Pratyush Sinha joined from the community
[15/04/24, 9:59:14 PM] ~ Karthikeyan Vijayan: ‎Your security code with ‪+91 81222 65690‬ changed.
[17/04/24, 2:23:04 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[17/04/24, 2:30:44 PM] ~ Pranjal Joshi: ‎Your security code with ‪+91 88897 23444‬ changed.
[17/04/24, 7:07:54 PM] ~ Mihir Kulkarni: ‎~ Mihir Kulkarni joined from the community
[18/04/24, 8:56:34 AM] Ojasvi Yadav: if you're interested in open source then I strongly recommend following @919616406460 on X

https://x.com/4evabehindsota/status/1780647676293582947?s=46
[18/04/24, 8:57:30 AM] Ojasvi Yadav: Using this as a thread to kick off a discussion
Is anyone hosting mixtral 8x7b in prod?
[18/04/24, 9:36:50 AM] Aashay Sarvam AI: Yes
[18/04/24, 9:41:26 AM] Nirant : Not great on HumanEval and reasoning no?
[18/04/24, 9:45:21 AM] ~ Ashutosh Srivastava: ‎~ Ashutosh Srivastava joined from the community
[18/04/24, 10:00:15 AM] Ojasvi Yadav: Yes for that command R and its plus variant are very good

Mixtral is amazing at content creation in my experience
[18/04/24, 10:00:23 AM] Ojasvi Yadav: We're using both in production
[18/04/24, 10:00:46 AM] Ojasvi Yadav: Want to eventually get to finetuning them
Currently exploring unsloth
[18/04/24, 10:04:14 AM] ~ Sid: which one is good for sales and lead creation? basically convince a user to buy a product/membership
[18/04/24, 10:04:54 AM] ~ Sid: currently we are using gpt35turbo and gpt35turbo-instruct but not getting good results
[18/04/24, 10:08:30 AM] Dr. Pratik Desai: Share your experience with unsloth if you end up trying Pro versions. Their claims are intriguing.
[18/04/24, 10:13:27 AM] Ojasvi Yadav: That is a very high level task. You'll have to frame it in the same class as programming, reasoning, creativity/content-creation etc.
[18/04/24, 10:15:10 AM] Ravi Theja: pro version is still not GA right? 🤔
[18/04/24, 10:18:51 AM] Dr. Pratik Desai: What is everyone using for production inference?
[18/04/24, 10:19:16 AM] ~ Adarsh: They arent selling it yet
[18/04/24, 10:20:14 AM] Dr. Pratik Desai: Can't believe100x+ speedup is possible, if not a very specific controlled case. ‎<This message was edited>
[18/04/24, 10:24:44 AM] Dr. Pratik Desai: I stumbled upon this today and am looking forward to utilizing my home RTXs to good use. https://github.com/PygmalionAI/aphrodite-engine
Would like to know folks' resident inference engine.
[18/04/24, 10:26:14 AM] ~ Sid: i think reasonings would be the closest class.
[18/04/24, 10:31:38 AM] ~ romit: I have been using vLLM. It's giving really good results so far. The behavior is also very predictable
[18/04/24, 10:33:50 AM] ~ rohan~: TGI was what I was using before they shifted to buisness licence 

have been using vLLM since

Aphrodite is very interesting, something I am also exploring
[18/04/24, 10:34:42 AM] ~ rohan~: vLLM is pretty cool, and fast, but sometimes takes time to support new models

TGI is compatible with most of the models, but is slower than vLLM
[18/04/24, 10:36:50 AM] ~ romit: Agreed, that's a challenge with vLLM
[18/04/24, 7:18:13 PM] Nirant : ‎Your security code with Nirant  changed.
[18/04/24, 9:00:25 PM] ~ Rahul Deora: Which vLLM? Have found open source ones to not be that great
[18/04/24, 9:34:35 PM] ~ romit: What problems did you face?
[18/04/24, 9:49:05 PM] ~ Rahul Deora: Descriptions of clothing items are not detailed enough
[18/04/24, 9:49:13 PM] ~ Rahul Deora: Gpt4V works so much better
[18/04/24, 9:51:53 PM] ~ romit: Hmm, I think we are confusing something. vLLM is a inference engine
[18/04/24, 9:58:09 PM] Dr. Pratik Desai: vLLM # LLM V
‎[18/04/24, 10:10:30 PM] Ojasvi Yadav: ‎image omitted
‎[18/04/24, 10:13:00 PM] Yash Bonde: ‎image omitted
[18/04/24, 10:13:06 PM] Ojasvi Yadav: Self hosted on azure
[18/04/24, 10:15:01 PM] Ojasvi Yadav: I'm Using AWQ quantisation
[18/04/24, 10:18:35 PM] ~ Adarsh: Is it multilingual?
[18/04/24, 10:19:00 PM] ~ Adarsh: Vocab length is 128k
[18/04/24, 10:22:13 PM] ~ Adithya S K: they are using tiktoken
[18/04/24, 10:22:41 PM] Ojasvi Yadav: Do you mean context length?
[18/04/24, 10:22:41 PM] ~ Adarsh: Yeah just saw Daniel's post
[18/04/24, 10:23:35 PM] ~ Adarsh: No vocab size sorry
‎[18/04/24, 10:24:00 PM] Ojasvi Yadav: ‎image omitted
‎[18/04/24, 10:24:26 PM] Yash Bonde: ‎image omitted
[18/04/24, 10:25:19 PM] Yash Bonde: Waiting for 70b to go up.
[18/04/24, 10:25:29 PM] ~ Adarsh: Are you using some system prompt?
‎[18/04/24, 10:25:59 PM] Ojasvi Yadav: ‎GIF omitted
[18/04/24, 10:26:26 PM] Ravi Theja: Its shit for telugu 😆
[18/04/24, 10:26:29 PM] Dr. Pratik Desai: Meta Keeping some work for Ravi and Adarsh
‎[18/04/24, 10:26:41 PM] Yash Bonde: ‎image omitted
[18/04/24, 10:27:07 PM] ~ Adarsh: Ouch
[18/04/24, 10:27:51 PM] Ojasvi Yadav: Weights downloaded
[18/04/24, 10:28:08 PM] Ojasvi Yadav: ‎You deleted this message.
[18/04/24, 10:28:50 PM] Ojasvi Yadav: ‎You deleted this message.
[18/04/24, 10:29:41 PM] Ojasvi Yadav: Will try deploying it on azure first
‎[18/04/24, 10:36:18 PM] Yash Bonde: ‎image omitted
[18/04/24, 10:38:24 PM] Ojasvi Yadav: Any confirmed numbers on context length?
[18/04/24, 10:39:52 PM] ~ Abhinand: 8k for both 8B and 70B models
[18/04/24, 10:42:04 PM] Ojasvi Yadav: Thank you
[18/04/24, 10:46:37 PM] Ojasvi Yadav: Let's rope it up
[18/04/24, 10:56:33 PM] Yash Bonde: Trying with 8b to run it on 16k tokens.
[18/04/24, 11:16:34 PM] Yash Bonde: Rope is being annoying not working correctly. For now trying to do fine tuning. ‎<This message was edited>
[18/04/24, 11:31:16 PM] Ojasvi Yadav: Is this instruct?
[19/04/24, 12:23:22 AM] Abhishek Mishra: 5% data is non English though
[19/04/24, 12:23:42 AM] Abhishek Mishra: so not really focused on multilingual like Gemma
[19/04/24, 12:23:48 AM] ~ Adarsh: 5% spread over 30 languages🥲
[19/04/24, 12:23:50 AM] Abhishek Mishra: but better than llama2
[19/04/24, 12:24:13 AM] ~ Adarsh: Id consider that token leakage😂
[19/04/24, 12:24:20 AM] Abhishek Mishra: would require continued pretraining
[19/04/24, 12:25:03 AM] ~ Adarsh: Yess
‎[19/04/24, 12:31:35 AM] Ojasvi Yadav: ‎image omitted
‎[19/04/24, 12:31:36 AM] Ojasvi Yadav: ‎image omitted
[19/04/24, 12:37:45 AM] Ojasvi Yadav: Why am I so picky about multilinguality? We pay nearly 100k USD/month to LLM providers across the board. 

Multilinguality is a selling feature for all our products so my north Star of this initiative is to cut costs by fine-tuning multilingual-friendly finetunes and self hosting. We just reduced a major project's running cost by 74% by self hosting our own finetune. ‎<This message was edited>
[19/04/24, 12:40:46 AM] Sthit Validity: Which use case ? Article writing ?
[19/04/24, 1:02:49 AM] ~ Priyank Agrawal: How do you scale your fine-tune model at that usage scale??
[19/04/24, 1:14:48 AM] ~ Sankalp: ‎~ Sankalp joined from the community
[19/04/24, 1:14:52 AM] ~ Karan Ganesan: ‎~ Karan Ganesan joined from the community
[19/04/24, 1:49:33 AM] Ojasvi Yadav: Project by project requirements are much lesser
[19/04/24, 9:17:10 AM] ~ G.S.K: ‎~ G.S.K joined from the community
[20/04/24, 11:05:25 AM] ~ Abhinav Dadhich: ‎Your security code with ‪+91 91 166 110 59‬ changed.
[20/04/24, 11:39:16 AM] ~ Ganesh Yeluri: ‎~ Ganesh Yeluri joined from the community
[20/04/24, 11:44:05 AM] ~ kaushik c m: ‎~ kaushik c m joined from the community
[20/04/24, 11:45:49 AM] ~ Abhinav Dadhich: ‎Your security code with ‪+91 91 166 110 59‬ changed.
[20/04/24, 2:56:08 PM] ~ Atishay: ‎~ Atishay joined from the community
[20/04/24, 2:58:33 PM] ~ pupa: ‎~ pupa joined from the community
[20/04/24, 7:20:02 PM] Nirant : ‎Nirant  changed the group name to “GPU, Performance & Infra”
[20/04/24, 7:21:58 PM] ~ dhruv: ‎~ dhruv joined using this group's invite link
[20/04/24, 7:24:25 PM] ~ Ankur Pandey: ‎~ Ankur Pandey joined using this group's invite link
[20/04/24, 7:26:32 PM] ~ bansal rahul14: ‎~ bansal rahul14 joined using this group's invite link
[20/04/24, 7:28:11 PM] ~ Maneesh Mishra: ‎~ Maneesh Mishra joined using this group's invite link
[20/04/24, 7:31:08 PM] ~ Sumba: ‎~ Sumba joined using this group's invite link
[20/04/24, 7:31:10 PM] GPU, Performance & Infra: ‎You're now an admin
[20/04/24, 7:31:50 PM] ~ Kay Jey: ‎~ Kay Jey joined using this group's invite link
[20/04/24, 7:33:25 PM] ~ A: ‎~ A joined using this group's invite link
[20/04/24, 7:34:19 PM] Nirant : Couple of quick changes: 

First, please welcome Ojasvi @919971004124 Abhishek @919616406460 and Ravi @919550164716 as our new moderators. 

We'll define the scope of our conversations in more details as we go along -- but broadly: Performance, Local LLMs, Infrastructure and Deployment Challenges go here.
[20/04/24, 7:34:51 PM] Nirant : * Nirant on behalf of GenerativeAI Community https://nirantk.com/community
[20/04/24, 7:37:46 PM] ~ Nishanth Chandrasekar: ‎~ Nishanth Chandrasekar joined using this group's invite link
[20/04/24, 7:43:45 PM] ~ Rahul Thota: ‎~ Rahul Thota joined using this group's invite link
[20/04/24, 7:44:56 PM] ~ Sandeep Bantia: ‎~ Sandeep Bantia joined using this group's invite link
[20/04/24, 7:48:43 PM] ~ Kiran Nambiar: ‎~ Kiran Nambiar joined using this group's invite link
[20/04/24, 7:56:53 PM] ~ Atharv: ‎~ Atharv joined using this group's invite link
[20/04/24, 7:57:57 PM] ~ Rajesh Parikh: ‎~ Rajesh Parikh joined using this group's invite link
[20/04/24, 8:00:39 PM] ~ Ady: ‎~ Ady joined using this group's invite link
[20/04/24, 8:06:41 PM] ~ Harsha: ‎~ Harsha joined using this group's invite link
[20/04/24, 8:13:33 PM] ~ Divya Dixit: ‎~ Divya Dixit joined using this group's invite link
[20/04/24, 8:20:37 PM] ~ Rajesh RS: ‎~ Rajesh RS joined using this group's invite link
[20/04/24, 8:39:51 PM] ~ Syed Moinudeen: ‎~ Syed Moinudeen joined using this group's invite link
[20/04/24, 8:59:13 PM] ~ Vinay Mimani: ‎~ Vinay Mimani joined using this group's invite link
[20/04/24, 9:13:20 PM] ~ Jayanth: ‎~ Jayanth joined using this group's invite link
[20/04/24, 9:23:42 PM] Ojasvi Yadav: Welcome everyone
[20/04/24, 9:51:10 PM] ~ Shanthi Vardhan: ‎~ Shanthi Vardhan joined using this group's invite link
[20/04/24, 10:04:16 PM] ~ Vishwam Jindal: ‎~ Vishwam Jindal joined using this group's invite link
[20/04/24, 10:24:17 PM] ~ Sumit: ‎~ Sumit joined using this group's invite link
[20/04/24, 10:25:31 PM] ~ Rezathema: ‎~ Rezathema joined using this group's invite link
[20/04/24, 11:49:22 PM] ~ Anjineyulu: What's the margin for llama2 inference costing 1 dollar for 1 million tokens which are provided by many vendors
[20/04/24, 11:55:15 PM] ~ Srijan: ‎~ Srijan joined from the community
[21/04/24, 1:54:49 AM] ~ Ayush Garg: ‎~ Ayush Garg joined using this group's invite link
[21/04/24, 2:44:21 AM] ~ KShivendu: ‎~ KShivendu joined using this group's invite link
[21/04/24, 2:46:46 AM] ~ Gaurav Chandak: ‎~ Gaurav Chandak joined using this group's invite link
[21/04/24, 6:19:54 AM] ~ Shuveb Hussain: ‎~ Shuveb Hussain joined using this group's invite link
[21/04/24, 6:27:12 AM] ~ Ankit: ‎~ Ankit joined using this group's invite link
[21/04/24, 6:49:29 AM] ~ Shan Shah: ‎~ Shan Shah joined using this group's invite link
[21/04/24, 7:37:57 AM] ~ Sandya Saravanan: ‎~ Sandya Saravanan joined using this group's invite link
[21/04/24, 8:12:46 AM] ~ ~md: ‎~ ~md joined using this group's invite link
[21/04/24, 8:45:29 AM] ~ Piyush Makhija: ‎~ Piyush Makhija joined using this group's invite link
[21/04/24, 8:45:35 AM] Nirant : 1. Discounting: Couple of these folks are probably discounting the inference to gain market share e.g. Replicate did that famously when Llama2 came out first
2. GPU cost: Folks who're able to run models on a smaller GPU e.g. A100 — which I assume Ray, Together and Fireworks are — will have better margins via distributed model inference. Some might've also leased GPUs via folks like Heztner, instead of buying them out right — which changes the math quite a bit.
3. Load balancing and routing: Some of these players do route and batch user questions specially during US waking hours, making inference cheaper and faster both because of GPU parallelisation

Overall, it's quite difficult to estimate margins accurately, but if I were to make a rough guess — I'd wager some of these are at 60-80%
[21/04/24, 9:03:37 AM] ~ Anjineyulu: That's a great detail !!!Then why should we invest in SLMs if the cost is so less,because we care about ESG reports?
[21/04/24, 9:04:06 AM] ~ Anjineyulu: Or to ensure that AI can satisfy infinitesimal TAM
[21/04/24, 9:11:28 AM] Nirant : Because narrow task specific systems can win is an evergreen thesis
[21/04/24, 9:12:56 AM] Nirant : I am personally net-short on that thesis though — I am old enough to remember when companies would crib about how expensive and large BERT was and you could just do the same things — but less general by using syntax trees and stats models. Today, 768M models are consider "medium" — even in embedding models!
[21/04/24, 9:15:22 AM] Nirant : The cost of compute drops fast, cost of memory drops faster — the only difficult things are organizing them e.g. NVIDIA/Intel/Google/Perplexity, orchestration e.g. K8s, EC2, S3, Postgres, Snowflake and logging e.g. datadog/langsmith. 
[21/04/24, 9:16:54 AM] Nirant : I specifically recall how spaCy and gensim were the winners of BERT era because they made Word2Vec portable and usable, similarly — if we hit a 3y performance plateau at 85 MMLU score (which looks like we will) the winners will be the 30-70B models
[21/04/24, 9:25:39 AM] ~ Anjineyulu: Cool,I am confident that coding has becoming fantastic with even llama3,should we not rather unlock new TAM,say metallurgy,to reduce the cost of gold like that rather.Why should we not take this route of blue ocean strategy
[21/04/24, 9:35:04 AM] Nirant : Lithium is more valuable than gold ;)

And there is more capex going into Lithium in US than goes into high school education in India
[21/04/24, 9:38:36 AM] ~ Anjineyulu: 😂😂😂 oh ROFL,ask them to have the r and d labs in india,we shall work 😂

Then we need more intelligent models quickly yo do research and aid human scientists 

I constantly believe that many who model are really trusting majorly luck,we need more strategies around it
[21/04/24, 9:40:10 AM] Nirant : ‎This message was deleted.
[21/04/24, 9:41:05 AM] Nirant : @919971004124 what does your cost estimate of Llama3 LoRA Fine-tuning look like?
[21/04/24, 9:57:08 AM] ‪+91 70860 34141‬: ‎‪+91 70860 34141‬ joined using this group's invite link
[21/04/24, 10:30:23 AM] ~ Yashwanth V: ‎~ Yashwanth V joined using this group's invite link
[21/04/24, 10:55:51 AM] ~ Deep: ‎~ Deep joined using this group's invite link
[21/04/24, 11:39:34 AM] ~ AA: ‎~ AA joined using this group's invite link
[21/04/24, 2:29:30 PM] Abhishek Mishra: Has anybody here used vLLM with chatml format with oss models?
[21/04/24, 2:32:48 PM] ~ Anindyadeep Sannigrahi: yes, just use HF tokenizer to apply chat template on the prompt (without doing tokenization i.e. `tokenize=False`) and then get the prompt with chat template and then use it with vLLM 


so roughly

prompt = tokenizer.apply_chat_template(messages, tokenize=False)

vllm_model([prompt])
[21/04/24, 2:40:21 PM] Abhishek Mishra: thank you
[21/04/24, 6:40:53 PM] ~ Ruchir: ‎~ Ruchir joined using this group's invite link
[21/04/24, 7:36:14 PM] Dr. Pratik Desai: That's why we need SLM. While compute may get cheaper, we still use turbines to generate energy and battery energy storage technologies not going anywhere. For on device models, we will need SLMs or more work in that directions. Maybe in future 70B becomes SLM cause we have 10T models.
[21/04/24, 7:42:06 PM] ~ Abhik: This would be helpful
[21/04/24, 7:43:08 PM] ~ Rajesh Parikh: Its kind of strange, we have been calling models with size less than 100B as SLM and beyond that as Large. May be we need better terminology here like clothes (S, M, L, XL, XXL). But largely in B2B context, 70-100B parameter models delivering GPT4 like performance which can further be fine-tuned on the private data is a good scenario for future progress with enterprise use-case. That keeps inference/running cost in check. Any future LLMs trained for bigger sizes in the context of enterprise will need to justify price per unit of additional performance gain. B2C ofcourse, may be a different game and model size increase and additional world knowledge may have use there.
[21/04/24, 8:19:28 PM] Dr. Pratik Desai: <80$ for 8b 😂
[21/04/24, 8:20:10 PM] Dr. Pratik Desai: Doesn't include the price of building the dataset though
[21/04/24, 8:20:28 PM] Ravi Theja: dataset size? epochs?
[21/04/24, 8:21:35 PM] Dr. Pratik Desai: 150k instructions ~1K token each, 3 epoch
[21/04/24, 8:24:31 PM] Dr. Pratik Desai: I think I'm going to start doing it on my 3090s from now on.
[21/04/24, 8:28:11 PM] Dr. Pratik Desai: @919616406460 is there a DPO dataset where I can just find+replace company name, and then clean the model so it doesn't say It's based on the OpenAI model, in case of bad training data used before? I know it has been done by many already, and I dataset like that can be very useful, like a mandatory step for every training going forward.
[21/04/24, 8:29:50 PM] Abhishek Mishra: these are some good high quality datasets created by argilla by filtering the previous ones

https://huggingface.co/collections/argilla/preference-datasets-for-dpo-656f0ce6a00ad2dc33069478
[21/04/24, 8:32:59 PM] ~ Trinath Yarlagadda: ‎~ Trinath Yarlagadda joined using this group's invite link
[21/04/24, 8:36:17 PM] Dr. Pratik Desai: Damn. That was fast. Thanks.
[21/04/24, 8:36:57 PM] ~ Srijan: Argilla datasets are pretty good! I've used them for multiple projects. Especially useful for incorporating general DPO preferences into your model. 

But if anyone is building/has built/wants to build fine tuning or DPO data on private corpus/specific data, I would like to pick your brain and discuss/learn. ‎<This message was edited>
[21/04/24, 8:39:26 PM] Dr. Pratik Desai: Nothing for removing “I’m a model trained by OpenAI” kind of model identity crisis 😔
[21/04/24, 8:39:48 PM] Abhishek Mishra: yes I've built positive and negative behaviour controlled datasets with DPO. You can drop your ideas here in this group as this will be a common query.
[21/04/24, 8:43:20 PM] ~ Srijan: Idea: You can create a reasonably-sized DPO instruction synthetic dataset with queries asking who built you/what model are you etc and putting "OpenAI" "GPT-4" etc in the rejected response. You can create chosen response as either your company's name/cant respond.
Thoughts? ‎<This message was edited>
[21/04/24, 8:47:09 PM] ~ Srijan: that's cool! A few questions to understand your process: 
Was your goal of doing DPO to train the model on your corpus' knowledge, or was it to make a trained model safe (non toxic, unbiased, etc)?  
What was in your starting corpus - pdfs, spreadsheets, etc? 
How did you create + and - responses? 
What size of DPO dataset is needed to train the model reasonably on the private corpus?
[21/04/24, 8:48:29 PM] Dr. Pratik Desai: I was just trying to save some effort if someone has already found a good one or released it. 😬 If not, will have to do it anyway.
[21/04/24, 8:50:21 PM] ~ Srijan: ha! I can try to create one and put it on hugging face. by when do you need it?
[21/04/24, 8:53:59 PM] Dr. Pratik Desai: At your schedule. Maybe next week. 
I think this can be a really helpful dataset to enforce model identity.
[21/04/24, 8:57:07 PM] ~ Srijan: more like "openai erasure" 😝
[21/04/24, 9:01:52 PM] Dr. Pratik Desai: 😂 Web is corrupted by ChatGPT generated text, we need a broom.
[21/04/24, 9:34:11 PM] ~ Srijan: If someone else has already built it, then i would also use that, instead of creating one from scratch!
[21/04/24, 9:38:51 PM] Abhishek Mishra: as per my knowledge no popular or known dataset has openai/GPT4 identity stuff in  negative preference.
anton had a discussion with me once and he built a dataset specifically for this but it converged to zero loss in ~200 samples
[21/04/24, 9:39:10 PM] Abhishek Mishra: so he found some success but not very robust with that fast convergence, that dataset was not made public
[21/04/24, 9:40:07 PM] Abhishek Mishra: but one can easily go and pick shareGPT chats and find tons of references to GPT4/openai in queries or responses, from there on, you can choose to just add standard refusals or responses to those types of queries
[21/04/24, 9:46:35 PM] Ojasvi Yadav: It's funny because it is so true
[21/04/24, 9:47:50 PM] Ojasvi Yadav: Further progress in this field will not depend on web data
[21/04/24, 9:48:01 PM] Ojasvi Yadav: As much as it used to
[21/04/24, 9:49:23 PM] Ojasvi Yadav: I scrape websites for a living and the quality of data is deteriorating + websites like reddit who know they've got high quality data are adding paid boundary walls
[21/04/24, 9:51:40 PM] Sthit Validity: Tweet worthy. The first line. Website scraping skillfully is an artform.
[21/04/24, 9:53:47 PM] Abhishek Mishra: One of the group who released the biggest ever open source dataset of 15T tokens mentioned that not all common crawl data from different years is similar
[21/04/24, 9:54:25 PM] Abhishek Mishra: That the quality of organic data changes from 2021 onwards every year and continues being polluted into 2023 with bots and assistants
[21/04/24, 9:55:16 PM] Sthit Validity: There will be a bespoke-style handloom inspired market for real human annotators I am sure. Even with AGI
[21/04/24, 10:33:59 PM] Dr. Pratik Desai: @13013291103 Found one from @919148574393 https://huggingface.co/datasets/AdithyaSK/CompanionLLama_instruction_30k
[22/04/24, 8:06:49 AM] ~ Satvik: ‎~ Satvik joined using this group's invite link
[22/04/24, 12:32:21 PM] ~ Sandeep: Any recommendations on the platform like heroku to deploy models of size t5-base and which is cost effective to show case demo apps
[22/04/24, 12:34:36 PM] Sthit Validity: https://www.vultr.com/vultr-vs-linode/
[22/04/24, 12:40:08 PM] ~ Sandeep: 200+ dollars per month is norm to deploy DL based apps ? Or anything lower is possible ?
[22/04/24, 12:41:06 PM] Sthit Validity: Not required at all. T5-base is ~0.2b if iirc, so any standard sized computer will do. 200 is just for fun 😂
[22/04/24, 12:41:17 PM] ~ Naveen Pandey: I think runpod and modal labs are new age heroku when it comes to model deployments
[22/04/24, 12:59:17 PM] ~ Ashutosh Srivastava: If it's a deep learning model you want to demo, you can use gradio
[22/04/24, 12:59:34 PM] ~ Ashutosh Srivastava: It is easy to use and hosts a proxy to your local
[22/04/24, 1:00:17 PM] ~ Ashutosh Srivastava: So you can deploy on GCP as well
[22/04/24, 5:32:11 PM] ~ Hemant Mohapatra: ‎~ Hemant Mohapatra joined using this group's invite link
[22/04/24, 6:38:38 PM] ~ Rohit Agarwal: ‎~ Rohit Agarwal joined using this group's invite link
[22/04/24, 7:15:07 PM] ~ S: ‎‎~ S changed their phone number to a new number. ‎Tap to message or add the new number.
[23/04/24, 8:33:51 AM] ~ A: ‎Your security code with ‪+91 97122 19412‬ changed.
[23/04/24, 11:22:36 AM] Ojasvi Yadav: https://x.com/1littlecoder/status/1782457617475920091?s=46
[23/04/24, 11:22:46 AM] Ojasvi Yadav: Can see this being a thing that happens overnight
[23/04/24, 11:23:40 AM] Ojasvi Yadav: In AI apps that have something to do with personalisation
[23/04/24, 11:24:01 AM] Ojasvi Yadav: Whose moat is on device AI
[23/04/24, 11:24:04 AM] Ojasvi Yadav: Like Siri
[23/04/24, 11:25:05 AM] Ojasvi Yadav: Each morning you wake up and your assistant gets better with the mispredictions/DPO/corrective data it gathered yesterday
[23/04/24, 11:30:02 AM] Ojasvi Yadav: Doing this overnight makes room to free up computation power when you're sleeping. Added bonus of power availability since most folks charge their phones overnight.
[23/04/24, 11:57:17 AM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[23/04/24, 12:40:15 PM] ~ Arihant Barjatya: ‎Your security code with ‪+91 6900 400 740‬ changed.
[23/04/24, 1:18:51 PM] ~ Varun Garg | KnitAI: ‎~ Varun Garg | KnitAI joined from the community
[24/04/24, 7:06:17 AM] ~ Anubhav Mishra: ‎Your security code with ‪+91 80721 73935‬ changed.
[24/04/24, 12:03:48 PM] ‪+91 99713 03951‬: ‎Your security code with ‪+91 99713 03951‬ changed.
[24/04/24, 1:49:17 PM] Ojasvi Yadav: https://x.com/tsarnick/status/1782889456657224119?s=61&t=bQh_EQwn0QrIk7qfETWGjw
[24/04/24, 1:50:42 PM] ~ Pratyush Sinha: ‎Your security code with ‪+91 94705 82772‬ changed.
[24/04/24, 1:51:01 PM] Ojasvi Yadav: Does anyone know if this Is this on device inference or are the models hosted anywhere? 

My educated guess is inference is on hosted models

Leading into this -> how difficult would it be to jailbreak these and route the requests elsewhere 😈
[24/04/24, 1:53:10 PM] Abhishek Mishra: yeah the requests will be routed to in-app assistants most likely
[24/04/24, 1:54:27 PM] Abhishek Mishra: regarding jailbreak, if they don't put an additional layer for filtering to limit types of requests, it should be the same difficulty
[24/04/24, 1:54:46 PM] Abhishek Mishra: jailbreaks typically require very long prompts, so voice command would be kind of limiting
[24/04/24, 2:11:53 PM] Ojasvi Yadav: My apologies, by jailbreak I meant changing the endpoint to which the input data is sent. 

So when user asks a question about what is in front of them, the image and the text prompt go to your own hosted Llava instead of the one hosted by Meta.
[24/04/24, 2:17:55 PM] ~ Sushant Kumar: https://github.com/dcrebbin/meta-vision-api
[24/04/24, 2:20:32 PM] ~ Sushant Kumar: By on-device, do you mean the glasses itself?
[24/04/24, 2:22:38 PM] Ojasvi Yadav: Yes
[24/04/24, 3:41:31 PM] ~ Abhinav Verma: If the api returns 500 error. What happens to the glasses
[24/04/24, 3:58:07 PM] Nirant : They simply have blue screen/sky of death 😂
[24/04/24, 3:59:22 PM] ~ Priyank Agrawal: Meta will be like feel free to burn your own GPU
[24/04/24, 4:00:47 PM] ~ Priyank Agrawal: It would be very unlikely to be on device, difficult for them to replace models and battery diying out will be frequnt
[24/04/24, 4:07:50 PM] Ojasvi Yadav: Hence, my curiosity in jailbreaking and redirecting the LLM calls to API endpoints of my own choice
[24/04/24, 4:08:11 PM] Ojasvi Yadav: If it was on device then it would've been quite hard
[24/04/24, 4:08:29 PM] ~ Priyank Agrawal: Yeah, for customisation it should be possible
[24/04/24, 4:08:30 PM] ~ Priyank Agrawal: Yeah
[24/04/24, 4:08:58 PM] ~ Priyank Agrawal: Not sure if they running an Android on it or something else
[24/04/24, 5:18:23 PM] ~ Shubhra Prakash: ‎~ Shubhra Prakash joined from the community
[24/04/24, 7:35:47 PM] ~ Sanjeed: ‎~ Sanjeed joined using this group's invite link
[24/04/24, 7:49:48 PM] ~ Tara Lodh: https://blogs.nvidia.com/blog/runai/
[25/04/24, 4:18:17 PM] ~ Aarish: ‎~ Aarish joined using this group's invite link
[25/04/24, 6:27:16 PM] ~ Karthikeyan Vijayan: ‎Your security code with ‪+91 81222 65690‬ changed.
[25/04/24, 9:13:29 PM] Ojasvi Yadav: Half my imposter syndrome in this field stems from getting stuck while trying to configure a fresh Linux instance with particular cuda config and drivers 

Not surprised since like most, I've always googled or chatGPTd the error and had to claw my way out instead of spending a day or two on bridging this skill gap
[25/04/24, 9:14:22 PM] Ojasvi Yadav: Any resources to make this process smoother?

I'm Certain there are others in this group who could use such a resource too
[25/04/24, 9:14:33 PM] Nirant : Use Modal Labs
[25/04/24, 9:14:35 PM] Chaitanya Agarwal: oh yeah same - have burnt $$s here too, nvidia owes me
[25/04/24, 9:14:45 PM] Chaitanya Agarwal: runpod images have been seamless for me tbh
[25/04/24, 9:15:02 PM] Nirant : I started using Modal Labs and since then my eye sight has improved and my skin has become clearer
[25/04/24, 9:15:22 PM] Nirant : Or something like Runpod images
[25/04/24, 9:15:45 PM] Nirant : It's basically dollar efficient to use these now
[25/04/24, 9:16:20 PM] ~ Kishore M R: 15 years with Linux since Ubuntu 7.0  tweaking kernel stuffs for embedded devices. It happens  every single time 🙌🏼
[25/04/24, 9:16:33 PM] Nirant : Because it's cheaper to use these for a month than to have someone of your caliber spend 10 days of a year dealing with these issues ‎<This message was edited>
[25/04/24, 9:17:34 PM] Chaitanya Agarwal: and storage is even cheaper so you dont have to worry about migrating data - you can turn the instances on/off whenever you want and plug your weights/data back in
[25/04/24, 9:26:43 PM] Aashay Sarvam AI: Be in touch with nvidia support 😝
[25/04/24, 9:27:39 PM] ~ Anindyadeep Sannigrahi: Ohh yeahh it is good, really good, we use it too
[25/04/24, 9:28:35 PM] ~ Anindyadeep Sannigrahi: smooth
[25/04/24, 9:32:39 PM] ~ Anindyadeep Sannigrahi: for the record, personally modal has a good community and support system, and it gives some free credits
[25/04/24, 9:32:59 PM] Ojasvi Yadav: They're a cloud provider?
[25/04/24, 9:34:59 PM] Ojasvi Yadav: My usecase is being able to install a particular cuda version and cuda toolkit version that is the need of the hour on any Linux instance
[25/04/24, 9:36:14 PM] Ojasvi Yadav: Like for people running local LLMs, or using their azure/aws credits
[25/04/24, 9:37:00 PM] Ojasvi Yadav: Because even Collab and sagemaker are preconfigured with cuda and cuda-toolkit
[25/04/24, 9:37:56 PM] Chaitanya Agarwal: yeah
[25/04/24, 9:38:02 PM] Ojasvi Yadav: Checking this now
[25/04/24, 9:57:17 PM] ~ Naveen Pandey: Another possible way is to never install cuda on base OS. Only drivers on base OS and use cuda-toolkit packaged with pytorch from conda.
[25/04/24, 10:06:44 PM] Abhishek Mishra: for most use cases, you can find cuda 11/12 + pytorch docker images very easily. runpod also has additional ones with SD, textgen webui, axolotl etc

installing cuda from scratch and building everything sucks
[25/04/24, 10:07:48 PM] ~ Adhitya Swaminathan: This + a tiny dockerfile for any missing libraries and customization is perfect
[25/04/24, 10:18:14 PM] Dr. Pratik Desai: This is so bad that every species has evolved differently to survive the Cuda era.
[25/04/24, 10:19:51 PM] ~ Anindyadeep Sannigrahi: ahhh I see
[25/04/24, 10:20:02 PM] ~ Anindyadeep Sannigrahi: in that case, modal might not be much help ig
[25/04/24, 10:20:48 PM] ~ Anindyadeep Sannigrahi: ig in that case, just use docker custom images, I mean personally yeah it is hassle but i find images from nvcr or cuda (from docker hub) a good to go, and build on top of it
[26/04/24, 7:05:16 AM] Saurabh Jain: ‎Saurabh Jain joined from the community
[26/04/24, 12:45:59 PM] ~ pt: ‎~ pt joined using this group's invite link
[27/04/24, 7:44:21 AM] ~ Shubhra Prakash: ‎~ Shubhra Prakash left
[27/04/24, 1:21:12 PM] ~ Arihant Barjatya: ‎Your security code with ‪+91 6900 400 740‬ changed.
[27/04/24, 2:15:21 PM] Ojasvi Yadav: I will believe in coding-agents the day they're able to solve cuda setup
Till then it's just a fad
[27/04/24, 2:33:38 PM] Pratyush Choudhury Together Fund: Lol,
[27/04/24, 2:34:29 PM] ~ Anindyadeep Sannigrahi: devin crying at the corner
[28/04/24, 6:50:45 PM] Ravi Theja: ‎Your security code with Ravi Theja changed.
[28/04/24, 8:31:36 PM] ~ marmik pandya: ‎~ marmik pandya joined from the community
[28/04/24, 9:14:10 PM] ~ Arihant Barjatya: ‎Your security code with ‪+91 6900 400 740‬ changed.
[29/04/24, 9:28:41 AM] ~ Ankit Saurav: ‎~ Ankit Saurav joined from the community
‎[30/04/24, 12:16:43 AM] ~ Nayan Shah: ‎image omitted
[30/04/24, 12:18:45 AM] ~ Nayan Shah: u can check this was my script ,  
https://gist.github.com/snayan06/0e1ae7e9999c78a8b4533a7ed570f69d
[30/04/24, 12:30:44 AM] ~ Nayan Shah: basically, i would want to benchmark in such a way that, I can be sure I have done this properly and make a sound decision if I need GPU for the latency I want 🙈 ‎<This message was edited>
[30/04/24, 12:31:07 AM] ~ Atishay: not exactly the answer you are looking for perhaps but 
1. Use multiple threads. No way around it. You can architect in different ways - either have a single asyncio server with 3 cores dedicated to the model and the service choosing which core in a round robin fashion. Or just slap on 9 workers (2n+1) and call it a day. 
2. ⁠Single request taking longer - cpu is not as good, is a fundamental limitation you can’t do anything without buying new hardware/ changing model itself.
3. Id generally always recommend using asyncio for a service - especially relevant if you are using the first approach listed in point 1. 
4. ⁠of course if you want better results use a gpu
[30/04/24, 12:33:44 AM] ~ Nayan Shah: yeah i think if bottleneck is the model only which is a CPU-bound opr then I have to upgrade the hardware but I want to decide on that part , the suggestion of using the asyncio was in my mind , was thinking the fastapi app which uses the event loop so that will only block the thread when the CPU bound opr comes up .
‎[30/04/24, 4:59:49 PM] Ojasvi Yadav: ‎image omitted
[30/04/24, 5:00:36 PM] Sthit Validity: Source ?
[30/04/24, 5:00:59 PM] Ojasvi Yadav: "just trust me bro"

Just kidding here you go ‎<This message was edited>
[30/04/24, 5:01:03 PM] ~ YP: Suhail most likely
[30/04/24, 5:01:08 PM] Ojasvi Yadav: https://www.fxguide.com/fxfeatured/actually-using-sora/
[30/04/24, 10:00:45 PM] ~ Jaswanth: ‎Your security code with ‪+91 89191 51312‬ changed.
[30/04/24, 10:07:25 PM] ~ Laji: ‎~ Laji joined from the community
[01/05/24, 7:56:39 AM] ~ Vinay Kumar: ‎~ Vinay Kumar joined from the community
[01/05/24, 8:25:12 AM] ~ Yogesh Ghaturle: ‎Your security code with ‪+91 99809 42947‬ changed.
[01/05/24, 12:19:45 PM] ~ Sourab Mangrulkar: ‎~ Sourab Mangrulkar joined from the community
[01/05/24, 2:07:54 PM] ~ Tushar Varshney: ‎~ Tushar Varshney joined from the community
[01/05/24, 7:22:22 PM] ~ Maheswaran Parameswaran: ‎Your security code with ‪+91 99301 29844‬ changed.
[02/05/24, 8:26:07 AM] ~ HP: Great article to understand the basics of GPU math 
https://blog.eleuther.ai/transformer-math/
[02/05/24, 10:41:01 AM] ~ Shashwat v3: Hey guys trying to complete finetuning on 5k MRI cases. GPUs are failing. Changed region from US to Mumbai worked. But Training stopped after 2.5 hrs. Using Google container on Vertex AI. Tesla T4 GPU. 16v CPU, 60GB RAM

Some questions: 
1. Can models be trained in batch. Is there any library for helping with batch training 
2. Parallel processing for model training ( want to use multi cloud platforms in batches to finetune single model using multiple cloud and GPU provider) - is it possible 

Are these pain-points currently? Any dev tool solving for it? 

My sense is fine-tuning and testing is a ever-on exercise for use cases and businesses. Libraries for these seems like a good gold-shovel opportunity. Wud love to learn from folks doing atleast 1 training per week in last 4-5 weeks.
[02/05/24, 10:43:23 AM] ~ HP: I suppose 3D parallelism can be achieved. You can explore deepmind/axolotl
[02/05/24, 10:47:59 AM] ~ Shashwat v3: Tried to look on Nvidia and Google. All of them are request basis so didn't apply yet. Waiting for more evidence. Heard Karpathy's interviews managing GPUs is the real engineering involved in ChatGPT AI. We are also considering to buy one GPU, i am expecting similar issues in setting that up with poor community support around the same. Making seemingly simple model fine-tuning time taking. This is even more critical for big datasets eg image, video I wud assume memory issues wudnt be so prominent on text datasets... (My hunch, didn't do text fine-tuning yet) ‎<This message was edited>
[02/05/24, 10:51:46 AM] ~ Shashwat v3: Found relevant https://www.youtube.com/watch?v=m6T-Mq1BPXg
[02/05/24, 10:52:50 AM] ~ Mahesh Sathiamoorthy: What kind of models are you finetuning?
[02/05/24, 10:54:18 AM] ~ Shashwat v3: Multimodal. Clip/ Blip
[02/05/24, 10:55:16 AM] ~ Shashwat v3: Plan is to do aggressive finetuning. And approach complete pretraining, MR and CT foundational model
[02/05/24, 10:55:35 AM] ~ Shashwat v3: Which is why I'm trying to what can be automated
[02/05/24, 11:00:42 AM] ~ Nishanth Chandrasekar: Depending on which size of clip you’ve chosen, a T4 can’t really handle large batch sizes. 
Clip especially requires large batch sizes to converge if I recall correctly.
[02/05/24, 11:03:08 AM] ~ Mahesh Sathiamoorthy: Why do you want to pretrain a model? looks like you have only 5k examples.
[02/05/24, 11:07:24 AM] Nirant : Exactly
[02/05/24, 11:11:53 AM] ~ Priyank Agrawal: Also, what's the usecase??
MRai CT have been solved by a few companies using (not so large) vision models. Like yolo or resnet etc
[02/05/24, 11:49:35 AM] Sthit Validity: Great write up. Thanks for sharing 🙏
[02/05/24, 11:58:47 AM] ~ Saransh: ‎~ Saransh joined from the community
[02/05/24, 11:58:35 AM] ~ Shashwat v3: Yeah. Wanted to do a quick and dirty run. Were successful with 50 got some output. Breaking at 5k. We are also shifting to TF from pytorch now.
[02/05/24, 11:59:51 AM] ~ Shashwat v3: Have roadmap for 300k, need to bridge 700k in next few years.
[02/05/24, 12:05:39 PM] ~ Nishanth Chandrasekar: Not sure how shifting to TF will help. 
Open CLIP used a batch size of ~80-90k. OpenAI used ~30k or something like that. 
700k is also not enough for pre training in my opinion. Take a look at the size of datasets they used for both OpenAI and open clip.
[02/05/24, 12:24:29 PM] Aashay Sarvam AI: I don’t think the setup right now is that bad (I mean 5k images you could train 5-6 years ago also ~ ex unet). 
You need to just perf benchmark your pipeline and see why it is breaking. 

What karpathy was talking about is 1k + gpus running for months where anything can go wrong.
[02/05/24, 1:18:24 PM] ~ Shashwat v3: Yeah my debugging says it's a known problem. Will share ss to u in DM
[02/05/24, 1:50:57 PM] ~ Kay Jey: ‎Your security code with ‪+91 78250 31908‬ changed.
[02/05/24, 5:31:43 PM] Dr. Pratik Desai: Yes. Clip needs larger batch size, and recommended GPUs are 80GB ones. 
Met Vik from Moondream this sunday and we had detailed discussion about training multimodal and the same thing with batch size. May be post findings sometime later.
[03/05/24, 9:54:54 AM] ~ Kay Jey: ‎Your security code with ‪+91 78250 31908‬ changed.
[03/05/24, 11:21:11 AM] ~ Himanshu Mittal: ‎~ Himanshu Mittal joined from the community
[03/05/24, 10:06:27 PM] ~ Omkar Jadhav: ‎Your security code with ‪+91 81692 34616‬ changed.
[04/05/24, 12:58:48 AM] ~ Mahesh CR: ‎~ Mahesh CR left
[04/05/24, 12:26:05 PM] ~ Venkat: Lambda labs images too.. I use in my home workstation both in OS & Docker
[04/05/24, 12:27:48 PM] ~ Venkat: Came across this benchmark across serving engines & frameworks

https://pages.run.ai/hubfs/PDFs/Serving-Large-Language-Models-Run-ai-Benchmarking-Study.pdf
[04/05/24, 12:33:22 PM] ~ Adarsh: https://twitter.com/Yampeleg/status/1751823896800583924?t=INgdeir1-QCdKzRXzKeW8Q&s=19

This might come handy haha

If you run this on a clean ubuntu 22.04 It will install CUDA + GPU Drivers + CuDNN and everything else you will need to start working.
[04/05/24, 1:00:18 PM] Ojasvi Yadav: Thanks folks, much appreciated
[05/05/24, 12:29:19 AM] ‪+91 88247 89982‬: ‎‪+91 88247 89982‬ joined from the community
[06/05/24, 12:33:58 PM] ~ Joy: ‎~ Joy joined from the community
[06/05/24, 12:35:51 PM] ~ Aditya Thakur: ‎~ Aditya Thakur joined from the community
[06/05/24, 5:25:51 PM] ~ Mukund: ‎~ Mukund joined from the community
[06/05/24, 7:30:04 PM] ~ Yash: ‎~ Yash joined from the community
[06/05/24, 10:03:25 PM] ~ Vipul Maheshwari: ‎~ Vipul Maheshwari joined from the community
[07/05/24, 2:11:19 PM] ~ Kaustubh Priye: ‎~ Kaustubh Priye joined from the community
[07/05/24, 7:51:54 PM] Ojasvi Yadav: Thoughts on the M4 chip for local inference?
[07/05/24, 7:52:13 PM] Ojasvi Yadav: https://www.youtube.com/live/f1J38FlDKxo?si=lZe_R_pAypkC_uPV
[07/05/24, 7:54:37 PM] ~ Akash Singh: Details on specs released?
[07/05/24, 8:51:25 PM] ~ Govind: ‎Your security code with ‪+91 81369 07685‬ changed.
[08/05/24, 12:47:41 AM] ~ Chirag Singla: ‎Your security code with ‪+91 75035 07887‬ changed.
[08/05/24, 1:50:08 AM] ~ Amit Singh: ‎~ Amit Singh joined from the community
[08/05/24, 4:05:59 AM] ~ Soumya: ‎~ Soumya joined from the community
[08/05/24, 12:59:37 PM] ~ Supan: ‎~ Supan joined from the community
[08/05/24, 2:23:13 PM] Ojasvi Yadav: None as of now. From the presentation it seems like they'll be beefing up the parallel compute capabilities in a power efficient way, which is quite ideal for on-device inference.
[08/05/24, 2:23:48 PM] Ojasvi Yadav: We'll know the real world performance once new Macs come out in September
[08/05/24, 8:32:11 PM] Rishab Jain: ‎Your security code with Rishab Jain changed.
[08/05/24, 10:06:44 PM] ~ Yash Khivasara: ‎~ Yash Khivasara joined from the community
[09/05/24, 12:58:57 AM] Dr. Pratik Desai: M4 has 38 tflops, while 450watts 4090 has just 100. How? https://www.apple.com/newsroom/2024/05/apple-introduces-m4-chip/
[09/05/24, 1:03:58 AM] ~ Yash: Possibly INT8. Their A17 marketed number was also for INT8. 

Gotta wait for anandtech deepdive for more details
[09/05/24, 1:05:01 AM] Dr. Pratik Desai: In their a11 comparison for the 60x increase, they used fp16.
‎[09/05/24, 1:07:04 AM] Dr. Pratik Desai: ‎image omitted
[09/05/24, 2:16:49 AM] ~ Sachin Dev: ‎~ Sachin Dev joined from the community
[09/05/24, 8:42:00 AM] Ojasvi Yadav: Btw have you used MLX?
[09/05/24, 8:46:28 AM] Dr. Pratik Desai: No
[09/05/24, 10:24:54 AM] ~ HP: Hi guys, I wanted to setup colab like environment on AWS ec2. Anyone aware of any good images that I can use directly?
[09/05/24, 12:22:11 PM] ~ Satrajit: ‎~ Satrajit joined from the community
[09/05/24, 4:13:50 PM] ~ Karthikeyan Vijayan: With GPU?
[09/05/24, 5:24:35 PM] ~ HP: Yes . G5dn or p4
‎[09/05/24, 5:37:36 PM] ~ Deepesh: ‎image omitted
[09/05/24, 7:36:55 PM] ~ Rahul Deora: SageMaker is pretty simple for notebooks
‎[09/05/24, 7:39:00 PM] ~ Sid: ‎image omitted
[09/05/24, 8:16:05 PM] ~ Satrajit: ‎Nirant  removed ~ Satrajit
[09/05/24, 8:38:16 PM] ~ Naveen Pandey: ‎Nirant  removed ~ Naveen Pandey
[09/05/24, 8:40:03 PM] ~ Pratik: ‎Nirant  removed ~ Pratik
[09/05/24, 8:40:34 PM] ~ Anirudh Pupneja: ‎~ Anirudh Pupneja joined from the community
[09/05/24, 8:41:10 PM] ~ Nikhil Chintawar: ‎Nirant  removed ~ Nikhil Chintawar
[09/05/24, 8:41:40 PM] ~ Vivek Kaushal: ‎Nirant  removed ~ Vivek Kaushal
[09/05/24, 8:48:15 PM] ~ Bharath Asokan: ‎~ Bharath Asokan joined from the community
[09/05/24, 8:51:45 PM] ~ Bharath Asokan: ‎Nirant  removed ~ Bharath Asokan
[09/05/24, 9:05:32 PM] ~ Wasim Madha: ‎Your security code with ‪+91 80737 12588‬ changed.
[09/05/24, 9:06:00 PM] ~ Prativa: ‎Nirant  removed ~ Prativa
[09/05/24, 9:06:21 PM] ~ Sparsh Drolia: ‎Nirant  removed ~ Sparsh Drolia
[09/05/24, 9:06:52 PM] ~ Haresh: ‎Nirant  removed ~ Haresh
[09/05/24, 9:08:00 PM] ~ Gurminder: ‎Nirant  removed ~ Gurminder
[09/05/24, 9:08:21 PM] ~ sajith: ‎Nirant  removed ~ sajith
[09/05/24, 9:11:14 PM] ~ Nishanth Gandhi: ‎~ Nishanth Gandhi joined from the community
[09/05/24, 9:19:33 PM] ~ Maheswaran Parameswaran: ‎Nirant  removed ~ Maheswaran Parameswaran
[09/05/24, 9:50:35 PM] ~ Yash: ‎~ Yash joined from the community
[09/05/24, 9:53:41 PM] ~ Prashant Dixit: ‎~ Prashant Dixit joined from the community
[09/05/24, 10:10:43 PM] ~ Saurabh Dash: ‎~ Saurabh Dash joined from the community
[09/05/24, 10:11:36 PM] ~ Nithin: ‎~ Nithin joined from the community
[09/05/24, 10:31:20 PM] ~ Arko C | xylem.ai: ‎Your security code with ‪+91 95641 91888‬ changed.
[09/05/24, 11:01:30 PM] ~ Vipul Maheshwari: Is there any way or adequate resource which can help me to guess an approximation on how much GPU compute I would need for fine tuning an Instruction Dataset?
[09/05/24, 11:06:28 PM] ~ Saurabh Dash: What kinda finetune? Full or Lora/peft/dora?
[09/05/24, 11:07:06 PM] ~ Vipul Maheshwari: PEFT , QLORA
[09/05/24, 11:09:58 PM] ~ Adarsh: https://tokentally.streamlit.app/

Made this a long time ago on recommendation from @19377081307. You can check the LLM cost tool
[09/05/24, 11:12:21 PM] ~ Saurabh Dash: This looks great
[09/05/24, 11:13:37 PM] ~ Adarsh: It's old now i'll have to update it a bit😅
[09/05/24, 11:32:25 PM] ~ Jay Anjankar: ‎~ Jay Anjankar joined from the community
[09/05/24, 11:48:04 PM] ~ Aakash Bakhle: ‎~ Aakash Bakhle joined from the community
[10/05/24, 5:30:08 AM] ~ Prashant Dixit: ‎Nirant  removed ~ Prashant Dixit
[10/05/24, 7:35:27 AM] ~ Saransh Gupta: ‎~ Saransh Gupta joined from the community
[10/05/24, 8:29:23 AM] ~ Souvik: ‎~ Souvik joined from the community
[10/05/24, 9:46:59 AM] ~ ~vishal: ‎~ ~vishal joined from the community
[10/05/24, 11:33:41 AM] ~ Pulkit Gupta: ‎~ Pulkit Gupta joined from the community
[10/05/24, 3:33:09 PM] ~ Shishir Bhaskarwar: ‎Your security code with ‪+91 99162 73372‬ changed.
[10/05/24, 8:25:45 PM] ~ Sushant Ardent: ‎~ Sushant Ardent joined from the community
[10/05/24, 9:59:19 PM] ~ Hadi Khan: ‎~ Hadi Khan joined from the community
[11/05/24, 12:05:17 PM] ~ Shubham Rao: ‎Your security code with ‪+91 94066 11134‬ changed.
[11/05/24, 9:49:15 AM] ~ Parth: ‎~ Parth joined from the community
[11/05/24, 9:51:15 AM] ~ Achal: ‎~ Achal left
[11/05/24, 6:08:21 PM] ~ Shivam Malpani: ‎~ Shivam Malpani joined from the community
[11/05/24, 7:40:01 PM] ~ Hanish..: ‎~ Hanish.. joined from the community
[11/05/24, 9:40:00 PM] ~ Abhinash Khare: Hi, is there a leaderboard or some doc where i can get the first token latency and throughput for LLM of various parameters w.r.t different Nvidia GPUs?
[11/05/24, 10:10:15 PM] ~ Soham (Composio.dev): https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md
[11/05/24, 10:05:36 PM] ~ Abhinash Khare: Thanks Soham for sharing 🙌🏻
[11/05/24, 11:16:37 PM] ~ Chirag Singla: ‎Your security code with ‪+91 75035 07887‬ changed.
[11/05/24, 11:41:49 PM] ~ Chirag Singla: ‎Your security code with ‪+91 75035 07887‬ changed.
[12/05/24, 7:32:33 AM] ~ Rajesh: ‎~ Rajesh joined from the community
[12/05/24, 4:57:58 PM] ~ Unni Krishnan: ‎Your security code with ‪+91 95610 95946‬ changed.
[12/05/24, 5:06:08 PM] ~ Krishna Iyengar: ‎~ Krishna Iyengar joined from the community
[13/05/24, 1:32:24 AM] ~ Soumya: ‎Your security code with ‪+1 (412) 608‑3512‬ changed.
[13/05/24, 10:22:07 AM] ~ .: ‎~ . left
[13/05/24, 12:07:45 PM] ~ Ash Arora: ‎~ Ash Arora joined from the community
[13/05/24, 12:09:45 PM] ~ Aman Beniwal: ‎~ Aman Beniwal joined from the community
[13/05/24, 1:40:01 PM] ~ Ash Arora: ‎Nirant  removed ~ Ash Arora
[13/05/24, 1:54:14 PM] ~ Fayaz: ‎~ Fayaz joined from the community
[13/05/24, 3:43:24 PM] ~ Riken Shah: ‎Your security code with ‪+91 79900 89211‬ changed.
[13/05/24, 6:01:14 PM] ~ Akshay Taneja: ‎~ Akshay Taneja joined from the community
[13/05/24, 10:03:14 PM] ~ Viz: ‎~ Viz joined from the community
[13/05/24, 10:27:07 PM] ~ .: ‎Your security code with ‪+91 93022 89000‬ changed.
[14/05/24, 5:46:17 AM] ~ Prakash Sanker: has anyone hosted mixtral 8x7b on Azure compute? What base image do you need?
[14/05/24, 9:47:53 AM] ~ G.S.K: ‎~ G.S.K left
[14/05/24, 11:12:18 AM] ~ KShivendu: Use ollama on Runpod or vast.ai instead? 

Would be cheaper/easier.
[14/05/24, 11:12:57 AM] ~ KShivendu: (Unless you have rest of your infra on Azure and you want lower network latency)
[14/05/24, 12:56:34 PM] ~ Prakash Sanker: I'm not trying to optimize for money
[14/05/24, 12:56:49 PM] ~ Prakash Sanker: I want to know what it takes to deploy mixtral 8x7b on azure compute - what base image, what instance etc
[14/05/24, 12:56:54 PM] ~ Prakash Sanker: can anyone shed light?
[14/05/24, 1:13:00 PM] ~ Sri Krishna: do you want directly deploy using ai.azure.com? it directly gives you an endpoint. any of the big gpus will work V100, A100. the model takes ~30GB without quantization (https://ai.azure.com/explore/models/mistralai-Mixtral-8x7B-Instruct-v01/version/6/registry/azureml) if you wanna do it manually use vLLM docker https://docs.mistral.ai/deployment/self-deployment/vllm/
[14/05/24, 4:41:19 PM] ~ Abhirami G: ‎~ Abhirami G joined from the community
[14/05/24, 4:48:58 PM] ~ Rajath: ‎~ Rajath joined from the community
[14/05/24, 6:54:47 PM] ~ Srimouli: ‎~ Srimouli joined from the community
[14/05/24, 6:58:37 PM] Ojasvi Yadav: Hi everyone 👋🏻
As part of our community efforts, we have created a new group called "Promotions".

I do commend the members of this group on so far keeping this one of the least promotional group in our community. ♥️ But I'm formalising it as our updated modus operandi moving forward

When to post in promotions group? When you are only promoting or sharing something without either adding to the ongoing discussion or without sharing something interesting you'd like to discuss about that URL.

When is it ok to share links in this "GPU, Performance & Infra" group? Whenever it adds to the ongoing discussion or is something you would like to discuss about - in which case, please clearly state what is interesting in that link so that others can follow up and we can continue the amazing discussions.

Thank you. On the flipside feel free to text me your suggestions on how we can increase the frequency of dialogue here, as that is what I need to maximize. ‎<This message was edited>
[14/05/24, 7:01:29 PM] Ojasvi Yadav: Straight bans are in order in the main group but I'll try to negotiate a reinstatement with @917737887058 if it's your first time
[14/05/24, 8:16:29 PM] ~ Darshan: ‎~ Darshan joined from the community
[15/05/24, 9:04:57 AM] ~ Pulkit Gupta: Hello everyone,

As a newcomer to this community group, I regret not being able to access past discussions. Despite my efforts to find relevant information in the community resources provided at https://nirantk.com/docs/resources/, I couldn't find what I was looking for.
I'm embarking on the journey of LLM(s) development, but with the intention of building an application on top of existing pre-trained models rather than training from scratch. Given this approach, I'm seeking advice on selecting an offline GPU machine (laptop or a desktop PC) to optimize "initial" costs for our solution.
Any insights or recommendations would be immensely helpful, especially considering my limited experience in this space.
[15/05/24, 9:13:25 AM] Nirant : Use the cheapest laptop and remote GPU like Modal Labs or Runpod
[15/05/24, 9:16:19 AM] ~ Rajesh RS: I have a really old (6 year old) Asus laptop with a GTX 1050. It can be used to run a few simple models (although it sounds like a plane taking off when running ollama). If you can get any newer machine with a GPU you should be good. There may be used laptops with RTX 3050 or similar chipsets which should be able to run some of the quantized models on HuggingFace or Ollama.
[15/05/24, 10:18:35 AM] ~ Pulkit Gupta: Thanks for the reply. I did checked the costing for remote GPU vs a decent laptop. I am just thinking which would make more sense as I am operating in stealth.
Though, once the initial infra is developed, I do plan to move on to a better model and for that A10g or a V100 would be required.
[15/05/24, 11:17:04 AM] ~ Srimouli: It all depends on the choice if you take up a cloud machine or api from an access provider you need not worry about installations and also the initial investment would be less, compared to investing on a GPU or a new laptop, because your investment goes up gradually as per the scale you are operating. And when you plan to spinoff your own dedicated machine even then the migration would be simpler. And for example let's say at some point you feel like I'd need a A100 or H100 or any other GPU you can simply upgrade at few 10s of dollars  on cloud. So that would be an advantage but if you are targeting to build something to run locally or on premise that too on a low compute resource scale then I'd suggest you go buy own GPU .

I hope this helps
[15/05/24, 1:10:26 PM] ~ Pulkit Gupta: This helps a lot. Thanks.
[15/05/24, 3:10:34 PM] ~ Ravi Srinivasan: ‎~ Ravi Srinivasan joined using this group's invite link
[15/05/24, 10:33:55 PM] ~ Vandit Gandotra: ‎~ Vandit Gandotra joined from the community
[16/05/24, 9:58:36 AM] ~ Sachin H: ‎~ Sachin H joined from the community
[16/05/24, 1:50:31 PM] ~ Sagar Patidar: ‎~ Sagar Patidar joined from the community
[16/05/24, 8:46:46 PM] ~ Parth Rajauria: ‎~ Parth Rajauria joined from the community
[16/05/24, 9:44:55 PM] ~ Pratyush Sinha: ‎~ Pratyush Sinha left
[16/05/24, 10:57:23 PM] ~ Kaushik Jaiswal: ‎~ Kaushik Jaiswal joined from the community
[17/05/24, 9:39:17 AM] ~ shreyas kowshik: ‎~ shreyas kowshik joined from the community
[17/05/24, 9:47:35 AM] ~ Rohan: ‎~ Rohan joined from the community
[17/05/24, 9:00:09 PM] ~ Ashwin: ‎~ Ashwin joined from the community
[18/05/24, 12:49:43 AM] ~ 🙂: ‎~ 🙂 left
[18/05/24, 12:42:46 PM] Bhavya R: ‎Bhavya R left
[18/05/24, 1:59:53 PM] ~ Unni Krishnan: ‎Your security code with ‪+91 95610 95946‬ changed.
[18/05/24, 1:59:54 PM] ~ Unni Krishnan: ‎Your security code with ‪+91 95610 95946‬ changed.
[18/05/24, 1:59:57 PM] ~ Unni Krishnan: ‎Your security code with ‪+91 95610 95946‬ changed.
[18/05/24, 3:04:42 PM] ~ Amit Bhor: https://x.com/sumo43_/status/1791589684121903555?t=lI0xgs4I0pHsbRwDLtgEdQ&s=08
[18/05/24, 3:14:52 PM] Ojasvi Yadav: Wonder the delta in performance drop if this was converted to black and white first
[18/05/24, 3:15:07 PM] Ojasvi Yadav: 3x lesser data to deal with
[18/05/24, 3:18:02 PM] ~ Amit Bhor: Would depend on mix in training data
[19/05/24, 12:52:06 AM] ~ JVS: Does anyone here try to see what Bend does?
[19/05/24, 6:50:43 AM] Bharat Shetty: Lots of hn discussion on this
[19/05/24, 12:03:53 PM] ~ Arya: ‎This message was deleted by admin Nirant .
[19/05/24, 12:39:10 PM] Bharat Shetty: thanks, but please share your insights here also, context also no ?
[19/05/24, 1:01:28 PM] Nirant : Umm. I'm going to delete context free links at random for now
[19/05/24, 1:08:46 PM] ~ Priyesh Srivastava: Aligned
[19/05/24, 1:08:58 PM] ~ JVS: Looks like right time to explore as it seems very interesting
[19/05/24, 2:47:54 PM] ~ Parth: ‎~ Parth left
[19/05/24, 4:16:46 PM] ~ WhatsApp User: ‎Your security code with ‪+91 79945 01137‬ changed.
[19/05/24, 5:01:37 PM] ~ ~vishal: Hey people, apart from Run AI’s benchmarking of a few LLM Engines/Servers like vLLM & Triton which is here (https://github.com/run-ai/llmperf) is there any other similar work published somewhere?
[19/05/24, 5:11:05 PM] ~ Arko C | xylem.ai: You want APIs or engine benchmarks?
[19/05/24, 5:11:13 PM] ~ Arko C | xylem.ai: Public API leaderboards are there by Artificial Analysis and Martian
[19/05/24, 5:12:45 PM] ~ Arko C | xylem.ai: https://artificialanalysis.ai/leaderboards/providers
[19/05/24, 5:14:33 PM] ~ Anindyadeep Sannigrahi: github.com/PremAI-io/benchmarks
[19/05/24, 5:26:09 PM] ~ Rajesh: ‎~ Rajesh left
[19/05/24, 5:23:33 PM] ~ ~vishal: More of engines to be able to run on premise
[19/05/24, 6:02:32 PM] ~ Shivam Malpani: ‎~ Shivam Malpani left
[19/05/24, 7:36:20 PM] ~ Pulkit: Hey might be a stupid question, but how do you calculate the no. of gpus required to run a model? For eg if im calculating for mixtral 8x22B how many many gpus would I required for the fp16 version?
[19/05/24, 7:45:54 PM] ~ Pulkit: And what would be the cpu requirements?
[19/05/24, 7:48:05 PM] ~ ~vishal: You will find some guidance in this repo which has lot of other useful details but specifically a bit on sizing. https://github.com/stas00/ml-engineering
[19/05/24, 7:51:23 PM] ~ ~vishal: Specifically look at this chapter https://github.com/stas00/ml-engineering/tree/master/compute/accelerator
[19/05/24, 7:51:43 PM] ~ Pulkit: Okay, thanks!
[19/05/24, 8:02:58 PM] ~ Adarsh: https://tokentally.streamlit.app/

you can try this. but the most basic calculation(to be on the safer side) I believe is vram required = 1.2 x model size(in GB) ‎<This message was edited>
[19/05/24, 8:06:20 PM] ~ Pulkit: Thanks!
[19/05/24, 8:09:03 PM] ~ Pulkit: What about GPU themselves, like there would be a difference in performance or like time taken for first token to be generated?
[19/05/24, 8:09:32 PM] ~ Pulkit: If im choosing between t4 , l4, a100 ‎<This message was edited>
[19/05/24, 8:11:29 PM] ~ Adarsh: you are pretty restricted in terms of gpus. 

Its Nvidia + Ampere architecture(If you want to use the standard inference libs vllm, hf tgi, etc. to support flash-attn) + 24GB< (to support the basic 7B models)
[19/05/24, 8:12:28 PM] ~ Adarsh: and in this category, it boils down to how you optimize for inference.
[19/05/24, 8:13:36 PM] ~ Adarsh: tough to measure inference perf. It all depends on how much of "optimization wizardry" you can do. But for standard inference, Nvidia A10 and A100s are p good
[19/05/24, 8:14:47 PM] ~ Pulkit: Okay thanks! This was very helpful! i need to do some reading on this.
[19/05/24, 8:16:15 PM] ~ Adarsh: https://blog.eleuther.ai/transformer-math/
https://cursor.sh/blog/llama-inference

will help you get started
[19/05/24, 8:18:12 PM] ~ Kaushik Jaiswal: https://youtu.be/bCz4OMemCcA?si=NX6AOKxqMGhzHLgE
[19/05/24, 8:36:10 PM] ~ Deepesh: This open source tool helps benchmarking your LLMs on aws. https://github.com/aws-samples/foundation-model-benchmarking-tool
[20/05/24, 2:48:49 AM] ~ Dalan Mendonca: ‎~ Dalan Mendonca joined from the community
[20/05/24, 9:29:54 AM] ~ Deepak: ‎~ Deepak joined from the community
[20/05/24, 11:57:39 AM] ~ Olivia Deka: ‎Your security code with ‪+91 93651 59104‬ changed.
[20/05/24, 6:16:46 PM] Ojasvi Yadav: https://x.com/naklecha/status/1792244347225641338?s=46
[20/05/24, 6:16:56 PM] Ojasvi Yadav: For anyone looking to learn the bare-bones
[20/05/24, 7:05:11 PM] ~ Heerthi Raja H - AI/ML/CV: ‎Your security code with ‪+91 84387 68286‬ changed.
[20/05/24, 7:53:18 PM] Dr. Pratik Desai: Just like people here shared best practices for cuda and torch, do we something for cuda and jax? ‎<This message was edited>
[21/05/24, 8:19:03 AM] Bharat Shetty: this is so good!
[21/05/24, 8:44:11 AM] Bharat Shetty: Also i love this -https://bbycroft.net/llm
[21/05/24, 9:53:33 AM] ~ Nithin: ‎~ Nithin left
[21/05/24, 12:11:14 PM] Ojasvi Yadav: https://arxiv.org/abs/2405.08448
[21/05/24, 12:12:25 PM] Ojasvi Yadav: Google researchers argue here that 
PPO > DPO

@919616406460 @19377081307 thoughts? ‎<This message was edited>
[21/05/24, 12:13:18 PM] Ojasvi Yadav: Seem to have some flavours of "DPO for reasoning, PPO for text generation"
[21/05/24, 12:26:29 PM] ~ Priyesh Srivastava: Although I wasnt asked. PPO is better and I can fight on it all day
[21/05/24, 12:26:54 PM] ~ Priyesh Srivastava: unless there is some very deep multi-turn stuff where a non-LLM based evaluation will fail spectaularly
[21/05/24, 12:28:13 PM] ~ Priyesh Srivastava: like the point is simpler evals with higher penalty on KL loss should be better than an LLM reward model
[21/05/24, 12:28:33 PM] Abhishek Mishra: DPO is hacky and imperfect in many ways. It costs much lesser though and has been used a lot in open source with strict hyperparameter tuning.
[21/05/24, 12:35:42 PM] ~ 🏫: this might help   https://arxiv.org/abs/2404.10719 ,  the main takeaways are that PPO is generally better than DPO, and DPO suffers more heavily from out-of-distribution data
[21/05/24, 5:56:27 PM] ~ AA @ Sugarcane AI: ‎~ AA @ Sugarcane AI joined from the community
[21/05/24, 7:32:11 PM] ‪+91 99770 01253‬: ‎Your security code with ‪+91 99770 01253‬ changed.
[21/05/24, 11:55:29 PM] ~ Navdeesh Ahuja: ‎~ Navdeesh Ahuja joined from the community
[22/05/24, 9:20:15 AM] ~ Sumit: ‎Your security code with ‪+91 88670 77352‬ changed.
[22/05/24, 10:39:00 AM] ~ Deepak: We observed that using both in finetuning is good for applications. Use PPO to do a coarse tuning improving creativity and a modified version of DPO to do further fine tuning to remove any safety or deviation issues. Like coarse targeting followed by fine targeting.
[22/05/24, 2:14:50 PM] ~ Muhammad Hammad Khan: Any good datasets examples for PPO coarse training?
[22/05/24, 3:37:10 PM] ~ Abhik: Chip Huyen is hosting a GPU optimization workshop


https://lu.ma/1wu5ppl5
[22/05/24, 4:17:47 PM] ~ Rajesh RS: Thanks. Seems to have Mark Saroufim on it - enjoyed his MLST talk some time back.https://softwareengineeringdaily.com/2021/06/04/machine-learning-the-great-stagnation-with-mark-saroufim/ Saroufim expresses concern over what he perceives as a stagnation in core ML research, attributing it to a focus on scaling existing models rather than innovating new techniques.Of course this was before ideas like KANs were being researched.
[23/05/24, 1:37:15 PM] ~ Deepak: Reward training and RL fine tuning datasets. An example https://huggingface.co/datasets/argilla/reward-model-data-falcon
‎[23/05/24, 4:06:07 PM] ~ 🏫: ‎image omitted
[23/05/24, 4:06:35 PM] ~ 🏫: Any ideas what are other area we should look at 🤔
[23/05/24, 4:11:30 PM] ~ Sumba: climate change
[23/05/24, 4:13:32 PM] ~ marmik pandya: He's quite bullish on joint embedding predictive architecture
[23/05/24, 4:35:17 PM] ~ KShivendu: Any links you'd recommend for further reading?
[23/05/24, 4:51:07 PM] ~ 🏫: https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/
[23/05/24, 4:51:24 PM] ~ 🏫: Just checking this
[23/05/24, 4:51:57 PM] ~ 🏫: For jepa
[23/05/24, 5:03:44 PM] ~ 🏫: https://arxiv.org/abs/2301.08243
[24/05/24, 1:44:14 AM] ~ Darshan Savaliya: ‎Your security code with ‪+91 74258 75024‬ changed.
[24/05/24, 3:46:49 AM] ~ Darshan Savaliya: ‎Your security code with ‪+91 74258 75024‬ changed.
[24/05/24, 4:21:09 AM] ~ Darshan Savaliya: ‎Your security code with ‪+91 74258 75024‬ changed.
[24/05/24, 1:39:28 PM] ~ Nishant: ‎Nirant  removed ~ Nishant
[24/05/24, 1:39:38 PM] ~ Aayush Mudgal: ‎Nirant  removed ~ Aayush Mudgal
[24/05/24, 1:39:45 PM] ~ Kay Jey: ‎Nirant  removed ~ Kay Jey
[24/05/24, 1:40:39 PM] ~ Tanuj Mendiratta: ‎Nirant  removed ~ Tanuj Mendiratta
[24/05/24, 1:41:17 PM] ~ Siddharth: ‎Nirant  removed ~ Siddharth
[24/05/24, 1:46:58 PM] ~ Ravi Srinivasan: ‎Nirant  removed ~ Ravi Srinivasan
[24/05/24, 1:47:08 PM] ~ Diwank: ‎Nirant  removed ~ Diwank
[24/05/24, 1:52:18 PM] ~ Ramkrishnan Lokanathan: ‎Nirant  removed ~ Ramkrishnan Lokanathan
[24/05/24, 1:52:32 PM] ~ Kiri: ‎Nirant  removed ~ Kiri
[24/05/24, 1:52:49 PM] ~ Aman: ‎Nirant  removed ~ Aman
[24/05/24, 1:55:35 PM] ~ Pooja Priyadarshini: ‎Nirant  removed ~ Pooja Priyadarshini
[24/05/24, 1:56:50 PM] ~ Harmandeep Singh Matharu: ‎Nirant  removed ~ Harmandeep Singh Matharu
[24/05/24, 1:58:27 PM] ~ Arpit: ‎Nirant  removed ~ Arpit
[24/05/24, 1:58:35 PM] ~ JVS: ‎Nirant  removed ~ JVS
[24/05/24, 1:58:43 PM] ~ Nimish Tiwari: ‎Nirant  removed ~ Nimish Tiwari
[24/05/24, 1:58:52 PM] ~ Chirag Gandhi: ‎Nirant  removed ~ Chirag Gandhi
[24/05/24, 1:59:04 PM] ~ Kiran Nambiar: ‎Nirant  removed ~ Kiran Nambiar
[24/05/24, 1:59:59 PM] ~ Surya Penmetsa: ‎Nirant  removed ~ Surya Penmetsa
[24/05/24, 2:01:29 PM] ~ Hariprasad P S: ‎Nirant  removed ~ Hariprasad P S
[24/05/24, 2:01:43 PM] ~ Abhinav Jain: ‎Nirant  removed ~ Abhinav Jain
[24/05/24, 2:02:03 PM] ~ Sandeep: ‎Nirant  removed ~ Sandeep
[24/05/24, 2:02:17 PM] ~ Sourabh: ‎Nirant  removed ~ Sourabh
[24/05/24, 2:05:23 PM] ~ Ayushman: ‎Nirant  removed ~ Ayushman
[24/05/24, 2:06:08 PM] ~ Ananth Radhakrishnan: ‎Nirant  removed ~ Ananth Radhakrishnan
[24/05/24, 2:06:33 PM] ~ Avinash: ‎Nirant  removed ~ Avinash
[24/05/24, 2:08:08 PM] Lavish autoGPT: ‎Nirant  removed Lavish autoGPT
[24/05/24, 2:12:00 PM] ~ Apoorv Saxena: ‎Nirant  removed ~ Apoorv Saxena
[24/05/24, 2:15:20 PM] ~ Amal David: ‎Nirant  removed ~ Amal David
[24/05/24, 2:15:42 PM] ~ Krishaay: ‎Nirant  removed ~ Krishaay
[24/05/24, 2:39:55 PM] ~ Siddharth: ‎Nirant  added ~ Siddharth
[24/05/24, 2:44:21 PM] ~ Kaushik Jaiswal: ‎Nirant  removed ~ Kaushik Jaiswal
[24/05/24, 2:48:14 PM] ~ Dhanush Ram (DR): ‎Nirant  removed ~ Dhanush Ram (DR)
[24/05/24, 2:48:27 PM] ~ Karan: ‎Nirant  removed ~ Karan
[24/05/24, 2:49:18 PM] ~ Ashish Sardana: ‎Nirant  removed ~ Ashish Sardana
[24/05/24, 2:50:07 PM] ~ Ganesh Yeluri: ‎Nirant  removed ~ Ganesh Yeluri
[24/05/24, 2:50:16 PM] ‪+91 6360 090 212‬: ‎Nirant  removed ‪+91 6360 090 212‬
[24/05/24, 2:50:36 PM] ‪+91 88247 89982‬: ‎Nirant  removed ‪+91 88247 89982‬
[24/05/24, 2:51:23 PM] RISHAV: ‎Nirant  removed RISHAV
[24/05/24, 2:52:10 PM] ~ Kartik Sawant: ‎Nirant  removed ~ Kartik Sawant
[24/05/24, 2:52:20 PM] ~ Antony Paul: ‎Nirant  removed ~ Antony Paul
[24/05/24, 2:52:42 PM] ‪+91 85939 07553‬: ‎Nirant  removed ‪+91 85939 07553‬
[24/05/24, 2:52:50 PM] ~ Sairam Chitreddy: ‎Nirant  removed ~ Sairam Chitreddy
[24/05/24, 2:53:50 PM] ~ Kanchi: ‎Nirant  removed ~ Kanchi
[24/05/24, 2:55:02 PM] ~ Paulfinneyx: ‎Nirant  removed ~ Paulfinneyx
[24/05/24, 2:55:28 PM] ~ unmeshr: ‎Nirant  removed ~ unmeshr
[24/05/24, 2:55:50 PM] ~ Wasim Madha: ‎Nirant  removed ~ Wasim Madha
[24/05/24, 2:56:16 PM] ~ Adithya: ‎Nirant  removed ~ Adithya
[24/05/24, 2:59:07 PM] ~ Hemesh Singh: ‎Nirant  removed ~ Hemesh Singh
[24/05/24, 2:59:15 PM] ~ neko: ‎Nirant  removed ~ neko
[24/05/24, 2:59:33 PM] ~ Avani vidhani: ‎Nirant  removed ~ Avani vidhani
[24/05/24, 2:59:51 PM] ~ Kaushik FiddleCube: ‎Nirant  removed ~ Kaushik FiddleCube
[24/05/24, 6:46:18 PM] ~ Aanchal Gupta: ‎~ Aanchal Gupta joined from the community
[24/05/24, 6:54:38 PM] ~ Abhishek Asawa: ‎~ Abhishek Asawa joined from the community
[24/05/24, 7:03:12 PM] ~ Aashray: ‎~ Aashray left
[24/05/24, 7:11:54 PM] ~ Anuj Modi: ‎~ Anuj Modi joined from the community
[24/05/24, 7:41:24 PM] Nirant : ‎Nirant  changed this group’s settings to allow all members to add others to this group.
[24/05/24, 7:57:19 PM] ~ Kishore M R: What could be the minimum token/sec from a edge inferred LLM   to simulate a seamless  audio conversational tone?
[24/05/24, 7:57:43 PM] ~ Kishore M R: Edge device could be the SBCs in market
[24/05/24, 8:03:10 PM] ~ Anindyadeep Sannigrahi: yeahhh straight up this is soo detailed and fun to read
[24/05/24, 8:30:25 PM] ~ Aravind Putrevu: ‎~ Aravind Putrevu joined from the community
[24/05/24, 9:00:13 PM] ~ Maitree Pasad: ‎~ Maitree Pasad joined from the community
[25/05/24, 8:40:57 AM] ~ Bibek: Has anyone tried deploying gpt 4o on azure ai
[25/05/24, 8:44:49 AM] ~ Shan Shah: Yes.
[25/05/24, 8:47:12 AM] Nirant : Please ask the question instead of asking if someone has done X
[25/05/24, 8:49:30 AM] ~ Bibek: I am getting a weird error request header field is too large
[25/05/24, 8:43:00 PM] Pathik Ghugare: Can you share some example and respective error? 
We've been using it for couple days but haven't received any issues
[25/05/24, 10:02:07 PM] ~ Bibek: I tried uploading some pdfs using file upload. PDFs of 15 mb. When try to ask the question in the azure playground I got this error header field is too large
‎[26/05/24, 7:16:45 AM] ~ Bibek: ‎image omitted
[26/05/24, 10:49:46 AM] ~ prasanna kumar: \
[26/05/24, 10:50:59 AM] ~ Mahesh Sathiamoorthy: Has anyone tried MAX from Modular?
modular.com
[26/05/24, 11:46:49 AM] ~ Anindyadeep Sannigrahi: Till now not
[26/05/24, 11:46:53 AM] ~ Anindyadeep Sannigrahi: But now have to
[26/05/24, 11:47:14 AM] ~ Anindyadeep Sannigrahi: Tbh i was kinda ignorant about mojo
[26/05/24, 1:11:57 PM] ~ Soumendra: ‎~ Soumendra joined from the community
[26/05/24, 1:47:56 PM] ~ Aman Beniwal: ‎This message was deleted.
[26/05/24, 6:46:45 PM] ~ Arko Cy: ‎Your security code with ‪+91 78996 62501‬ changed.
[26/05/24, 7:16:38 PM] ~ Arko Cy: ‎Your security code with ‪+91 78996 62501‬ changed.
[26/05/24, 10:48:32 PM] ~ Mihir Kulkarni: ‎Your security code with ‪+91 98191 38350‬ changed.
[27/05/24, 8:23:26 AM] ~ Nikhil Pareek-Future AGI: ‎~ Nikhil Pareek-Future AGI joined using this group's invite link
[27/05/24, 9:25:58 AM] ~ Diptanu Choudhury: I have, we have some MLX based extractors in Indexify. They work great on Macs.
[27/05/24, 11:06:28 AM] ~ Deepak: ‎Your security code with ‪+91 78299 03846‬ changed.
[27/05/24, 11:20:12 AM] ~ ~vishal: Hey folks - has any seen networking from a networking vendor instead of Nvidia Infiniband being deployed in production in a AI data center?

We are working with a networking vendor who claims to perform as good as Nvidia Infiniband and provide speeds upto 800GBPS in a lab setup

But wanted to see if there is a better way to validate/know from field.
[27/05/24, 11:50:18 AM] ~ ~vishal: Just to add a bit more context to above question - Infiniband works great but
- People want to use ROCE V2 based networking I think from a skill set pov as it is generally better understood
- ⁠Cost optimisation
[27/05/24, 7:00:19 PM] ~ Shaurya Gupta: https://github.com/VinciGit00/Scrapegraph-ai has anyone tried this? Uses graph and LLMs for scraping, Looks promising
[27/05/24, 11:06:15 PM] ~ Bibek: will be a bit expensive since gpt tokens cost alot
[28/05/24, 8:45:47 AM] Ashutosh Gen AI: Has anyone here compared various toolkits (skypilot, vllm, tgi, litserve etc etc) for throughput/latency comparison for deploying your own llm in a distributed cluster? What's the one that has given you best performance?
[28/05/24, 9:13:22 AM] ~ Srimouli: For me I got the best on using vllm when having local deployment
[28/05/24, 10:06:14 AM] Ojasvi Yadav: As far as I know
Skypilots skyserve is a model serving library that can run both the serving frameworks TGI and vllm 

And I've used skypilot in production (with vllm as the serving framework) for a decent amount of throughput, but haven't compared it with bare-bones vllm nor tgi
[28/05/24, 10:07:26 AM] Ashutosh Gen AI: What was your traffic pattern - Batch or non concurrent sync or concurrent sync?
[28/05/24, 10:10:49 AM] Ashutosh Gen AI: By chance have you compared throughput with these other providers like groq, together ai etc? I'm assuming it'd be lesser with just vllm? @919971004124 ‎<This message was edited>
[28/05/24, 10:11:39 AM] ~ ~vishal: RunAI folks have done some work, their scripts here: https://github.com/run-ai/llmperf (It has link to the PDF which has finding/results of test)

Also AWS has published their scripts & findings which you may use: https://github.com/aws-samples/foundation-model-benchmarking-tool
[28/05/24, 10:11:56 AM] Ojasvi Yadav: As far as I know skypilot does all 3 based on the GPU config

You can configure how each would be handled based on the total token count of all the queued up requests

Larger GPU clusters can naturally provide more batch and concurrent sync
[28/05/24, 10:12:52 AM] Ojasvi Yadav: No, like I said throughputs rely heavily on the GPU infra and your batch processing config
[28/05/24, 10:13:29 AM] Ojasvi Yadav: Very difficult to test and compare in a controlled environment
[28/05/24, 10:14:18 AM] Ojasvi Yadav: My suggestion would be to compare throughput with your costs to get the best bang for buck
And when I've done that I found skypilot to be more economical for my usecase as compared to together ai
[28/05/24, 10:14:32 AM] Aishwarya Goel: some independent study on Azure A100 across 8 LLMs and six different libraries to calculate tokens/second and time to first token alongside comparison of these libraries. This can help you in your task. You can check the detailed analysis here:
Part 1: https://www.inferless.com/learn/exploring-llms-speed-benchmarks-independent-analysis
Part 2: https://www.inferless.com/learn/exploring-llms-speed-benchmarks-independent-analysis---part-2
[28/05/24, 11:38:48 AM] Ashutosh Gen AI: Alright, thanks a bunch folks! Let me go through these resources! 🙂
[28/05/24, 11:48:13 AM] Ashutosh Gen AI: @919971004124 is it ok if i dm you?
[28/05/24, 12:10:14 PM] ~ Joy: Try llamafile as well. As per their website they are 2x faster than ollama
[28/05/24, 12:15:59 PM] Ashutosh Gen AI: Correct me if i’m wrong. But these don’t enable you to deploy a restful server on a distributed gpu cluster (with auto scaling etc) right? I checked ollama briefly, it seems more for local usecases or for single instance usecases at best, distributed support doesn’t seem to be there. I could be wrong. Checked llamafile briefly, seems similar to ollama in it being local only? ‎<This message was edited>
[28/05/24, 12:18:58 PM] ~ rohan~: I have tried TGI and vLLM on single node, multi GPU setups
But I know they also support multi node, multi GPU setups

In my experience, vLLM is best all rounder inference server with really fast engine and lots of usefully features like JSON mode
[28/05/24, 12:19:39 PM] ~ rohan~: I have heard vLLM + Ray + K8s works really well *for multi node setups ‎<This message was edited>
[28/05/24, 12:20:15 PM] Ashutosh Gen AI: Thanks! Have you tried deploying any of the llama3 variants? Possibly the 70B one? ‎<This message was edited>
[28/05/24, 12:22:11 PM] ~ rohan~: Yes! I have tried Hermes 2 Pro, both TGI and vLLM supports it
[28/05/24, 12:22:36 PM] ~ rohan~: Sadly, don't have budget for 70b 🥲
[28/05/24, 12:25:23 PM] ~ Venkat: - Rayserve 
- vLLM with Rayserve for distributed inference 
- ⁠nvidia Triton 

What I’m seeing in OSS inference space is every ~6 months there are new algo / optimizations that makes inference engines faster.  All the other servers / engines implement it

So far vLLM & TensorRTLLM are leading
[28/05/24, 12:26:49 PM] ~ Joy: Yes you are right probably. I only tried on single GPUs
[28/05/24, 12:27:47 PM] Ashutosh Gen AI: Hmm, fair. That’s why particularly I was slightly leaning for a toolkit like skypilot, which adds new engines and abstracts it for us to change engines easily down the line if needed.

I did come across Rayserve as well, will play around with it, if it supports multiple engines as well. ‎<This message was edited>
[28/05/24, 12:35:01 PM] ~ Venkat: I haven’t tried sky pilot 

Decision should be based on:
- just running oss models or
- fine tuning + inference 
- ⁠retraining + FT + inference
[28/05/24, 1:23:30 PM] ~ Akshay Taneja: https://www.linkedin.com/posts/zhousharon_introducing-lamini-inference-with-52x-more-activity-7199450943677976576-NFQc?utm_source=share&utm_medium=member_android
[28/05/24, 1:23:47 PM] ~ Akshay Taneja: Also try this from a startup called Lamini in USA
[28/05/24, 2:09:00 PM] ~ Aravind Putrevu: The proprietary stack still plays a significant role in the Distributed LLM Inference space. 

Some special configs + custom kernel and model optimizations make it super efficient. For example, you can do 800 tokens/s without something like Groq’s LPU.
[28/05/24, 2:09:23 PM] ~ Aravind Putrevu: I meant to say there is more to come in OSS yet.
[28/05/24, 5:47:40 PM] ~ romit: Hey folks, I wrote a couple of blogs explaining the GPU hardware and the programming model:
1. https://cmeraki.github.io/gpu-part1.html
2. https://cmeraki.github.io/gpu-part2.html

Also, I implemented the ViT model completely from scratch in Triton, the performance is competitive to that of HF (https://github.com/cmeraki/vit.triton)

Do check it out if you want to learn about Triton and CUDA.
Thanks
[29/05/24, 2:10:17 AM] ~ Saurabh Dash: TensorRT is the fastest but it is a bit of a pain to deal with — also takes forever to build. comparatively vLLM has an easier curve.
[29/05/24, 2:46:25 AM] Aishwarya Goel: Agreed.. wrote a doc on using tensorRT effectively - https://github.com/inferless/TensorRT-LLM ( see if this helps)
[29/05/24, 2:51:52 AM] ~ Saurabh Dash: That does look neat! 
Another issue to be aware of with tensorRT LLM is that the team casually drops a 3M LoC update without any change logs. Good luck merging with upstream if you want updated features.
[29/05/24, 12:24:52 PM] Ashutosh Gen AI: Anyone here, has tried deploying any of the llama quantised models (llama.cpp for ex)? say using ollama, which gives a nice restful server out of the box? Skypilot also has out of the box integration with ollama.

What kind of performance (model performance/and throughput performance) have you seen with it? ‎<This message was edited>
[29/05/24, 2:05:05 PM] ~ Albert: ‎~ Albert joined from the community
[29/05/24, 3:22:36 PM] ~ Rajesh RS: Hi folks, are there any resources that can help me build on-device ML applications? Mainly, I'm thinking about an image/video segmentation problem for pose, with a focus on human gait. Ideally some guidebook for on-device deployment on Android phones would help. Thank you.
[30/05/24, 1:24:16 AM] GPU, Performance & Infra: ‎You turned on admin approval to join this group
