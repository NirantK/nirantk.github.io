{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../20230726_Messages.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc49eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Message\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a210b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by week\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
    "df[\"Week\"] = df[\"Datetime\"].dt.isocalendar().week\n",
    "df[\"Date\"] = df[\"Datetime\"].dt.date\n",
    "\n",
    "# Group by Date\n",
    "daily_df = df.groupby(\"Date\").agg({\"Message\": \" \\n \".join}).reset_index()\n",
    "daily_df = pd.DataFrame(daily_df)\n",
    "len(daily_df)\n",
    "\n",
    "# # Group by Week\n",
    "# weekly_df = df.groupby('Week').agg({'Message': ' \\n '.join}).reset_index()\n",
    "# weekly_df = pd.DataFrame(messages_df)\n",
    "# print(weekly)\n",
    "# print(weekly_df[\"Message\"][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"wc\"] = daily_df[\"Message\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8296bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"wc\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "d = daily_df[\"Date\"][42]\n",
    "\n",
    "\n",
    "def human_date(d):\n",
    "    def ordinal(n):\n",
    "        return \"%d%s\" % (\n",
    "            n,\n",
    "            \"tsnrhtdd\"[((n // 10 % 10 != 1) * (n % 10 < 4) * n % 10) :: 4],\n",
    "        )\n",
    "\n",
    "    formatted_date = d.strftime(f\"{ordinal(d.day)} %B %Y\")\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "print(human_date(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113c0bf",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "Built with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77948434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8699fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "class SummarizeDay:\n",
    "    def __init__(self, plain_text):\n",
    "        self.prompt_template = \"\"\"This is a chaotic Generative AI Group Chat transcript. Write detailed, exhaustive bullet point recap of topics discussed. Extract COMPLETE URL of web and social links with context. Please organise it into sections, only when needed:\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "Use Markdown. Add ## for section titles. TOPICS RECAP:\"\"\"\n",
    "        # Research with weblinks where relevant EXACTLY ONCE:\n",
    "        self.PROMPT = PromptTemplate(\n",
    "            template=self.prompt_template, input_variables=[\"text\"]\n",
    "        )\n",
    "        #         self.chain = load_summarize_chain(ChatOpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "        self.chain = load_summarize_chain(\n",
    "            ChatOpenAI(temperature=0), chain_type=\"stuff\", prompt=self.PROMPT\n",
    "        )\n",
    "        self.docs = self.make_docs(plain_text)\n",
    "\n",
    "    @lru_cache\n",
    "    def make_docs(self, plain_text: str):\n",
    "        texts = text_splitter.split_text(plain_text)\n",
    "        docs = [Document(page_content=t) for t in texts]\n",
    "        return docs\n",
    "\n",
    "    def summarize_docs(self):\n",
    "        chain_output = self.chain(\n",
    "            {\"input_documents\": self.docs}, return_only_outputs=True\n",
    "        )\n",
    "        return chain_output\n",
    "\n",
    "\n",
    "def summarize(message: str) -> str:\n",
    "    sd = SummarizeDay(message)\n",
    "    chain_output = sd.summarize_docs()\n",
    "    summary_text = chain_output[\"output_text\"]\n",
    "    print(summary_text)\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "daily_df[\"Summary\"] = daily_df[\"Message\"].apply(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "WINDOW = 1\n",
    "\n",
    "\n",
    "def extract_urls_with_context(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    url_pattern = re.compile(\n",
    "        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    )\n",
    "    urls_with_context = []\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        for match in url_pattern.finditer(line):\n",
    "            start, end = match.span()\n",
    "            prev_line = lines[idx - WINDOW] if idx > 0 else \"\"\n",
    "            next_line = lines[idx + WINDOW] if idx < len(lines) - 1 else \"\"\n",
    "            context = f\"{prev_line}\\n{line}\\n{next_line}\".strip()\n",
    "            urls_with_context.append((match.group(), context))\n",
    "\n",
    "    return urls_with_context\n",
    "\n",
    "\n",
    "class LinksContext:\n",
    "    def __init__(self, plain_text):\n",
    "        self.prompt_template = \"\"\"For the given URL, there is some context. Newlines may or may not be related to the link, but the message in the same link as link is related to the link.\n",
    "        \n",
    "{text}\n",
    "        \n",
    "Mention URL with context. Single bullet point:\"\"\"\n",
    "        # Research with weblinks where relevant EXACTLY ONCE:\n",
    "        self.PROMPT = PromptTemplate(\n",
    "            template=self.prompt_template, input_variables=[\"text\"]\n",
    "        )\n",
    "        #         self.chain = load_summarize_chain(ChatOpenAI(temperature=0, model_name=\"gpt-4\"), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "        self.chain = load_summarize_chain(\n",
    "            ChatOpenAI(temperature=0), chain_type=\"stuff\", prompt=self.PROMPT\n",
    "        )\n",
    "        self.docs = self.make_docs(plain_text)\n",
    "\n",
    "    @lru_cache\n",
    "    def make_docs(self, plain_text: str):\n",
    "        texts = text_splitter.split_text(plain_text)\n",
    "        docs = [Document(page_content=t) for t in texts]\n",
    "        return docs\n",
    "\n",
    "    def summarize_docs(self):\n",
    "        chain_output = self.chain(\n",
    "            {\"input_documents\": self.docs}, return_only_outputs=True\n",
    "        )\n",
    "        return chain_output\n",
    "\n",
    "\n",
    "def end_note_with_links(message: str) -> str:\n",
    "    url_groups = extract_urls_with_context(message)\n",
    "    link_with_description = []\n",
    "    for ug in url_groups:\n",
    "        lc = LinksContext(ug[1])\n",
    "        chain_output = lc.summarize_docs()\n",
    "        output_text = chain_output[\"output_text\"]\n",
    "        link_with_description.append(output_text)\n",
    "    return \"\\n\".join(link_with_description)\n",
    "\n",
    "\n",
    "# message = daily_df[\"Message\"][33]\n",
    "# print(end_note_with_links(message))\n",
    "daily_df[\"EndNote\"] = daily_df[\"Message\"].apply(end_note_with_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae48303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleDescription:\n",
    "    def __init__(self, plain_text):\n",
    "        self.prompt_template = \"\"\"For the given discussion, write a short title and description, separate both by \\n\\n\n",
    "\n",
    "{text}\n",
    "        \n",
    "Return a single valid JSON with \n",
    "```\n",
    "\"title\":\n",
    "\"description:\":\n",
    "```:\"\"\"\n",
    "        # Research with weblinks where relevant EXACTLY ONCE:\n",
    "        self.PROMPT = PromptTemplate(\n",
    "            template=self.prompt_template, input_variables=[\"text\"]\n",
    "        )\n",
    "        self.chain = load_summarize_chain(\n",
    "            ChatOpenAI(temperature=0, model_name=\"gpt-4\"),\n",
    "            chain_type=\"map_reduce\",\n",
    "            return_intermediate_steps=True,\n",
    "            map_prompt=self.PROMPT,\n",
    "            combine_prompt=self.PROMPT,\n",
    "        )\n",
    "        #         self.chain = load_summarize_chain(ChatOpenAI(temperature=0), chain_type=\"stuff\", prompt=self.PROMPT)\n",
    "        self.docs = self.make_docs(plain_text)\n",
    "\n",
    "    @lru_cache\n",
    "    def make_docs(self, plain_text: str):\n",
    "        texts = text_splitter.split_text(plain_text)\n",
    "        docs = [Document(page_content=t) for t in texts]\n",
    "        return docs\n",
    "\n",
    "    def summarize_docs(self):\n",
    "        chain_output = self.chain(\n",
    "            {\"input_documents\": self.docs}, return_only_outputs=True\n",
    "        )\n",
    "        return chain_output\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def get_title_description(summary_text: str) -> dict[str, str]:\n",
    "    td = TitleDescription(summary_text)\n",
    "    title_description = td.summarize_docs()\n",
    "    try:\n",
    "        fields = json.loads(title_description[\"output_text\"])\n",
    "    except Exception as e:\n",
    "        print(title_description[\"output_text\"])\n",
    "        title_description = td.summarize_docs()\n",
    "        fields = json.loads(title_description[\"output_text\"])\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9337354",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5709c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13beefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pytz\n",
    "from datetime import datetime, time\n",
    "\n",
    "\n",
    "def get_page_header_date(date_object):\n",
    "    # Combine the date object with a time object and set the desired timezone\n",
    "    dt = datetime.combine(date_object, time())\n",
    "    desired_timezone = pytz.timezone(\"Asia/Kolkata\")\n",
    "    localized_dt = desired_timezone.localize(dt)\n",
    "\n",
    "    # Format the datetime object using strftime\n",
    "    formatted_datetime = localized_dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    formatted_datetime = formatted_datetime[:-2] + \":\" + formatted_datetime[-2:]\n",
    "\n",
    "    return formatted_datetime\n",
    "\n",
    "\n",
    "def make_page_header(row):\n",
    "    date, summary_text = row[\"Date\"], row[\"Summary\"]\n",
    "    dt = get_page_header_date(date)\n",
    "    fields = get_title_description(summary_text)\n",
    "    summary_title, summary_description = fields[\"title\"], fields[\"description\"]\n",
    "\n",
    "    page_header = f\"\"\"+++\n",
    "title =  \"{summary_title}\"\n",
    "date = {dt}\n",
    "tags = [\"daily_summary\"]\n",
    "featured_image = \"\"\n",
    "description = \"{summary_description}\"\n",
    "toc = true\n",
    "+++\n",
    "\"\"\"\n",
    "    return page_header\n",
    "\n",
    "\n",
    "page_headers = []\n",
    "for idx in range(len(daily_df)):\n",
    "    page_headers.append(make_page_header(daily_df.iloc[idx]))\n",
    "    print(page_headers[-1])\n",
    "# daily_df[\"page_header\"] = [make_page_header(row=df_row) for df_row in daily_df.iloc[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[\"page_headers\"] = page_headers\n",
    "daily_df.to_json(\"daily_backup.json\")\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f62273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_page(row):\n",
    "    page = (\n",
    "        row[\"page_headers\"]\n",
    "        + \"\\n\"\n",
    "        + row[\"Summary\"]\n",
    "        + \"\\n\"\n",
    "        + \"\\n## Links\\nThe description and link can be mismatched because of extraction errors.\\n\\n\"\n",
    "        + row[\"EndNote\"]\n",
    "    )\n",
    "    file_name = f\"{human_date(row['Date'])}.md\"\n",
    "    return page, file_name\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "write_dir = Path(\"../../content/ai/\").resolve()\n",
    "\n",
    "for idx in range(len(daily_df)):\n",
    "    page, file_name = make_page(daily_df.iloc[idx])\n",
    "    file_path = write_dir / file_name\n",
    "    with file_path.open(\"w\") as f:\n",
    "        f.write(page)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
